<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Cognitive Technologies: From Theory and Data to Application</title>
  <meta name="description" content="Papers on assorted cognitive technologies">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Cognitive Technologies: From Theory and Data to Application" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Papers on assorted cognitive technologies" />
  <meta name="github-repo" content="CrumpLab/cognitivetechnologies" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Cognitive Technologies: From Theory and Data to Application" />
  
  <meta name="twitter:description" content="Papers on assorted cognitive technologies" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html">
<link rel="next" href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Cognitive Technologies</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html"><i class="fa fa-check"></i><b>1</b> Reflections on our tour of Cognitive Technologies</a><ul>
<li class="chapter" data-level="1.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#snake-oil-an-old-technology"><i class="fa fa-check"></i><b>1.1</b> Snake oil: An old technology</a></li>
<li class="chapter" data-level="1.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#tech-that-works"><i class="fa fa-check"></i><b>1.2</b> Tech that works</a></li>
<li class="chapter" data-level="1.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#connection-to-instance-theory"><i class="fa fa-check"></i><b>1.3</b> Connection to Instance Theory</a><ul>
<li class="chapter" data-level="1.3.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#procedures-of-mind"><i class="fa fa-check"></i><b>1.3.1</b> Procedures of Mind</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#exciting-directions"><i class="fa fa-check"></i><b>1.4</b> Exciting Directions</a><ul>
<li class="chapter" data-level="1.4.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#conversational-ai"><i class="fa fa-check"></i><b>1.4.1</b> Conversational AI</a></li>
<li class="chapter" data-level="1.4.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#decoding-brain-states"><i class="fa fa-check"></i><b>1.4.2</b> Decoding Brain states</a></li>
<li class="chapter" data-level="1.4.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#detecting-deception"><i class="fa fa-check"></i><b>1.4.3</b> Detecting Deception</a></li>
<li class="chapter" data-level="1.4.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#inner-voice-decoding-with-a-chinstrap"><i class="fa fa-check"></i><b>1.4.4</b> Inner Voice decoding with a chinstrap!</a></li>
<li class="chapter" data-level="1.4.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#image-memorability"><i class="fa fa-check"></i><b>1.4.5</b> Image Memorability</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#thats-all"><i class="fa fa-check"></i><b>1.5</b> That’s all</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><i class="fa fa-check"></i><b>2</b> Computational Classification Techniques for Biomedical and Clinical Big Data</a><ul>
<li class="chapter" data-level="2.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#abstract"><i class="fa fa-check"></i><b>2.1</b> Abstract</a></li>
<li class="chapter" data-level="2.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#introduction"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#previous-work"><i class="fa fa-check"></i><b>2.3</b> Previous Work</a><ul>
<li class="chapter" data-level="2.3.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#topic-modeling"><i class="fa fa-check"></i><b>2.3.1</b> Topic Modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#neural-networks"><i class="fa fa-check"></i><b>2.3.2</b> Neural Networks</a></li>
<li class="chapter" data-level="2.3.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clustering"><i class="fa fa-check"></i><b>2.3.3</b> Clustering</a></li>
<li class="chapter" data-level="2.3.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#word-sense-disambiguation"><i class="fa fa-check"></i><b>2.3.4</b> Word Sense Disambiguation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#medical-and-clinical-applications"><i class="fa fa-check"></i><b>2.4</b> Medical and Clinical Applications</a></li>
<li class="chapter" data-level="2.5" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clerical-applications"><i class="fa fa-check"></i><b>2.5</b> Clerical Applications</a></li>
<li class="chapter" data-level="2.6" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#real-world-applications-and-future-work"><i class="fa fa-check"></i><b>2.6</b> Real World Applications and Future Work</a></li>
<li class="chapter" data-level="2.7" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
<li class="chapter" data-level="2.8" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#references"><i class="fa fa-check"></i><b>2.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html"><i class="fa fa-check"></i><b>3</b> Sonification and augmented cognition: A brief overview</a><ul>
<li class="chapter" data-level="3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#types-of-sonification."><i class="fa fa-check"></i><b>3.1</b> Types of sonification.</a></li>
<li class="chapter" data-level="3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#why-sonify-non-sonic-information"><i class="fa fa-check"></i><b>3.2</b> Why sonify non-sonic information?</a></li>
<li class="chapter" data-level="3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#using-sonification-to-augment-cognition"><i class="fa fa-check"></i><b>3.3</b> Using sonification to augment cognition</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-attention-and-situational-awareness"><i class="fa fa-check"></i><b>3.3.1</b> Perception, attention, and situational awareness</a></li>
<li class="chapter" data-level="3.3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-and-action-in-motor-skill-learning"><i class="fa fa-check"></i><b>3.3.2</b> Perception and action in motor skill learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#data-analysis-and-pattern-recognition."><i class="fa fa-check"></i><b>3.3.3</b> Data analysis and pattern recognition.</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#references-1"><i class="fa fa-check"></i><b>3.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><i class="fa fa-check"></i><b>4</b> A Brief Review of Augmented Reality Display Technologies and Combination with Brain-Computer Interfaces</a><ul>
<li class="chapter" data-level="4.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#abstract-1"><i class="fa fa-check"></i><b>4.1</b> Abstract</a></li>
<li class="chapter" data-level="4.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#introduction-1"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#short-overview-brain-structure"><i class="fa fa-check"></i><b>4.3</b> Short overview: Brain Structure</a></li>
<li class="chapter" data-level="4.4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-technologies-and-basic-principles-of-brain-data-acquisition"><i class="fa fa-check"></i><b>4.4</b> BCI Technologies and Basic Principles of Brain Data Acquisition</a></li>
<li class="chapter" data-level="4.5" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#common-electroencephalography-methods"><i class="fa fa-check"></i><b>4.5</b> Common Electroencephalography Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electroencephalography-eeg-and-even-related-potential-erp"><i class="fa fa-check"></i><b>4.5.1</b> Electroencephalography (EEG) and Even-Related Potential (ERP)</a></li>
<li class="chapter" data-level="4.5.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#eeg-channel-selection-examples"><i class="fa fa-check"></i><b>4.5.2</b> EEG Channel Selection Examples</a></li>
<li class="chapter" data-level="4.5.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electromyography-emg-and-electrooculography-eog"><i class="fa fa-check"></i><b>4.5.3</b> Electromyography (EMG) and Electrooculography (EOG)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#brain-computer-interfaces-bci"><i class="fa fa-check"></i><b>4.6</b> Brain-Computer Interfaces (BCI)</a><ul>
<li class="chapter" data-level="4.6.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-functions"><i class="fa fa-check"></i><b>4.6.1</b> BCI Functions</a></li>
<li class="chapter" data-level="4.6.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#different-types-of-bci"><i class="fa fa-check"></i><b>4.6.2</b> Different Types of BCI</a></li>
<li class="chapter" data-level="4.6.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#applications-for-bci"><i class="fa fa-check"></i><b>4.6.3</b> Applications for BCI</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#augmented-reality-ar"><i class="fa fa-check"></i><b>4.7</b> Augmented Reality (AR)</a><ul>
<li class="chapter" data-level="4.7.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-technologies"><i class="fa fa-check"></i><b>4.7.1</b> AR Technologies</a></li>
<li class="chapter" data-level="4.7.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-devices"><i class="fa fa-check"></i><b>4.7.2</b> AR Devices</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#combining-ar-and-bci"><i class="fa fa-check"></i><b>4.8</b> Combining AR and BCI</a></li>
<li class="chapter" data-level="4.9" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#limitations-and-interpretations"><i class="fa fa-check"></i><b>4.9</b> Limitations and Interpretations</a></li>
<li class="chapter" data-level="4.10" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#references-2"><i class="fa fa-check"></i><b>4.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><i class="fa fa-check"></i><b>5</b> A Methodology for Microdosing Research: Cognitive behavioral tasks as investigative tools for tracking low-dose effects of psilocybin</a><ul>
<li class="chapter" data-level="5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#abstract-2"><i class="fa fa-check"></i><b>5.1</b> Abstract</a></li>
<li class="chapter" data-level="5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#introduction-2"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#the-third-wave-of-psychedelic-science"><i class="fa fa-check"></i><b>5.3</b> The Third Wave of Psychedelic Science</a><ul>
<li class="chapter" data-level="5.3.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#clinical-science"><i class="fa fa-check"></i><b>5.3.1</b> Clinical Science</a></li>
<li class="chapter" data-level="5.3.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#neuroscience"><i class="fa fa-check"></i><b>5.3.2</b> Neuroscience</a></li>
<li class="chapter" data-level="5.3.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#summary"><i class="fa fa-check"></i><b>5.3.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#microdosing"><i class="fa fa-check"></i><b>5.4</b> Microdosing</a></li>
<li class="chapter" data-level="5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#tasks-to-target-psychedelic-drug-effects"><i class="fa fa-check"></i><b>5.5</b> Tasks to Target Psychedelic Drug Effects</a><ul>
<li class="chapter" data-level="5.5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-effects"><i class="fa fa-check"></i><b>5.5.1</b> Cognitive Effects</a></li>
<li class="chapter" data-level="5.5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#working-memory-and-inhibition-tasks"><i class="fa fa-check"></i><b>5.5.2</b> Working-Memory and Inhibition Tasks</a></li>
<li class="chapter" data-level="5.5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-flexibility-tasks"><i class="fa fa-check"></i><b>5.5.3</b> Cognitive Flexibility Tasks</a></li>
<li class="chapter" data-level="5.5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#creativity-task"><i class="fa fa-check"></i><b>5.5.4</b> Creativity Task</a></li>
<li class="chapter" data-level="5.5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-effects"><i class="fa fa-check"></i><b>5.5.5</b> Perceptual Effects</a></li>
<li class="chapter" data-level="5.5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-processing-tasks"><i class="fa fa-check"></i><b>5.5.6</b> Perceptual Processing Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#discussion-and-concluding-remarks"><i class="fa fa-check"></i><b>5.6</b> Discussion and Concluding Remarks</a></li>
<li class="chapter" data-level="5.7" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#references-3"><i class="fa fa-check"></i><b>5.7</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><i class="fa fa-check"></i><b>6</b> Perceiving the World Around Us: How Divergent Methods Illustrate Convergent Perspectives</a><ul>
<li class="chapter" data-level="6.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#the-visual-system-and-present-controversy"><i class="fa fa-check"></i><b>6.2</b> The Visual System and Present Controversy</a><ul>
<li class="chapter" data-level="6.2.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#neurophysiological-evidence"><i class="fa fa-check"></i><b>6.2.1</b> Neurophysiological Evidence</a></li>
<li class="chapter" data-level="6.2.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#emotions-motivations-and-perception"><i class="fa fa-check"></i><b>6.2.2</b> Emotions, Motivations, and Perception</a></li>
<li class="chapter" data-level="6.2.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#zooming-in-on-fear"><i class="fa fa-check"></i><b>6.2.3</b> Zooming in on Fear</a></li>
<li class="chapter" data-level="6.2.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#other-emotions"><i class="fa fa-check"></i><b>6.2.4</b> Other Emotions</a></li>
<li class="chapter" data-level="6.2.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#motivated-perception"><i class="fa fa-check"></i><b>6.2.5</b> Motivated Perception</a></li>
<li class="chapter" data-level="6.2.6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-1"><i class="fa fa-check"></i><b>6.2.6</b> Conclusion</a></li>
<li class="chapter" data-level="6.2.7" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches"><i class="fa fa-check"></i><b>6.2.7</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-and-computational-approaches"><i class="fa fa-check"></i><b>6.3</b> Cognitive and computational approaches</a><ul>
<li class="chapter" data-level="6.3.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-approaches"><i class="fa fa-check"></i><b>6.3.1</b> Cognitive Approaches</a></li>
<li class="chapter" data-level="6.3.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#computational-approaches"><i class="fa fa-check"></i><b>6.3.2</b> Computational Approaches</a></li>
<li class="chapter" data-level="6.3.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-2"><i class="fa fa-check"></i><b>6.3.3</b> Conclusion</a></li>
<li class="chapter" data-level="6.3.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches-1"><i class="fa fa-check"></i><b>6.3.4</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-and-implications"><i class="fa fa-check"></i><b>6.4</b> Conclusion and Implications</a></li>
<li class="chapter" data-level="6.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#references-4"><i class="fa fa-check"></i><b>6.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html"><i class="fa fa-check"></i><b>7</b> Brain Training and Cognition</a><ul>
<li class="chapter" data-level="7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#abstract-3"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#introduction-4"><i class="fa fa-check"></i><b>7.2</b> Introduction</a></li>
<li class="chapter" data-level="7.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#chess-and-music"><i class="fa fa-check"></i><b>7.3</b> Chess and Music</a><ul>
<li class="chapter" data-level="7.3.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#near-and-far-transfer"><i class="fa fa-check"></i><b>7.3.1</b> Near and Far Transfer</a></li>
<li class="chapter" data-level="7.3.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study"><i class="fa fa-check"></i><b>7.3.2</b> A Meta-Analysis Study</a></li>
<li class="chapter" data-level="7.3.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#patterns-in-findings"><i class="fa fa-check"></i><b>7.3.3</b> Patterns in Findings</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training"><i class="fa fa-check"></i><b>7.4</b> Cognitive Training</a></li>
<li class="chapter" data-level="7.5" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training-programs-do-they-work"><i class="fa fa-check"></i><b>7.5</b> Cognitive Training Programs: Do They Work?</a><ul>
<li class="chapter" data-level="7.5.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#brain-training-games"><i class="fa fa-check"></i><b>7.5.1</b> Brain Training Games</a></li>
<li class="chapter" data-level="7.5.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#bilingual-brain-training"><i class="fa fa-check"></i><b>7.5.2</b> Bilingual Brain Training</a></li>
<li class="chapter" data-level="7.5.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#working-memory-training"><i class="fa fa-check"></i><b>7.5.3</b> Working Memory Training</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#fitnessphysical-activities"><i class="fa fa-check"></i><b>7.6</b> Fitness/Physical activities</a><ul>
<li class="chapter" data-level="7.6.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study-1"><i class="fa fa-check"></i><b>7.6.1</b> A Meta-Analysis Study</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#combining-cognitive-and-aerobic-training"><i class="fa fa-check"></i><b>7.7</b> Combining Cognitive and Aerobic Training</a><ul>
<li class="chapter" data-level="7.7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#problems-with-the-research"><i class="fa fa-check"></i><b>7.7.1</b> Problems with the research</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#limitations-and-future-work"><i class="fa fa-check"></i><b>7.8</b> Limitations and Future Work</a><ul>
<li class="chapter" data-level="7.8.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#long-term-effects"><i class="fa fa-check"></i><b>7.8.1</b> Long-Term Effects</a></li>
<li class="chapter" data-level="7.8.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#various-age-groups"><i class="fa fa-check"></i><b>7.8.2</b> Various Age Groups</a></li>
<li class="chapter" data-level="7.8.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#participants-with-health-problems"><i class="fa fa-check"></i><b>7.8.3</b> Participants with Health Problems</a></li>
<li class="chapter" data-level="7.8.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#individual-vs.group-setting"><i class="fa fa-check"></i><b>7.8.4</b> Individual vs. Group Setting</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#conclusion-3"><i class="fa fa-check"></i><b>7.9</b> Conclusion</a></li>
<li class="chapter" data-level="7.10" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#references-5"><i class="fa fa-check"></i><b>7.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html"><i class="fa fa-check"></i><b>8</b> Human Object Recognition and Computational Models</a><ul>
<li class="chapter" data-level="8.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#introduction-5"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#human-object-recognition-system"><i class="fa fa-check"></i><b>8.2</b> Human Object Recognition System</a><ul>
<li class="chapter" data-level="8.2.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-homology-of-human-and-macaques-visual-systems"><i class="fa fa-check"></i><b>8.2.1</b> The homology of human and macaque’s visual systems</a></li>
<li class="chapter" data-level="8.2.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#object-selective-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.2</b> Object-selective visual areas in the human brain</a></li>
<li class="chapter" data-level="8.2.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#facial-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.3</b> Facial visual areas in the human brain</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomena-of-interest-in-object-recognition"><i class="fa fa-check"></i><b>8.3</b> The behavioral phenomena of interest in object recognition</a></li>
<li class="chapter" data-level="8.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-other-race-effect"><i class="fa fa-check"></i><b>8.4</b> The behavioral phenomenon: Other-race effect</a><ul>
<li class="chapter" data-level="8.4.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model"><i class="fa fa-check"></i><b>8.4.1</b> Face space model</a></li>
<li class="chapter" data-level="8.4.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#perceptual-learning-theory"><i class="fa fa-check"></i><b>8.4.2</b> Perceptual learning theory</a></li>
<li class="chapter" data-level="8.4.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#neural-networks-evidence"><i class="fa fa-check"></i><b>8.4.3</b> Neural networks evidence</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-unfamiliar-face"><i class="fa fa-check"></i><b>8.5</b> The behavioral phenomenon: Unfamiliar face</a><ul>
<li class="chapter" data-level="8.5.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model-1"><i class="fa fa-check"></i><b>8.5.1</b> Face-space model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#how-we-deal-with-the-difficulties-of-computational-models"><i class="fa fa-check"></i><b>8.6</b> How we deal with the difficulties of computational models?</a><ul>
<li class="chapter" data-level="8.6.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#core-recognition"><i class="fa fa-check"></i><b>8.6.1</b> Core Recognition</a></li>
<li class="chapter" data-level="8.6.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#invariance-problem"><i class="fa fa-check"></i><b>8.6.2</b> Invariance problem</a></li>
<li class="chapter" data-level="8.6.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-explanation-of-it-neuronal-populations-on-object-recognition"><i class="fa fa-check"></i><b>8.6.3</b> The explanation of IT neuronal populations on object recognition</a></li>
<li class="chapter" data-level="8.6.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#shape-similarity-vs.semantic-category-information-in-it-neuronal-populations"><i class="fa fa-check"></i><b>8.6.4</b> Shape similarity vs. semantic category information in IT neuronal populations</a></li>
<li class="chapter" data-level="8.6.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#computational-models-accounting-for-the-it-representation"><i class="fa fa-check"></i><b>8.6.5</b> Computational models accounting for the IT representation</a></li>
<li class="chapter" data-level="8.6.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.6.6</b> 4.6. Deep Neural Networks</a></li>
<li class="chapter" data-level="8.6.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-advantages-of-hierarchical-features-in-computational-models"><i class="fa fa-check"></i><b>8.6.7</b> 4.7. The advantages of hierarchical features in computational models</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#conclusion-4"><i class="fa fa-check"></i><b>8.7</b> 5. Conclusion</a></li>
<li class="chapter" data-level="8.8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#references-6"><i class="fa fa-check"></i><b>8.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html"><i class="fa fa-check"></i><b>9</b> Language Acquisition and Machine Learning</a><ul>
<li class="chapter" data-level="9.1" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#introduction-6"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#speech-categorization-in-language-acquisition"><i class="fa fa-check"></i><b>9.2</b> Speech Categorization in language acquisition</a></li>
<li class="chapter" data-level="9.3" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#speech-categorization-as-a-machine-learning-task"><i class="fa fa-check"></i><b>9.3</b> Speech categorization as a Machine Learning task</a></li>
<li class="chapter" data-level="9.4" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#word-category-acquisition"><i class="fa fa-check"></i><b>9.4</b> Word Category Acquisition</a></li>
<li class="chapter" data-level="9.5" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#word-categorization-as-a-machine-learning-problem"><i class="fa fa-check"></i><b>9.5</b> Word Categorization as a Machine Learning Problem</a></li>
<li class="chapter" data-level="9.6" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#references-7"><i class="fa fa-check"></i><b>9.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-8.html"><a href="references-8.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Cognitive Technologies: From Theory and Data to Application</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sonification-and-augmented-cognition-a-brief-overview" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Sonification and augmented cognition: A brief overview</h1>
<p>Nicholaus P. Brosowsky, The Graduate Center of the City University of New York</p>
<p>In the most general sense, <em>sonification</em> refers to the transformation of non-sonic data into audible (non-speech) sound to represent or convey information to a listener. Thus, sonification is a rather general, all-encompassing umbrella term that might include everything from fire alarms, stethoscopes and Geiger counters, to Stravinsky’s <em>The Rite of Spring</em> and John Cage’s <em>4’33”</em>. Though there has been recent attempts to more firmly define sonification as a discipline (e.g., Hermann, Hunt, &amp; Neuhoff, 2011; Nees &amp; Walker, 2009), for better or worse, sonification straddles the boundaries of science, art, design, and application. To better situate this review, I will focus on the ways in which sonification could be used as a tool to enhance or augment cognition. Specifically, I focus on three areas: situational awareness, perception and action in motor skills, and data analysis. But before doing so, I will provide a brief outline of the types of sonification previous work has identified and the general rationale for adopting sonification methods at all.</p>
<div id="types-of-sonification." class="section level2">
<h2><span class="header-section-number">3.1</span> Types of sonification.</h2>
<p>One taxonomy of sonification (for others, see Fitch &amp; Kramer, 1994; Nees &amp; Walker, 2009), distinguishes between five functions of sonification: alerting, status indication, data exploration and, art and entertainment (for more in-depth reviews, see (Hermann et al., 2011; Walker &amp; Kramer, 2004).</p>
<p><em>Alerts</em> refer to sounds that notify the listener that an event has, or is about to occur, and that something in the environment requires their attention. These range from rather simple, low-information alerts like a door-bell, indicating someone is at the door; to more complex alerts that attempt to convey more information, like warning systems in a helicopter cockpit indicating a range of telemetry and avionics data (Edworthy, Hellier, Aldrich, &amp; Loxley, 2004) or forward collision systems in modern cars (P. Bazilinskyy, Petermeijer, Petrovych, Dodou, &amp; De Winter, 2015; Jamson, Lai, &amp; Carsten, 2008).</p>
<p>Closely related to the alerting function, is the <em>status or progress indicating function</em>. In this case a listener monitors a constant sound for small changes that indicate a change in status or progress update. For example, using auditory displays to monitor for changes in blood pressure (T. Watson &amp; Lip, 2006), internet network traffic (Debashi &amp; Vickers, 2018; Vickers, Laing, &amp; Fairfax, 2017), or telephone hold time (Garcia, Peres, Ritchey, Kortum, &amp; Stallmann, 2011; Kortum, Peres, Knott, &amp; Bushey, 2005).</p>
<p><em>Data exploration</em> is likely the function most closely associated with the term sonification. Simply put, sound is used to represent data in a way that enables the listener to recognize or search for patterns. This includes auditory graphs, created to summarize and communicate a set of data with known patterns (e.g., Flowers, 2005; Stockman, Nickerson, &amp; Hind, 2005; Walker &amp; Mauney, 2010), or as a way to explore more complex data sets to facilitate interpretation and exploratory analyses (e.g., Grond &amp; Hermann, 2014; Stanton, 2015). Data exploration and pattern recognition will be discussed in greater detail below, however data sonification has been used successfully across a range scientific disciplines from astronomy (Diaz Merced, 2013; W. L. Diaz-Merced et al., 2011) to the social sciences (Dayé &amp; de Campo, 2006). To give just one example, Pereverzev, Loshak, Backhaus, Davis, &amp; Packard (1997) used sonification methods to discover quantum oscillations between two weakly coupled reservoirs of superfluid helium 3, confirming previous theoretical predictions.</p>
<p>Finally, sonification can be used for <em>entertainment, art, sports, and leisure.</em> That is, sonification can be used for artistic expression and/or recreation. This final category includes the creation audio-only versions of games (i.e., sonified games) like Tower of Hanoi (Winberg &amp; Hellstrom, 2001) and Tic-Tac-Toe (Targett &amp; Fernstrom, 2003), as well as using sonified feedback to improve performance in sports like rowing (Dubus &amp; Bresin, 2015) and figure-skating (Boyd &amp; Godbout, 2010). Perhaps unsurprisingly, musical composition is another popular use of sonification methods. Here, often large data sets (e.g., weather changes, shark movements, seismic data) are mapped to musical representations to create works of art (e.g., Ballora, 2014; Parkinson &amp; Tanaka, 2013; Quinn, 2001, 2012). In one demonstration, for example, a century of weather data was transformed into compositions for cello and string quartets to describe and communicate climate change (George, Crawford, Reubold, &amp; Giorgi, 2017). In another, DNA sequences were used to compose music in an effort to summarize complex microbial ecology data (“Microbial Bebop”; Larsen, 2016). The line between artistic expression and data display is obviously blurred here. However, in these cases, the primary goal is to create an aesthetically pleasing work of art and communicating an interpretation of the data, if considered at all, is secondary.</p>
</div>
<div id="why-sonify-non-sonic-information" class="section level2">
<h2><span class="header-section-number">3.2</span> Why sonify non-sonic information?</h2>
<p>Since visual display has become the dominant form of communicating data, one might wonder why we would consider auditory display and sonification at all. This issues has been discussed fairly extensively in various contexts (e.g., Hermann et al., 2011; Kramer, 2000; Nees &amp; Walker, 2009; Sanderson, 2006). Briefly however, the auditory system excels and perhaps outperforms the visual system, in a number of important ways that are relevant to auditory display and sonification.</p>
<p>For one, the auditory system excels at detecting rhythmic and temporal changes. For example, we can perceptually separate two brief sounds like finger snap or metronome tick with as little as five milliseconds separating them; far better than the 35-40 milliseconds required by the visual system (Ashmead, Leroy, &amp; Odom, 1990; Gfeller, Woodworth, Robin, Witt, &amp; Knutson, 1997; Tervaniemi &amp; Brattico, 2004). Therefore, more information can be displayed in audition, compressed at a higher rate, and still maintain discriminability. Similarly, the auditory system is highly sensitive to temporal changes and pattern deviations (Escera, Alho, Winkler, &amp; Näätänen, 1998; Näätänen, Paavilainen, Rinne, &amp; Alho, 2007). As a result, auditory display may be well-suited to data sets that contain complex patterns and temporal changes.</p>
<p>More practically speaking, audition is omnidirectional, not requiring the listener to be oriented towards the display. This is especially important given that most of our primary tasks in our work environments are visual, restricting our ability to orient to other displays. Therefore, adding more visual information may be inappropriate because the visual system might already be occupied (Fitch &amp; Kramer, 1994; Wickens &amp; Liu, 1988) or, by adding additional visual displays, we may be overtaxing an already overburdened visual system (M. L. Brown, Newsome, &amp; Glinert, 1989). Additionally, we are able time-share multiple tasks more efficiently when they are presented in different modalities (Driver, 2001; Driver &amp; Spence, 1998; Wickens, 2002; Wickens, Parasuraman, &amp; Davies, 1984). Therefore, sonification presents an opportunity to present additional information and augment task performance without interfering with, or overloading the visual system.</p>
</div>
<div id="using-sonification-to-augment-cognition" class="section level2">
<h2><span class="header-section-number">3.3</span> Using sonification to augment cognition</h2>
<div id="perception-attention-and-situational-awareness" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Perception, attention, and situational awareness</h3>
<p>One way in which sonification could be used to enhance or augment cognition, is by improving situational awareness. Here I refer to situational awareness broadly, as maintaining conscious knowledge of the immediate environment and all the events happening within it. Our ability to maintain situational awareness, while obviously important for many tasks, is limited not only by what information is available in the environment, but also by our ability to process it (e.g., capacity limitations in working memory, attention, perception, etc.). Work in this area has demonstrated some success in improving situational awareness and task performance by using sonification to facilitate attention and perception processes.</p>
<p>Attention is perhaps the most obvious way sonification could be used to improve situational awareness, and the most easily demonstrated. Maintaining situational awareness in complex environments requires that we constantly monitor multiple streams of information. One obvious way to facilitate situational awareness is to offload the monitoring task using auditory alerts or alarms (Hermann et al., 2011; Nees &amp; Walker, 2009). The ubiquity of auditory alarms, from phone alerts to emergency vehicle sirens, makes it easy to over-look. However, they provide an easy way to offload what would be cognitively demanding task (i.e., vigilance or prospective memory), allowing the listener to engage in other tasks. The use of complex auditory alarms has proven useful in a range of settings and tasks including medical or patient monitoring (Cabrera, Ferguson, &amp; Laing, 2005), air-traffic controllers (Cabrera et al., 2005), and piloting aircraft (Edworthy et al., 2004).</p>
<p>Situational awareness in complex environments can be difficult because of the overwhelming amount of information and our limitations in dividing attention. Another way that sonification can aid situational awareness is by transforming multiple streams of information into a more useful, easier-to-manage format for real-time monitoring. There are two fields that have demonstrated the usefulness of sonification tools to facilitate situational awareness by overcoming limitations in divided attention: computer-network traffic monitoring and anesthesiology.</p>
<p>Computer network administrators must monitor flow of traffic in real-time to identify anomalous events like drops in traffic that may reflect hardware failures, or sudden increases in certain types of traffic that could reflect network intrusions (Axon, Alahmadi, Nurse, Goldsmith, &amp; Creese, 2018). Given the large amount of data the network receives every second, the data needs to be aggregated in a way that allows for real-time monitoring. Sonification tools have been shown to be useful for this purpose, demonstrating that listeners can detect network intrusions and anomalous changes in network activity using different sonification methods (e.g., Ballora, Giacobe, &amp; Hall, 2011; Debashi &amp; Vickers, 2018; Vickers, Laing, Debashi, &amp; Fairfax, 2014; Vickers et al., 2017). For example, Qi, Martin, Kapralos, Green, &amp; García-Ruiz (2007) mapped various network traffic data to piano sounds that allowed listeners to detect different types of network intrusions and Gilfix and Couch (2000) mapped network traffic to naturalistic sounds (e.g., chirping, heartbeats) which allowed listeners to detect anomalies in network traffic.</p>
<p>Similarly, anesthesiologists are faced with a similar problem. They need to monitor multiple streams of information about the patient in real-time (e.g., heart rate, central venous pressure, central artery pressure, etc.), often while time-sharing between other tasks. Work in this area tends to show that anesthesiologists and non-anesthesiologists can detect changes using auditory displays as good as when they used visual displays. However, they tend to time-share between tasks better when using an auditory display (Fitch &amp; Kramer, 1994; Loeb &amp; Fitch, 2002; Paterson, Sanderson, Paterson, &amp; Loeb, 2017; Seagull, Wickens, &amp; Loeb, 2001; M. Watson &amp; Sanderson, 2004).</p>
<p>Sonification can also improve situational awareness by augmenting perception. That is, sonification methods can be used to enhance the perceptual representation of our environment by providing extrasensory information. Many studies, for example, have focused on supplementing visual information for the blind using sonification. To aid in navigation, there has been success sonifying depth information (Brock &amp; Kristensson, 2013), and the location of objects (Pavlo Bazilinskyy et al., 2016), and even one demonstration of using echolocation (Kish, 2009). Others have shown success sonifying more complex visual information like object identity (Nagarajan, Yaacob, &amp; Sainarayanan, 2003) and line graphs (L. M. Brown &amp; Brewster, 2003).</p>
<p>However, there are other examples, where extrasensory information is sonified to enhance perception. Probably, the most well-known, and most-often cited example is the Geiger counter. Developed in the early 1900’s, and still used today, the Geiger counter transforms ionization events into audible clicks allowing us to perceive radiation levels in the environment (Knoll, 2010). Another example, called the “Visor” transposes color into sounds to create artificial synesthesia (Foner, 1999). Given our visual system, different sets of wavelengths can appear as the same color, assuming you adjust the relative amplitudes accordingly. Therefore, two objects could then appear to have the same color although they have different spectra; we can perceive the color, we cannot perceive the shape of the spectrum. The visor was designed to sonify the color spectra to enable the user to discriminate colors based on the shapes of the spectrum. For example, you could hear the difference between a painting and a copy of painting, even if visually they are indistinguishable, hear camouflaged objects, or as the authors suggest, the device could be extended to allow you to hear ultraviolet, infrared, or polarized light.</p>
</div>
<div id="perception-and-action-in-motor-skill-learning" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Perception and action in motor skill learning</h3>
<p>Another way in which sonification could augment cognition, is by improving perception and action in motor skill learning. That is, sound could be used to provide real-time feedback about performance in a motor task, guiding a learner towards their goal or correct performance. Enhancing motor learning has been explored using auditory alarms, sonified movement feedback, and sonified error feedback (J.F. Dyer, Stapleton, &amp; Rodger, 2017; Sigrist, Rauter, Riener, &amp; Wolf, 2013).</p>
<p>Auditory alarms have proven useful for improving motor skill learning. They are simplest form of sonification in that any movement considered an error triggers an alarm. They are easily interpreted by the learner, though they provide little information about how to correct performance. In rehabilitation, for example, auditory alarms have been used to inform patients about errors in movement (e.g., incorrect gait, unphysiological loading), and shown success in helping the learner correct the behavrio (Batavia, Gianutsos, Vaccaro, &amp; Gold, 2001; Eriksson &amp; Bresin, 2010; Petrofsky, 2001; Riskowski, Mikesky, Bahamonde, &amp; Burr, 2009). Similarly, auditory alarms have facilitated motor training in gymnastics (Baudry, Leroy, Thouvarecq, &amp; Chollet, 2006) and improving rifle movements for professional shooters (Underwood, 2009).</p>
<p>Sound has also been used to provide constant, real-time feedback about movement. This is considered ‘direct sonification’ because some body movement is directly mapped to sound to provide additional information and guide the learner to correct performance. For example, your location is 3D space could be mapped to amplitude and pitch of a constant sound helping you navigate through space. There is some evidence that continuous sonified feedback is beneficial in simple motor tasks; In simple reaching tasks, for example (Oscari, Secoli, Avanzini, Rosati, &amp; Reinkensmeyer, 2012; Schmitz &amp; Bock, 2014). Unfortunately, however, there is little direct evidence that continuous auditory feedback is beneficial in complex motor tasks. There was some success using sonified movement feedback in swimming tasks (Chollet, Madani, &amp; Micallef, 1992; Chollet, Micallef, &amp; Rabischong, 1988), although these effects might be explained better by increased motivation (Sigrist et al., 2013). Constant sonified movement feedback has also been incorporated in a number of different motor tasks like karate (Yamamoto, Shiraki, Takahata, Sakane, &amp; Takebayashi, 2004), rowing (Schaffert, Mattes, &amp; Effenberg, 2009), and skiing (Kirby, 2009), but there have not been corresponding motor learning studies to validate whether they are in fact beneficial for the learner (Sigrist et al., 2013).</p>
<p>There has been more success in using sonified movement error feedback to improve motor-skill learning (Oscari et al., 2012; Schmitz &amp; Bock, 2014). Here, the sound does not directly correspond to your movements, but instead corresponds to your movements in relation to some criterion. For example, instead of directly mapping sound to your location in 3D space, you could map sound parameters to the relationship between your position and some target location (e.g., increase in pitch as you move closer to the target). Using this method has shown some benefits across different complex motor tasks such as speed skating (Boyd &amp; Godbout, 2010) and rowing (Sigrist et al., 2011). Shooting scores during rifle training was also improved with error feedback. Here, the pitch of a pure tone was mapped to the deviation of the gun barrel to the bullseye.</p>
</div>
<div id="data-analysis-and-pattern-recognition." class="section level3">
<h3><span class="header-section-number">3.3.3</span> Data analysis and pattern recognition.</h3>
<p>One of the goals of datamining or data exploration is to detect hidden regularities in high dimensional data. Our ability to detect these hidden regularities is of course dependent on the representation of the data and our ability to recognize the patterns. As mentioned earlier, our auditory system excels at detecting very subtle patterns in sounds (Grond &amp; Hermann, 2014a, 2014b; Hermann et al., 2011). The use of auditory data representations in fact has a long history, well-before there was a term for it (see Frysinger, 2005). The stethoscope, for example, still provides valuable information for a physician, and Pollack and Ficks (1954) mapped multi-dimensional data onto sound parameters to evaluate the information transmission properties of auditory stimuli (i.e., information “bits”).</p>
<p>Speeth (1961), provided one of the earliest studies that showed the advantages of using auditory data representations over visual for data pattern recognition. Here they were interested in using seismic measurements to discriminate between earthquakes and underground bomb blasts. The seismometer produces complex wave patterns and using visual displays of the data for categorization proved to be a very difficult task. However, once the seismic data transformed into sound, subjects could accurately classify seismic activity on 90% of the trials. Additionally, because the data was time compressed, an analyst could review up to 24 hours of data in 5 minutes.</p>
<p>Other early work has also shown the advantages to using auditory representations when dealing with complex multivariate data. Morrison and Lunney used sound to represent infrared spectral data (Baecker &amp; Buxton, 1987) and Yeung (1980) used sound to represent experimental data from analytical chemistry where subjects achieved 98% classification with little practice. Similarly, Mezrich, Frysinger, &amp; Slivjanovski (1984) used both auditory and visual components to represent multivariate time-series economic data. They found that their dynamic multi-modal display generally outperformed static visual displays.</p>
<p>This early work is important in that it demonstrates that some data sets are well-suited for sonification and confers pattern recognition benefits. These are often dense, multivariate data sets that can take advantage of the temporal nature of auditory representations. More recent work has expanded the range of applications of sonification for data exploration with some notable successes.</p>
<p>One area that has shown the usefulness of sonification is in the interpretation of brain data. For example, real-time monitoring and analysis of ectroencephalographic (EEG) data has diverse application areas including medical screening, brain computer interfaces, and neurofeedback (Väljamäe et al., 2013). Recent work shows that sonification facilitates the interpretation and categorization of EEG data (Baier, Hermann, &amp; Stephani, 2007; Baier et al., 2007; De Campo, Hoeldrich, Eckel, &amp; Wallisch, 2007). For example, sonified EEG data has been used to detect epilectic seizures. One study transformed EEG data into music (snapping time-frequency data to notes in a musical scale) and found that subjects could identify seizures from the auditory data alone (Loui, Koplin-Green, Frick, &amp; Massone, 2014; Parvizi, Gururangan, Razavi, &amp; Chafe, 2018). Similarly, positron emission topographical (PET) data has been sonified to facilitate the diagnosis of Alzheimer’s disease (Gionfrida &amp; Roginska, 2017). Not limited to brain data, other biomedical signals like electrocardiographic (ECG) data have been sonified facilitating the detection of cardiopathic pathologies and other anomalies (Avbelj, 2012; Kather et al., 2017).</p>
<p>The range of fields that have begun to adopt sonification for data exploration, and have shown promising results, is in fact staggeringly diverse. From astronomical data (W. L. Diaz-Merced et al., 2011; W. L. L. Diaz-Merced, 2017; Lunn &amp; Hunt, 2011), meterological data (George et al., 2017), oceanography (Sturm, 2005), physics (Pereverzev et al., 1997), biomedicine (Avbelj, 2012; Larsen, 2016), social sciences (Dayé &amp; de Campo, 2006), to space exploration. During the Voyager 2 mission, the spacecraft was going through the rings of Saturn when it encountered a problem. The operators could not identify the problem through visual analysis of the data. However, once the data was played through a music synthesizer, a “machine-gunning” sound was heard, leading them to conclude that the problem was caused the problems were caused by high-speed collisions with electromagnetically charged micro-meteoroids (Barrass &amp; Kramer, 1999). Sonification can alter our perception of the data allowing insights and pattern recognition that were not possible using visual displays.</p>
</div>
</div>
<div id="references-1" class="section level2">
<h2><span class="header-section-number">3.4</span> References</h2>
<p>Ashmead, D. H., Leroy, D., &amp; Odom, R. D. (1990). Perception of the relative distances of nearby sound sources. <em>Perception &amp; Psychophysics</em>, <em>47</em>(4), 326–331.</p>
<p>Avbelj, V. (2012). Auditory display of biomedical signals through a sonic representation: ECG and EEG sonification. In <em>MIPRO, 2012 Proceedings of the 35th International Convention</em> (pp. 474–475). IEEE.</p>
<p>Axon, L., Alahmadi, B., Nurse, J. R., Goldsmith, M., &amp; Creese, S. (2018). Sonification in security operations centres: what do security practitioners think? <em>Analyst</em>, <em>7</em>, 3.</p>
<p>Baecker, R. M., &amp; Buxton, W. A. (1987). <em>Human-computer interaction: a multidisciplinary approach</em>. Morgan Kaufmann Publishers Inc.</p>
<p>Baier, G., Hermann, T., &amp; Stephani, U. (2007). Event-based sonification of EEG rhythms in real time. <em>Clinical Neurophysiology</em>, <em>118</em>(6), 1377–1386.</p>
<p>Ballora, M. (2014). Sonification, Science and Popular Music: In search of the ‘wow.’ <em>Organised Sound</em>, <em>19</em>(01), 30–40. <a href="https://doi.org/10.1017/S1355771813000381" class="uri">https://doi.org/10.1017/S1355771813000381</a></p>
<p>Ballora, M., Giacobe, N. A., &amp; Hall, D. L. (2011). Songs of cyberspace: an update on sonifications of network traffic to support situational awareness. In <em>Multisensor, Multisource Information Fusion: Architectures, Algorithms, and Applications 2011</em> (Vol. 8064, p. 80640P). International Society for Optics and Photonics.</p>
<p>Barrass, S., &amp; Kramer, G. (1999). Using sonification. <em>Multimedia Systems</em>, <em>7</em>(1), 23–31.</p>
<p>Batavia, M., Gianutsos, J. G., Vaccaro, A., &amp; Gold, J. T. (2001). A do-it-yourself membrane-activated auditory feedback device for weight bearing and gait training: a case report. <em>Archives of Physical Medicine and Rehabilitation</em>, <em>82</em>(4), 541–545.</p>
<p>Baudry, L., Leroy, D., Thouvarecq, R., &amp; Chollet, D. (2006). Auditory concurrent feedback benefits on the circle performed in gymnastics. <em>Journal of Sports Sciences</em>, <em>24</em>(2), 149–156.</p>
<p>Bazilinskyy, P., Petermeijer, S. M., Petrovych, V., Dodou, D., &amp; De Winter, J. C. F. (2015). <em>Take-over requests in highly automated driving: A crowdsourcing survey on auditory, vibrotactile, and visual displays</em>. Unpublished.</p>
<p>Bazilinskyy, Pavlo, van Haarlem, W., Quraishi, H., Berssenbrugge, C., Binda, J., &amp; de Winter, J. (2016). Sonifying the location of an object: A comparison of three methods. <em>IFAC-PapersOnLine</em>, <em>49</em>(19), 531–536.</p>
<p>Boyd, J., &amp; Godbout, A. (2010). Corrective Sonic Feedback for Speed Skating: A Case Study. Georgia Institute of Technology.</p>
<p>Brock, M., &amp; Kristensson, P. O. (2013). Supporting blind navigation using depth sensing and sonification. In <em>Proceedings of the 2013 ACM conference on Pervasive and ubiquitous computing adjunct publication</em> (pp. 255–258). ACM.</p>
<p>Brown, L. M., &amp; Brewster, S. A. (2003). Drawing by ear: Interpreting sonified line graphs. Georgia Institute of Technology.</p>
<p>Brown, M. L., Newsome, S. L., &amp; Glinert, E. P. (1989). An experiment into the use of auditory cues to reduce visual workload. In <em>ACM SIGCHI Bulletin</em> (Vol. 20, pp. 339–346). ACM.</p>
<p>Cabrera, D., Ferguson, S., &amp; Laing, G. (2005). Development of auditory alerts for air traffic control consoles. In <em>Audio Engineering Society Convention 119</em>. Audio Engineering Society.</p>
<p>Chollet, D., Madani, M., &amp; Micallef, J. P. (1992). Effects of two types of biomechanical bio-feedback on crawl performance. <em>Biomechanics and Medicine in Swimming, Swimming Science VI</em>, <em>48</em>, 53.</p>
<p>Chollet, D., Micallef, J. P., &amp; Rabischong, P. (1988). Biomechanical signals for external biofeedback to improve swimming techniques. <em>Swimming Science V. Champaign, IL: Human Kinetics Books</em>, 389–396.</p>
<p>Dayé, C., &amp; de Campo, A. (2006). Sounds sequential: sonification in the social sciences. <em>Interdisciplinary Science Reviews</em>, <em>31</em>(4), 349–364. <a href="https://doi.org/10.1179/030801806X143286" class="uri">https://doi.org/10.1179/030801806X143286</a></p>
<p>De Campo, A., Hoeldrich, R., Eckel, G., &amp; Wallisch, A. (2007). New sonification tools for EEG data screening and monitoring. Georgia Institute of Technology.</p>
<p>Debashi, M., &amp; Vickers, P. (2018). Sonification of network traffic flow for monitoring and situational awareness. <em>PLOS ONE</em>, <em>13</em>(4), e0195948. <a href="https://doi.org/10.1371/journal.pone.0195948" class="uri">https://doi.org/10.1371/journal.pone.0195948</a></p>
<p>Diaz Merced, W. L. (2013). <em>Sound for the exploration of space physics data</em> (PhD). University of Glasgow. Retrieved from <a href="http://encore.lib.gla.ac.uk/iii/encore/record/C\_\_Rb3090263" class="uri">http://encore.lib.gla.ac.uk/iii/encore/record/C\_\_Rb3090263</a></p>
<p>Diaz-Merced, W. L., Candey, R. M., Brickhouse, N., Schneps, M., Mannone, J. C., Brewster, S., &amp; Kolenberg, K. (2011). Sonification of Astronomical Data. <em>Proceedings of the International Astronomical Union</em>, <em>7</em>(S285), 133–136. <a href="https://doi.org/10.1017/S1743921312000440" class="uri">https://doi.org/10.1017/S1743921312000440</a></p>
<p>Diaz-Merced, W. L. L. (2017). We too may find new planets. In <em>AASTCS5 Radio Exploration of Planetary Habitability, Proceedings of the conference 7-12 May, 2017 in Palm Springs, CA. Published in Bulletin of the American Astronomical Society, Vol. 49, No. 3, id. 202.01</em> (Vol. 49).</p>
<p>Driver, J. (2001). A selective review of selective attention research from the past century. <em>British Journal of Psychology</em>, <em>92</em>(1), 53–78.</p>
<p>Driver, J., &amp; Spence, C. (1998). Crossmodal attention. <em>Current Opinion in Neurobiology</em>, <em>8</em>(2), 245–253. <a href="https://doi.org/10.1016/S0959-4388(98)80147-5" class="uri">https://doi.org/10.1016/S0959-4388(98)80147-5</a></p>
<p>Dubus, G., &amp; Bresin, R. (2015). Exploration and evaluation of a system for interactive sonification of elite rowing. <em>Sports Engineering</em>, <em>18</em>(1), 29–41. <a href="https://doi.org/10.1007/s12283-014-0164-0" class="uri">https://doi.org/10.1007/s12283-014-0164-0</a></p>
<p>Dyer, J. F., Stapleton, P., &amp; Rodger, M. (2017). Mapping Sonification for Perception and Action in Motor Skill Learning. <em>Frontiers in Neuroscience</em>, <em>11</em>. <a href="https://doi.org/10.3389/fnins.2017.00463" class="uri">https://doi.org/10.3389/fnins.2017.00463</a></p>
<p>Edworthy, J., Hellier, E., Aldrich, K., &amp; Loxley, S. (2004). Designing trend-monitoring sounds for helicopters: methodological issues and an application. <em>Journal of Experimental Psychology: Applied</em>, <em>10</em>(4), 203.</p>
<p>Eriksson, M., &amp; Bresin, R. (2010). Improving running mechanics by use of interactive sonification. <em>Proceedings of ISon</em>, 95–98.</p>
<p>Escera, C., Alho, K., Winkler, I., &amp; Näätänen, R. (1998). Neural mechanisms of involuntary attention to acoustic novelty and change. <em>Journal of Cognitive Neuroscience</em>, <em>10</em>(5), 590–604.</p>
<p>Fitch, W. T., &amp; Kramer, G. (1994). Sonifying the body electric: Superiority of an auditory over a visual display in a complex, multivariate system. In <em>SANTA FE INSTITUTE STUDIES IN THE SCIENCES OF COMPLEXITY-PROCEEDINGS VOLUME-</em> (Vol. 18, pp. 307–307). Addison-Wesley Publishing Co.</p>
<p>Flowers, J. H. (2005). Thirteen years of reflection on auditory graphing: Promises, pitfalls, and potential new directions. Georgia Institute of Technology.</p>
<p>Foner, L. N. (1999). Artificial synesthesia via sonification: A wearable augmented sensory system. <em>Mobile Networks and Applications</em>, <em>4</em>(1), 75–81.</p>
<p>Frysinger, S. P. (2005). A brief history of auditory data representation to the 1980s. Georgia Institute of Technology.</p>
<p>Garcia, A., Peres, S. C., Ritchey, P., Kortum, P., &amp; Stallmann, K. (2011). Auditory Progress Bars: Estimations of Time Remaining. <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, <em>55</em>(1), 1338–1341. <a href="https://doi.org/10.1177/1071181311551278" class="uri">https://doi.org/10.1177/1071181311551278</a></p>
<p>George, S. S., Crawford, D., Reubold, T., &amp; Giorgi, E. (2017). Making Climate Data Sing: Using Music-like Sonifications to Convey a Key Climate Record. <em>Bulletin of the American Meteorological Society</em>, <em>98</em>(1), 23–27.</p>
<p>Gfeller, K., Woodworth, G., Robin, D. A., Witt, S., &amp; Knutson, J. F. (1997). Perception of rhythmic and sequential pitch patterns by normally hearing adults and adult cochlear implant users. <em>Ear and Hearing</em>, <em>18</em>(3), 252–260.</p>
<p>Gilfix, M., &amp; Couch, A. L. (2000). Peep (The Network Auralizer): Monitoring Your Network with Sound. In <em>LISA</em> (pp. 109–117).</p>
<p>Gionfrida, L., &amp; Roginska, A. (2017). A Novel Sonification Approach to Support the Diagnosis of Alzheimer’s Dementia. <em>Frontiers in Neurology</em>, <em>8</em>, 647. <a href="https://doi.org/10.3389/fneur.2017.00647" class="uri">https://doi.org/10.3389/fneur.2017.00647</a></p>
<p>Grond, F., &amp; Hermann, T. (2014a). Interactive Sonification for Data Exploration: How listening modes and display purposes define design guidelines. <em>Organised Sound</em>, <em>19</em>(1), 41–51.</p>
<p>Grond, F., &amp; Hermann, T. (2014b). Interactive Sonification for Data Exploration: How listening modes and display purposes define design guidelines. <em>Organised Sound</em>, <em>19</em>(01), 41–51. <a href="https://doi.org/10.1017/S1355771813000393" class="uri">https://doi.org/10.1017/S1355771813000393</a></p>
<p>Hermann, T., Hunt, A., &amp; Neuhoff, J. G. (2011). <em>The sonification handbook</em>. Logos Verlag Berlin.</p>
<p>Jamson, A. H., Lai, F. C., &amp; Carsten, O. M. (2008). Potential benefits of an adaptive forward collision warning system. <em>Transportation Research Part C: Emerging Technologies</em>, <em>16</em>(4), 471–484.</p>
<p>Kather, J. N., Hermann, T., Bukschat, Y., Kramer, T., Schad, L. R., &amp; Zöllner, F. G. (2017). Polyphonic sonification of electrocardiography signals for diagnosis of cardiac pathologies. <em>Scientific Reports</em>, <em>7</em>, 44549. <a href="https://doi.org/10.1038/srep44549" class="uri">https://doi.org/10.1038/srep44549</a></p>
<p>Kirby, R. (2009). Development of a real-time performance measurement and feedback system for alpine skiers. <em>Sports Technology</em>, <em>2</em>(1–2), 43–52.</p>
<p>Kish, D. (2009, April 11). Seeing with sound: What is it like to “see” the world using sonar? Daniel Kish, who lost his sight in infancy, reveals all. <em>New Scientist</em>.</p>
<p>Knoll, G. F. (2010). <em>Radiation detection and measurement</em>. John Wiley &amp; Sons.</p>
<p>Kortum, P., Peres, S. C., Knott, B. A., &amp; Bushey, R. (2005). The Effect of Auditory Progress Bars on Consumer’s Estimation of Telephone wait Time. <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, <em>49</em>(4), 628–632. <a href="https://doi.org/10.1177/154193120504900406" class="uri">https://doi.org/10.1177/154193120504900406</a></p>
<p>Kramer, G. (2000). <em>Auditory display: sonification, audification and auditory interfaces</em>. Addison-Wesley Longman Publishing Co., Inc.</p>
<p>Larsen, P. E. (2016). More of an art than a science: Using microbial DNA sequences to compose music. <em>Journal of Microbiology &amp; Biology Education</em>, <em>17</em>(1), 129.</p>
<p>Loeb, R. G., &amp; Fitch, W. T. (2002). A laboratory evaluation of an auditory display designed to enhance intraoperative monitoring. <em>Anesthesia &amp; Analgesia</em>, <em>94</em>(2), 362–368.</p>
<p>Loui, P., Koplin-Green, M., Frick, M., &amp; Massone, M. (2014). Rapidly Learned Identification of Epileptic Seizures from Sonified EEG. <em>Frontiers in Human Neuroscience</em>, <em>8</em>. <a href="https://doi.org/10.3389/fnhum.2014.00820" class="uri">https://doi.org/10.3389/fnhum.2014.00820</a></p>
<p>Lunn, P., &amp; Hunt, A. (2011). Listening to the invisible: Sonification as a tool for astronomical discovery.</p>
<p>Mezrich, J. J., Frysinger, S., &amp; Slivjanovski, R. (1984). Dynamic representation of multivariate time series data. <em>Journal of the American Statistical Association</em>, <em>79</em>(385), 34–40.</p>
<p>Näätänen, R., Paavilainen, P., Rinne, T., &amp; Alho, K. (2007). The mismatch negativity (MMN) in basic research of central auditory processing: A review. <em>Clinical Neurophysiology</em>, <em>118</em>(12), 2544–2590. <a href="https://doi.org/10.1016/j.clinph.2007.04.026" class="uri">https://doi.org/10.1016/j.clinph.2007.04.026</a></p>
<p>Nagarajan, R., Yaacob, S., &amp; Sainarayanan, G. (2003). Role of object identification in sonification system for visually impaired. In <em>TENCON 2003. Conference on Convergent Technologies for the Asia-Pacific Region</em> (Vol. 2, pp. 735–739). IEEE.</p>
<p>Nees, M. A., &amp; Walker, B. N. (2009). <em>Auditory Interfaces and Sonification.</em></p>
<p>Oscari, F., Secoli, R., Avanzini, F., Rosati, G., &amp; Reinkensmeyer, D. J. (2012). Substituting auditory for visual feedback to adapt to altered dynamic and kinematic environments during reaching. <em>Experimental Brain Research</em>, <em>221</em>(1), 33–41.</p>
<p>Parkinson, A., &amp; Tanaka, A. (2013). Making Data Sing: Embodied Approaches to Sonification. In <em>Sound, Music, and Motion</em> (pp. 151–160). Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-12976-1\_9" class="uri">https://doi.org/10.1007/978-3-319-12976-1\_9</a></p>
<p>Parvizi, J., Gururangan, K., Razavi, B., &amp; Chafe, C. (2018). Detecting silent seizures by their sound. <em>Epilepsia</em>, <em>59</em>(4), 877–884.</p>
<p>Paterson, E., Sanderson, P. M., Paterson, N. a. B., &amp; Loeb, R. G. (2017). Effectiveness of enhanced pulse oximetry sonifications for conveying oxygen saturation ranges: a laboratory comparison of five auditory displays. <em>British Journal of Anaesthesia</em>, <em>119</em>(6), 1224–1230. <a href="https://doi.org/10.1093/bja/aex343" class="uri">https://doi.org/10.1093/bja/aex343</a></p>
<p>Pereverzev, S. V., Loshak, A., Backhaus, S., Davis, J. C., &amp; Packard, R. E. (1997). Quantum oscillations between two weakly coupled reservoirs of superfluid 3 He. <em>Nature</em>, <em>388</em>(6641), 449.</p>
<p>Petrofsky, J. (2001). The use of electromyogram biofeedback to reduce Trendelenburg gait. <em>European Journal of Applied Physiology</em>, <em>85</em>(5), 491–495.</p>
<p>Pollack, I., &amp; Ficks, L. (1954). Information of elementary multidimensional auditory displays. <em>The Journal of the Acoustical Society of America</em>, <em>26</em>(2), 155–158.</p>
<p>Qi, L., Martin, M. V., Kapralos, B., Green, M., &amp; García-Ruiz, M. (2007). Toward sound-assisted intrusion detection systems. In *OTM Confederated International Conferences&quot; On the Move to Meaningful Internet Systems“* (pp. 1634–1645). Springer.</p>
<p>Quinn, M. (2001). Research set to music: The climate symphony and other sonifications of ice core, radar, DNA, seismic and solar wind data. Georgia Institute of Technology.</p>
<p>Quinn, M. (2012). “Walk on the Sun”: an interactive image sonification exhibit. <em>AI &amp; Society</em>, <em>27</em>(2), 303–305.</p>
<p>Riskowski, J. L., Mikesky, A. E., Bahamonde, R. E., &amp; Burr, D. B. (2009). Design and validation of a knee brace with feedback to reduce the rate of loading. <em>Journal of Biomechanical Engineering</em>, <em>131</em>(8), 084503.</p>
<p>Sanderson, P. (2006). The multimodal world of medical monitoring displays. <em>Applied Ergonomics</em>, <em>37</em>(4), 501–512.</p>
<p>Schaffert, N., Mattes, K., &amp; Effenberg, A. O. (2009). A sound design for the purposes of movement optimisation in elite sport (using the example of rowing). Georgia Institute of Technology.</p>
<p>Schmitz, G., &amp; Bock, O. (2014). A Comparison of Sensorimotor Adaptation in the Visual and in the Auditory Modality. <em>PloS One</em>, <em>9</em>(9), e107834.</p>
<p>Seagull, F. J., Wickens, C. D., &amp; Loeb, R. G. (2001). When is less more? Attention and workload in auditory, visual, and redundant patient-monitoring conditions. In <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em> (Vol. 45, pp. 1395–1399). SAGE Publications Sage CA: Los Angeles, CA.</p>
<p>Sigrist, R., Rauter, G., Riener, R., &amp; Wolf, P. (2013). Augmented visual, auditory, haptic, and multimodal feedback in motor learning: A review. <em>Psychonomic Bulletin &amp; Review</em>, <em>20</em>(1), 21–53. <a href="https://doi.org/10.3758/s13423-012-0333-8" class="uri">https://doi.org/10.3758/s13423-012-0333-8</a></p>
<p>Sigrist, R., Schellenberg, J., Rauter, G., Broggi, S., Riener, R., &amp; Wolf, P. (2011). Visual and auditory augmented concurrent feedback in a complex motor task. <em>Presence: Teleoperators and Virtual Environments</em>, <em>20</em>(1), 15–32.</p>
<p>Speeth, S. D. (1961). Seismometer sounds. <em>The Journal of the Acoustical Society of America</em>, <em>33</em>(7), 909–916.</p>
<p>Stanton, J. (2015). Sensing big data: Multimodal information interfaces for exploration of large data sets. In <em>Big Data at Work</em> (pp. 172–192). Routledge.</p>
<p>Stockman, T., Nickerson, L. V., &amp; Hind, G. (2005). Auditory graphs: A summary of current experience and towards a research agenda. Georgia Institute of Technology.</p>
<p>Sturm, B. L. (2005). Pulse of an Ocean: Sonification of Ocean Buoy Data. <em>Leonardo</em>, <em>38</em>(2), 143–149.</p>
<p>Targett, S., &amp; Fernstrom, M. (2003). Audio games: Fun for all? All for fun! Georgia Institute of Technology.</p>
<p>Tervaniemi, M., &amp; Brattico, E. (2004). From sounds to music towards understanding the neurocognition of musical sound perception. <em>Journal of Consciousness Studies</em>, <em>11</em>(3–4), 9–27.</p>
<p>Underwood, S. M. (2009). Effects of augmented real-time auditory feedback on top-level precision shooting performance.</p>
<p>Väljamäe, A., Steffert, T., Holland, S., Marimon, X., Benitez, R., Mealla, S., … Jordà, S. (2013). A review of real-time EEG sonification research (pp. 85–93). Presented at the International Conference on Auditory Display 2013 (ICAD 2013), Lodz, Poland. Retrieved from <a href="http://icad2013.com/index.php" class="uri">http://icad2013.com/index.php</a></p>
<p>Vickers, P., Laing, C., Debashi, M., &amp; Fairfax, T. (2014). Sonification Aesthetics and Listening for Network Situational Awareness. <em>ArXiv:1409.5282 [Cs]</em>. <a href="https://doi.org/10.13140/2.1.4225.6648" class="uri">https://doi.org/10.13140/2.1.4225.6648</a></p>
<p>Vickers, P., Laing, C., &amp; Fairfax, T. (2017). Sonification of a network’s self-organized criticality for real-time situational awareness. <em>Displays</em>, <em>47</em>, 12–24. <a href="https://doi.org/10.1016/j.displa.2016.05.002" class="uri">https://doi.org/10.1016/j.displa.2016.05.002</a></p>
<p>Walker, B. N., &amp; Kramer, G. (2004). Ecological psychoacoustics and auditory displays: Hearing, grouping, and meaning making. <em>Ecological Psychoacoustics</em>, 150–175.</p>
<p>Walker, B. N., &amp; Mauney, L. M. (2010). Universal design of auditory graphs: A comparison of sonification mappings for visually impaired and sighted listeners. <em>ACM Transactions on Accessible Computing (TACCESS)</em>, <em>2</em>(3), 12.</p>
<p>Watson, M., &amp; Sanderson, P. (2004). Sonification supports eyes-free respiratory monitoring and task time-sharing. <em>Human Factors</em>, <em>46</em>(3), 497–517.</p>
<p>Watson, T., &amp; Lip, G. Y. H. (2006). Blood pressure measurement in atrial fibrillation: goodbye mercury? <em>Journal of Human Hypertension</em>, <em>20</em>(9), 638.</p>
<p>Wickens, C. D. (2002). Multiple resources and performance prediction. <em>Theoretical Issues in Ergonomics Science</em>, <em>3</em>(2), 159–177.</p>
<p>Wickens, C. D., &amp; Liu, Y. (1988). Codes and modalities in multiple resources: A success and a qualification. <em>Human Factors</em>, <em>30</em>(5), 599–616.</p>
<p>Wickens, C. D., Parasuraman, R., &amp; Davies, D. R. (1984). Varieties of attention.</p>
<p>Winberg, F., &amp; Hellstrom, S. O. (2001). Qualitative aspects of auditory direct manipulation. A case study of the towers of Hanoi. Georgia Institute of Technology.</p>
<p>Yamamoto, G., Shiraki, K., Takahata, M., Sakane, Y., &amp; Takebayashi, Y. (2004). Multimodal knowledge for designing new sound environments. In <em>The International Conference on Human Computer Interaction with Mobile Devices and Services</em>.</p>
<p>Yeung, E. S. (1980). Pattern recognition by audio representation of multivariate analytical data. <em>Analytical Chemistry</em>, <em>52</em>(7), 1120–1123.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/CrumpLab/cognitivetechnologies/tree/master/book/03-Brosowsky.Rmd",
"text": "Edit"
},
"download": ["cog_tech.pdf", "cog_tech.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
