<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Cognitive Technologies: From Theory and Data to Application</title>
  <meta name="description" content="Papers on assorted cognitive technologies">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Cognitive Technologies: From Theory and Data to Application" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Papers on assorted cognitive technologies" />
  <meta name="github-repo" content="CrumpLab/cognitivetechnologies" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Cognitive Technologies: From Theory and Data to Application" />
  
  <meta name="twitter:description" content="Papers on assorted cognitive technologies" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="human-object-recognition-and-computational-models.html">
<link rel="next" href="references-8.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Cognitive Technologies</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html"><i class="fa fa-check"></i><b>1</b> Reflections on our tour of Cognitive Technologies</a><ul>
<li class="chapter" data-level="1.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#snake-oil-an-old-technology"><i class="fa fa-check"></i><b>1.1</b> Snake oil: An old technology</a></li>
<li class="chapter" data-level="1.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#tech-that-works"><i class="fa fa-check"></i><b>1.2</b> Tech that works</a></li>
<li class="chapter" data-level="1.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#connection-to-instance-theory"><i class="fa fa-check"></i><b>1.3</b> Connection to Instance Theory</a><ul>
<li class="chapter" data-level="1.3.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#procedures-of-mind"><i class="fa fa-check"></i><b>1.3.1</b> Procedures of Mind</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#exciting-directions"><i class="fa fa-check"></i><b>1.4</b> Exciting Directions</a><ul>
<li class="chapter" data-level="1.4.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#conversational-ai"><i class="fa fa-check"></i><b>1.4.1</b> Conversational AI</a></li>
<li class="chapter" data-level="1.4.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#decoding-brain-states"><i class="fa fa-check"></i><b>1.4.2</b> Decoding Brain states</a></li>
<li class="chapter" data-level="1.4.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#detecting-deception"><i class="fa fa-check"></i><b>1.4.3</b> Detecting Deception</a></li>
<li class="chapter" data-level="1.4.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#inner-voice-decoding-with-a-chinstrap"><i class="fa fa-check"></i><b>1.4.4</b> Inner Voice decoding with a chinstrap!</a></li>
<li class="chapter" data-level="1.4.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#image-memorability"><i class="fa fa-check"></i><b>1.4.5</b> Image Memorability</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#thats-all"><i class="fa fa-check"></i><b>1.5</b> That’s all</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><i class="fa fa-check"></i><b>2</b> Computational Classification Techniques for Biomedical and Clinical Big Data</a><ul>
<li class="chapter" data-level="2.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#abstract"><i class="fa fa-check"></i><b>2.1</b> Abstract</a></li>
<li class="chapter" data-level="2.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#introduction"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#previous-work"><i class="fa fa-check"></i><b>2.3</b> Previous Work</a><ul>
<li class="chapter" data-level="2.3.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#topic-modeling"><i class="fa fa-check"></i><b>2.3.1</b> Topic Modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#neural-networks"><i class="fa fa-check"></i><b>2.3.2</b> Neural Networks</a></li>
<li class="chapter" data-level="2.3.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clustering"><i class="fa fa-check"></i><b>2.3.3</b> Clustering</a></li>
<li class="chapter" data-level="2.3.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#word-sense-disambiguation"><i class="fa fa-check"></i><b>2.3.4</b> Word Sense Disambiguation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#medical-and-clinical-applications"><i class="fa fa-check"></i><b>2.4</b> Medical and Clinical Applications</a></li>
<li class="chapter" data-level="2.5" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clerical-applications"><i class="fa fa-check"></i><b>2.5</b> Clerical Applications</a></li>
<li class="chapter" data-level="2.6" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#real-world-applications-and-future-work"><i class="fa fa-check"></i><b>2.6</b> Real World Applications and Future Work</a></li>
<li class="chapter" data-level="2.7" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
<li class="chapter" data-level="2.8" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#references"><i class="fa fa-check"></i><b>2.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html"><i class="fa fa-check"></i><b>3</b> Sonification and augmented cognition: A brief overview</a><ul>
<li class="chapter" data-level="3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#types-of-sonification."><i class="fa fa-check"></i><b>3.1</b> Types of sonification.</a></li>
<li class="chapter" data-level="3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#why-sonify-non-sonic-information"><i class="fa fa-check"></i><b>3.2</b> Why sonify non-sonic information?</a></li>
<li class="chapter" data-level="3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#using-sonification-to-augment-cognition"><i class="fa fa-check"></i><b>3.3</b> Using sonification to augment cognition</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-attention-and-situational-awareness"><i class="fa fa-check"></i><b>3.3.1</b> Perception, attention, and situational awareness</a></li>
<li class="chapter" data-level="3.3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-and-action-in-motor-skill-learning"><i class="fa fa-check"></i><b>3.3.2</b> Perception and action in motor skill learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#data-analysis-and-pattern-recognition."><i class="fa fa-check"></i><b>3.3.3</b> Data analysis and pattern recognition.</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#references-1"><i class="fa fa-check"></i><b>3.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><i class="fa fa-check"></i><b>4</b> A Brief Review of Augmented Reality Display Technologies and Combination with Brain-Computer Interfaces</a><ul>
<li class="chapter" data-level="4.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#abstract-1"><i class="fa fa-check"></i><b>4.1</b> Abstract</a></li>
<li class="chapter" data-level="4.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#introduction-1"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#short-overview-brain-structure"><i class="fa fa-check"></i><b>4.3</b> Short overview: Brain Structure</a></li>
<li class="chapter" data-level="4.4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-technologies-and-basic-principles-of-brain-data-acquisition"><i class="fa fa-check"></i><b>4.4</b> BCI Technologies and Basic Principles of Brain Data Acquisition</a></li>
<li class="chapter" data-level="4.5" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#common-electroencephalography-methods"><i class="fa fa-check"></i><b>4.5</b> Common Electroencephalography Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electroencephalography-eeg-and-even-related-potential-erp"><i class="fa fa-check"></i><b>4.5.1</b> Electroencephalography (EEG) and Even-Related Potential (ERP)</a></li>
<li class="chapter" data-level="4.5.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#eeg-channel-selection-examples"><i class="fa fa-check"></i><b>4.5.2</b> EEG Channel Selection Examples</a></li>
<li class="chapter" data-level="4.5.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electromyography-emg-and-electrooculography-eog"><i class="fa fa-check"></i><b>4.5.3</b> Electromyography (EMG) and Electrooculography (EOG)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#brain-computer-interfaces-bci"><i class="fa fa-check"></i><b>4.6</b> Brain-Computer Interfaces (BCI)</a><ul>
<li class="chapter" data-level="4.6.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-functions"><i class="fa fa-check"></i><b>4.6.1</b> BCI Functions</a></li>
<li class="chapter" data-level="4.6.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#different-types-of-bci"><i class="fa fa-check"></i><b>4.6.2</b> Different Types of BCI</a></li>
<li class="chapter" data-level="4.6.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#applications-for-bci"><i class="fa fa-check"></i><b>4.6.3</b> Applications for BCI</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#augmented-reality-ar"><i class="fa fa-check"></i><b>4.7</b> Augmented Reality (AR)</a><ul>
<li class="chapter" data-level="4.7.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-technologies"><i class="fa fa-check"></i><b>4.7.1</b> AR Technologies</a></li>
<li class="chapter" data-level="4.7.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-devices"><i class="fa fa-check"></i><b>4.7.2</b> AR Devices</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#combining-ar-and-bci"><i class="fa fa-check"></i><b>4.8</b> Combining AR and BCI</a></li>
<li class="chapter" data-level="4.9" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#limitations-and-interpretations"><i class="fa fa-check"></i><b>4.9</b> Limitations and Interpretations</a></li>
<li class="chapter" data-level="4.10" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#references-2"><i class="fa fa-check"></i><b>4.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><i class="fa fa-check"></i><b>5</b> A Methodology for Microdosing Research: Cognitive behavioral tasks as investigative tools for tracking low-dose effects of psilocybin</a><ul>
<li class="chapter" data-level="5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#abstract-2"><i class="fa fa-check"></i><b>5.1</b> Abstract</a></li>
<li class="chapter" data-level="5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#introduction-2"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#the-third-wave-of-psychedelic-science"><i class="fa fa-check"></i><b>5.3</b> The Third Wave of Psychedelic Science</a><ul>
<li class="chapter" data-level="5.3.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#clinical-science"><i class="fa fa-check"></i><b>5.3.1</b> Clinical Science</a></li>
<li class="chapter" data-level="5.3.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#neuroscience"><i class="fa fa-check"></i><b>5.3.2</b> Neuroscience</a></li>
<li class="chapter" data-level="5.3.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#summary"><i class="fa fa-check"></i><b>5.3.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#microdosing"><i class="fa fa-check"></i><b>5.4</b> Microdosing</a></li>
<li class="chapter" data-level="5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#tasks-to-target-psychedelic-drug-effects"><i class="fa fa-check"></i><b>5.5</b> Tasks to Target Psychedelic Drug Effects</a><ul>
<li class="chapter" data-level="5.5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-effects"><i class="fa fa-check"></i><b>5.5.1</b> Cognitive Effects</a></li>
<li class="chapter" data-level="5.5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#working-memory-and-inhibition-tasks"><i class="fa fa-check"></i><b>5.5.2</b> Working-Memory and Inhibition Tasks</a></li>
<li class="chapter" data-level="5.5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-flexibility-tasks"><i class="fa fa-check"></i><b>5.5.3</b> Cognitive Flexibility Tasks</a></li>
<li class="chapter" data-level="5.5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#creativity-task"><i class="fa fa-check"></i><b>5.5.4</b> Creativity Task</a></li>
<li class="chapter" data-level="5.5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-effects"><i class="fa fa-check"></i><b>5.5.5</b> Perceptual Effects</a></li>
<li class="chapter" data-level="5.5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-processing-tasks"><i class="fa fa-check"></i><b>5.5.6</b> Perceptual Processing Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#discussion-and-concluding-remarks"><i class="fa fa-check"></i><b>5.6</b> Discussion and Concluding Remarks</a></li>
<li class="chapter" data-level="5.7" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#references-3"><i class="fa fa-check"></i><b>5.7</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><i class="fa fa-check"></i><b>6</b> Perceiving the World Around Us: How Divergent Methods Illustrate Convergent Perspectives</a><ul>
<li class="chapter" data-level="6.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#the-visual-system-and-present-controversy"><i class="fa fa-check"></i><b>6.2</b> The Visual System and Present Controversy</a><ul>
<li class="chapter" data-level="6.2.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#neurophysiological-evidence"><i class="fa fa-check"></i><b>6.2.1</b> Neurophysiological Evidence</a></li>
<li class="chapter" data-level="6.2.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#emotions-motivations-and-perception"><i class="fa fa-check"></i><b>6.2.2</b> Emotions, Motivations, and Perception</a></li>
<li class="chapter" data-level="6.2.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#zooming-in-on-fear"><i class="fa fa-check"></i><b>6.2.3</b> Zooming in on Fear</a></li>
<li class="chapter" data-level="6.2.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#other-emotions"><i class="fa fa-check"></i><b>6.2.4</b> Other Emotions</a></li>
<li class="chapter" data-level="6.2.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#motivated-perception"><i class="fa fa-check"></i><b>6.2.5</b> Motivated Perception</a></li>
<li class="chapter" data-level="6.2.6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-1"><i class="fa fa-check"></i><b>6.2.6</b> Conclusion</a></li>
<li class="chapter" data-level="6.2.7" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches"><i class="fa fa-check"></i><b>6.2.7</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-and-computational-approaches"><i class="fa fa-check"></i><b>6.3</b> Cognitive and computational approaches</a><ul>
<li class="chapter" data-level="6.3.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-approaches"><i class="fa fa-check"></i><b>6.3.1</b> Cognitive Approaches</a></li>
<li class="chapter" data-level="6.3.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#computational-approaches"><i class="fa fa-check"></i><b>6.3.2</b> Computational Approaches</a></li>
<li class="chapter" data-level="6.3.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-2"><i class="fa fa-check"></i><b>6.3.3</b> Conclusion</a></li>
<li class="chapter" data-level="6.3.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches-1"><i class="fa fa-check"></i><b>6.3.4</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-and-implications"><i class="fa fa-check"></i><b>6.4</b> Conclusion and Implications</a></li>
<li class="chapter" data-level="6.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#references-4"><i class="fa fa-check"></i><b>6.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html"><i class="fa fa-check"></i><b>7</b> Brain Training and Cognition</a><ul>
<li class="chapter" data-level="7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#abstract-3"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#introduction-4"><i class="fa fa-check"></i><b>7.2</b> Introduction</a></li>
<li class="chapter" data-level="7.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#chess-and-music"><i class="fa fa-check"></i><b>7.3</b> Chess and Music</a><ul>
<li class="chapter" data-level="7.3.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#near-and-far-transfer"><i class="fa fa-check"></i><b>7.3.1</b> Near and Far Transfer</a></li>
<li class="chapter" data-level="7.3.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study"><i class="fa fa-check"></i><b>7.3.2</b> A Meta-Analysis Study</a></li>
<li class="chapter" data-level="7.3.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#patterns-in-findings"><i class="fa fa-check"></i><b>7.3.3</b> Patterns in Findings</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training"><i class="fa fa-check"></i><b>7.4</b> Cognitive Training</a></li>
<li class="chapter" data-level="7.5" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training-programs-do-they-work"><i class="fa fa-check"></i><b>7.5</b> Cognitive Training Programs: Do They Work?</a><ul>
<li class="chapter" data-level="7.5.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#brain-training-games"><i class="fa fa-check"></i><b>7.5.1</b> Brain Training Games</a></li>
<li class="chapter" data-level="7.5.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#bilingual-brain-training"><i class="fa fa-check"></i><b>7.5.2</b> Bilingual Brain Training</a></li>
<li class="chapter" data-level="7.5.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#working-memory-training"><i class="fa fa-check"></i><b>7.5.3</b> Working Memory Training</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#fitnessphysical-activities"><i class="fa fa-check"></i><b>7.6</b> Fitness/Physical activities</a><ul>
<li class="chapter" data-level="7.6.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study-1"><i class="fa fa-check"></i><b>7.6.1</b> A Meta-Analysis Study</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#combining-cognitive-and-aerobic-training"><i class="fa fa-check"></i><b>7.7</b> Combining Cognitive and Aerobic Training</a><ul>
<li class="chapter" data-level="7.7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#problems-with-the-research"><i class="fa fa-check"></i><b>7.7.1</b> Problems with the research</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#limitations-and-future-work"><i class="fa fa-check"></i><b>7.8</b> Limitations and Future Work</a><ul>
<li class="chapter" data-level="7.8.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#long-term-effects"><i class="fa fa-check"></i><b>7.8.1</b> Long-Term Effects</a></li>
<li class="chapter" data-level="7.8.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#various-age-groups"><i class="fa fa-check"></i><b>7.8.2</b> Various Age Groups</a></li>
<li class="chapter" data-level="7.8.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#participants-with-health-problems"><i class="fa fa-check"></i><b>7.8.3</b> Participants with Health Problems</a></li>
<li class="chapter" data-level="7.8.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#individual-vs.group-setting"><i class="fa fa-check"></i><b>7.8.4</b> Individual vs. Group Setting</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#conclusion-3"><i class="fa fa-check"></i><b>7.9</b> Conclusion</a></li>
<li class="chapter" data-level="7.10" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#references-5"><i class="fa fa-check"></i><b>7.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html"><i class="fa fa-check"></i><b>8</b> Human Object Recognition and Computational Models</a><ul>
<li class="chapter" data-level="8.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#introduction-5"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#human-object-recognition-system"><i class="fa fa-check"></i><b>8.2</b> Human Object Recognition System</a><ul>
<li class="chapter" data-level="8.2.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-homology-of-human-and-macaques-visual-systems"><i class="fa fa-check"></i><b>8.2.1</b> The homology of human and macaque’s visual systems</a></li>
<li class="chapter" data-level="8.2.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#object-selective-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.2</b> Object-selective visual areas in the human brain</a></li>
<li class="chapter" data-level="8.2.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#facial-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.3</b> Facial visual areas in the human brain</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomena-of-interest-in-object-recognition"><i class="fa fa-check"></i><b>8.3</b> The behavioral phenomena of interest in object recognition</a></li>
<li class="chapter" data-level="8.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-other-race-effect"><i class="fa fa-check"></i><b>8.4</b> The behavioral phenomenon: Other-race effect</a><ul>
<li class="chapter" data-level="8.4.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model"><i class="fa fa-check"></i><b>8.4.1</b> Face space model</a></li>
<li class="chapter" data-level="8.4.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#perceptual-learning-theory"><i class="fa fa-check"></i><b>8.4.2</b> Perceptual learning theory</a></li>
<li class="chapter" data-level="8.4.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#neural-networks-evidence"><i class="fa fa-check"></i><b>8.4.3</b> Neural networks evidence</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-unfamiliar-face"><i class="fa fa-check"></i><b>8.5</b> The behavioral phenomenon: Unfamiliar face</a><ul>
<li class="chapter" data-level="8.5.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model-1"><i class="fa fa-check"></i><b>8.5.1</b> Face-space model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#how-we-deal-with-the-difficulties-of-computational-models"><i class="fa fa-check"></i><b>8.6</b> How we deal with the difficulties of computational models?</a><ul>
<li class="chapter" data-level="8.6.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#core-recognition"><i class="fa fa-check"></i><b>8.6.1</b> Core Recognition</a></li>
<li class="chapter" data-level="8.6.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#invariance-problem"><i class="fa fa-check"></i><b>8.6.2</b> Invariance problem</a></li>
<li class="chapter" data-level="8.6.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-explanation-of-it-neuronal-populations-on-object-recognition"><i class="fa fa-check"></i><b>8.6.3</b> The explanation of IT neuronal populations on object recognition</a></li>
<li class="chapter" data-level="8.6.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#shape-similarity-vs.semantic-category-information-in-it-neuronal-populations"><i class="fa fa-check"></i><b>8.6.4</b> Shape similarity vs. semantic category information in IT neuronal populations</a></li>
<li class="chapter" data-level="8.6.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#computational-models-accounting-for-the-it-representation"><i class="fa fa-check"></i><b>8.6.5</b> Computational models accounting for the IT representation</a></li>
<li class="chapter" data-level="8.6.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.6.6</b> 4.6. Deep Neural Networks</a></li>
<li class="chapter" data-level="8.6.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-advantages-of-hierarchical-features-in-computational-models"><i class="fa fa-check"></i><b>8.6.7</b> 4.7. The advantages of hierarchical features in computational models</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#conclusion-4"><i class="fa fa-check"></i><b>8.7</b> 5. Conclusion</a></li>
<li class="chapter" data-level="8.8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#references-6"><i class="fa fa-check"></i><b>8.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html"><i class="fa fa-check"></i><b>9</b> Language Acquisition and Machine Learning</a><ul>
<li class="chapter" data-level="9.1" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#introduction-6"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#speech-categorization-in-language-acquisition"><i class="fa fa-check"></i><b>9.2</b> Speech Categorization in language acquisition</a></li>
<li class="chapter" data-level="9.3" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#speech-categorization-as-a-machine-learning-task"><i class="fa fa-check"></i><b>9.3</b> Speech categorization as a Machine Learning task</a></li>
<li class="chapter" data-level="9.4" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#word-category-acquisition"><i class="fa fa-check"></i><b>9.4</b> Word Category Acquisition</a></li>
<li class="chapter" data-level="9.5" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#word-categorization-as-a-machine-learning-problem"><i class="fa fa-check"></i><b>9.5</b> Word Categorization as a Machine Learning Problem</a></li>
<li class="chapter" data-level="9.6" data-path="language-acquisition-and-machine-learning.html"><a href="language-acquisition-and-machine-learning.html#references-7"><i class="fa fa-check"></i><b>9.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-8.html"><a href="references-8.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Cognitive Technologies: From Theory and Data to Application</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="language-acquisition-and-machine-learning" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Language Acquisition and Machine Learning</h1>
<p>Xiaomeng Ma</p>
<p>The City University of New York – Graduate Center</p>
<div id="introduction-6" class="section level2">
<h2><span class="header-section-number">9.1</span> Introduction</h2>
<p>Child language acquisition and machine learning are two different topics but the core questions are alike. Infants or algorithms are exposed to linguistic input. They are able to find patterns in the input and either produce input-like output (e.g. children start to speak.) or perform other task based on the input (e.g machine translation).</p>
<p>Language is a defining property to human beings. It is a cultural artifact and an important communication tool. Linguistically speaking, all languages in the world can be defined as a collection of sound/meaning pairs. People hear speech stream and make sense out to it. Moreover, they can also produce similar speech in order to communicate. Language is an extremely complex multi-modal system, however, it is acquired by normal developed infants in an effortless manner. For decades, linguistics and psychologists have been trying to understand the mechanism of language acquisition. Chomsky (1986) defined the question on language acquisition into two part: what constitutes knowledge of a language, and how is the knowledge acquired by its users? Theoretical linguists have been working on the first part of the question and psycholinguistics have been working on the second part of the question. The internal paradoxical tension between two parts has been noticed by Chomsky: “To achieve descriptive adequacy it often seems necessary to enrich the system of available devices, whereas to solve our case of Plato’s problem we must restrict the system of available devices so that only a few languages or just one are determined by the given data. It is the tension between these two tasks that makes the field an interesting one, in my view.” (Chomsky, 1986)</p>
<p>This contradiction between adequate descriptive device vs restricted system is also reflects in language acquisition of infants. They learn from the utterances of people around (parents), but the utterances are finite, incomplete, idiosyncratic… They learned language without explicit instructions, that nobody teaches a 2y/o to put a subject, a verb and an object in a sentence. They all learn it rapidly. Usually by the age of 5, children are able to communicate without difficulty. Also, the output of acquisition is uniformly. Typical developed children usually achieved the same level of fluency in their native language. To summarize the paradox here, children receive finite and limited set of input and produce infinite and highly original output. To solve this problem, linguists need to find a class of representations that is sufficiently rich to account for the observed dependencies in natural language.</p>
<p>In machine learning tasks, most of the goals are achieved by learning instances through neural network to build a representation of the instance and produce input-like output. Tom Mitchell (1997) definition of Machine Learning: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. The instances in language-related machine learning task would be sentences. The more instances for input the better representation could be formed. In language related machine learning tasks, the representation the algorithm needs to mirror is concepts in language, such as phonemes, word, or grammar.</p>
<p>In this way, language acquisition and machine learning are similar in the way that, both children and algorithms have to process input data, find features, build representation and perform desired tasks based on the representation they built. Given a stream of linguistic input, an algorithm or human brain incrementally learns a grammar that captures its statistical patterns, which can then be used to parse or generate new data. Therefore, putting language acquisition and machine learning tasks together could probably provide some new perspectives to solve these two challenges faced in both field. In the following paragraphs of this article, two specific tasks (speech categorization and word categorization) will be discussed in the perspective of child language acquisition and machine learning.</p>
</div>
<div id="speech-categorization-in-language-acquisition" class="section level2">
<h2><span class="header-section-number">9.2</span> Speech Categorization in language acquisition</h2>
<p>An early and essential task for infants is to make sense of speech that they hear. There are at least three levels of discrimination for infants to: phonemic/phonological level, lexical level and phrasal/sentence level. At phonemic/phonological level, infants must learn to partition varied sounds into phonemic categories. For example, English speaking infants need to know that /kæt/ (cat) and /hæt/ (hat) refer to different objects, but /wɔt̬ɚ/ and /wɔtɚ/ (water) both mean the transparent liquid they drink when they feel thirsty. Tonal language speaking infants also need to differentiate between different tones. For example, Chinese speaking infants usually learn it at a young age that /mā/ (mother) and /mǎ/ (horse) are totally different. Infants’ first task in language acquisition is to figure out the phonemic categories before trying to make sense out of them. The complex phonetic learning task is somewhat innate for all typical developing infants. Previous studies have shown that young infants are especially sensitive to acoustic change and speech sounds. Brain imaging studies showed that infants as young as 4 months age can discriminate speech sounds vs non-speech sounds and they prefer languages they are familiar with to unfamiliar ones (Minagawa-Kawai, Cristià and Dupoux, 2011; Moon, Lagercrantz and Kuhl, 2013). Moreover, infants are also able to identify phonological boundaries and segment word based on the rich linguistic information in the auditory stream (Juszky, Cutler and Redanz, 1993; Jusczyk, Houston, and Newsome, 1999; Houston et al., 2000). As well as differentiating all the audio input, the infants must also learn to perceptually group different sounds as the same group. For example, when infants hear the word “cookie” said by mother and father, even though perceptually two “cookie” sound different, they have to be able to integrate the different input and group them together. Kuhl (1985) has this is the problem of speech categorization.</p>
<p>Speech categorization is a very difficult task. In human languages, there are finite sets of phonetic units, or combinations of vowels and consonants. Those phonetic units are difficult to define physically. The features of phonetic units are influenced by talker variety, rates of speech and contexts. When different speakers produce the same sound, the acoustic feature vary widely (Figure 1). Also, when people talk fast or when a sound occurs in a different environment, all can affect the physical feature of a sound.</p>
</div>
<div id="speech-categorization-as-a-machine-learning-task" class="section level2">
<h2><span class="header-section-number">9.3</span> Speech categorization as a Machine Learning task</h2>
<p>Given the messy nature of speech categorization, speech recognition technologies also face the same challenges as human infants. Last twenty years have witnessed high speed development of speech recognition technology, that enabled the brith of products like Siri and Alexa. However, the performance of Siri and Alexa are still not satisfactory. “Sorry I don’t understand what you said” is a constant frustration by Siri and Alexa users. Siri and Alexa are based on the traditional speech technology, which relies on linguistic resources and textual information to build acoustic and speech models. Traditional speech technology is developed based on larger and larger amounts of labeled data to train models. Traditional approaches in speech recognition include Hidden Markov Models (HMM), Dynamic Time Warping (DTW), combined with artificial neural networks. HMM based models are the most popular models in speech recognition field. Speech signals can be viewed as piecewise signals that fit into ta quantities matrix. Acoustic models and language model are both trained on speech signals in HMM. One advantage of HMM based models is that accuracy is highly related to training data size. It is easier to improve a HMM based model as long as large size of training data are available. However, when several gigabytes of data or memory space are not available, HMM based models could not perform well. This is also the reason Amazon and Apple both deployed cloud space and require network connection as for the working environment of Alexa and Siri, instead of local device. More recent work has focused on an end-to-end speech recognition model that jointly combine all components of the speech and train them together. Recent attempts have successfully trained supervised systems using textual transcripts only (Hannun et al., 2014, Miao, Gowayyed and Metze, 2015). However, this is still not the most efficient model in speech recognition since there is an extra step of translating speech input into textual information. Computational linguists therefore turned to a more efficient approach, which is the way of how human infants process speech. From babbling at 6 months of age and producing full sentence by age of 3 years, young children learn how to talk before they know how to read and write, and with minimal instructions. Inspired by early language acquisition, zero-resource speech technologies were first proposed in the JH CLSP Workshop in 2012, “with an aim to construct a system that learn an end-to-end Spoken Dialog (SD) system, in an unknown language, from scratch, using only information available to a language learning infant” (Jansen et al., 2013). Zero resource refers to zero labelled data in training data in order to imitate the unsupervised learning process. In 2015, first Zero Resource Speech Challenge was organized in order to bring researchers together and compare their systems within a common open source evaluation setting. The participants work on the same data provided and evaluated based on the same criterion.</p>
<p>There are two major tasks in zero resource speech challenge: subword modeling and spoken term discovery. Subword modeling requires to build a representation of speech signal that is robust across different talkers, speech speed and context, which is similar to speech categorization. Since this is an unsupervised task, the definition of subword is not confined to phonemes or sound, or any arbitrary linguistic category; instead, subword is defined as basic unit to distinguish words. Subword modeling is similar to speech categorization in child language acquisition. Both tasks involve finding speech features in sound stream that are linguistically relevant (i.e. phoneme structure) and discard non linguistic features (i.e speaker identity). In Zero Resource Speech Challenge, the participants are required to provide a feature representation that maximally discriminate speech units in the raw input. The evaluation usually involves training a phone classifier and evaluating its classification accuracy. In Zero Resource Speech Challenge, Minimal-Pair ABX tasks are used to evaluate feature representation, which does not require any labelled training data (Schatz, 2013; Schatz, 2014). Minimal-Pair ABX task is a match-to-sample tasks to measure discriminability between two sound categories. If sounds A and sounds B belong to two separate categories, α and β, given a new sound X, the task is to decide whether X belongs to α or β. Discriminability of ABX task is defined as the probability that the Dynamic Time Warping (DTW) divergence between α and X and β and X. The dissimilarity is calculated either by the cosine distance or KL-divergence.</p>
<p>In 2015 Zero Resource Speech Challenge, two data set were used for participants to use: Buckeye corpus of conversational English (Pitt et al.,2007) and the Xitsonga section of the NCHLT corpus of South Africa’s languages (de Vries et al.,2014). For the English corpus, 6 male and 6 female native speaker of English recorded a total of 4h59m05s of speech; for Xitsonga section, 12 male and 12 female speakers recorded a total of 2h29m07s speech (Versteegh et al., 2016). There were total 5 algorithms on subword modeling accepted for publications. The scores on ABX discriminability is shown in Table 1. The baseline feature representation is the result of Mel-FrequencyCepstral Coefficients (MFCC). MFCC are coefficients of collective representation of short-term power spectrum of a sound. MFCC is used as baseline feature since it is not linguistic specific. The topline feature representation is the result from labeled data training, which is derived from Kaldi GMM-HMM system. As shown in table 1, for English language, most of the algorithms performed better than baseline in across-speaker and within-speaker task. Two of them even beat the topline in within-speaker task. For Xitsonga language, most of the algorithms performed better than baseline but none of them performed better than topline.</p>
<p>The best performing algorithm for crossand withinspeaker in English and within speaker in Xitsonga is DPGMM (Chen et al. 2015). Chen and his colleagues applied a pipeline of talker-normalized MFCC’s followed by a Dirichlet process Gaussian mixture model (DPGMM). DPGMM is a Bayesian nonparametric model which automatically learn the number of components according to the observed data which has been successfully applied to speech segments clustering (Kamper, Jansen, King and Goldwater, 2014). This approach generated very close result to the topline in across-speaker tasks and in within-speaker tasks, it even out performed topline. This results indicate that speech recognition without previous labeled data is a plausible that worth further pursuit. Badino et al (2015) also applied feature space modeling in their algorithm. They use binarized auto-encoders and HMM encoders to learn input features. The results in cross-speaker tasks were only slightly better than MFCC model and worse in within-speaker tasks. One possible explanation is that phonological features are not binary in nature. Applying binary coder to analyzing non-binary data will result in overrepresentation.</p>
<p>Instead of modeling the feature space, Renshaw’s and Thiolliere’s team both applied topdown information exploiting. They generated word-like pairs using an unsupervised discovery system and used the found pairs as input into a neural network. Renshaw’s team used correspondence auto-encoder (CAE) to learn the patterns in the input. Thiolliere’s team used the discovered pairs to train a siamese network. Their achieved the best results in Xitsonga cross-speaker task.</p>
<p>Baljekar’s team applied articulatory information derived from previously trained speech synthesis system for languages without a writing system. The results were worse than the baseline. They also compared the articulatory features with segment-based inferred phones, and found that inferred phones had the worst performance in Xitsonga tasks. Baljekar’s team did not build a strict unsupervised system since they relied on the information from a partially supervised system. Their results are interesting in the way that it demonstrated how supervised feature interact with unsupervised systems.</p>
<p>In 2017 Zero Resource Speech Challenge, there were two group of data sets: development data and the surprise data. The development data consists English, French and Mandarin corpora, with phone force-aligned using Kaldi (Povey, et al.,2011, Wang, Zhang and Zhang, 2015). The surprise data consists of German and Wolof corpora (Gauthier et al., 2016), but it is not revealed to the participants (Dunbar et al., 2017). A description of the corpus statistics is shown in table 2. There are total 6 papers with 16 systems for subword modeling, which is almost three times as last challenge. All the systems are evaluated using Mixed pair ABX tasks, with a focus on phone triplet minimal pairs that differ in the central sound. For example, A = beg (α) and B = bag (β), X = bag’ should be categorized as α. The scores for each system is shown in table 3. In general, most of the submitted models have better performance on development data than surprise data. All the sixteen systems can be categorized into four strategies.</p>
<p>Heck et al. applied bottom-up frame-level clustering, inspired by the success of Chen et well as learned feature transformations (LDA, neutralize talker variance. The training lables sound is the same as that of its left and right neighbors. The results showed that both P1 and P2 are successful since they are all better than baseline results. Comparing P1 and P2, re-estimation the centroids only slightly improved the results.</p>
<p>Chen et al. applied DPGMM to cluster frames separately on each language. The labels then is trained on MFCCs (C1) and transformed using unsupervised linear VTLN (C2). The results on both development data and surprise data all outperformed baseline. In German within speaker task, the algorithm outperformed topline too. Ansari et al. trained all five languages on two sets of features. The first set is high-dimensional hidden layer trained by MFCC frames. The second set is a hidden layer trained based labels gathered by a Gaussian mixture model on speech frames. The input to the deep neural network are labels trained by MFCC (A1), Gaussian-mixture-HMM (A2), auto encoder features (A3), and another HMM posteriograms features (A4). The results showed that all four models have better results than baseline. MFCC and GaussianMixture-HMM models had better performance than the other two models.</p>
<p>The third strategy is to improve spoken term discovery. Inspired by Thiolliere et al (2015) and Renshaw et al (2015), Yuan (2017) obtained bottle-neck features through unsupervised word-pair generating model and applied STD system to discover acoustic features of word pairs on English only (Y1), all five languages (Y2). They also created a supervised comparison, using transcribed pairs from Switchboard corpus as labels to train STD system (YS). The results are better than baseline. The results of two unsupervised models were very similar to the supervised one.</p>
<p>The last strategy is to use supervised training on nontarget language. Shibata et al generated features from a neural network acoustic models on Japanese as part of an HMM (S1). In (S2), they trained ten other languages (including English, Mandarin and German) on an end-toend convolutional network and bidirectional LSTM. The model with ten languages out-performed the Japanese one. However, since target languages are also included in the training data, it is not a strict zero resource speech recognition task.</p>
<p>The clear winner for 2015 and 2017 Zero Resource Speech Challenge is DPGMM model, as demonstrated in Chen et al (2016) and Heck et al (2017). The most successful strategy for speech unit categorization is bottom-up clustering, which is true for both monolingual environment and multilingual environment. Bottom-up clustering also best resembles how young children build mental representation of speech units among all other algorithms. The success in bottom-up clustering is inspiring to the field of child language acquisition. For decades, psycholinguists struggle to model the process of speech categorization. Successful machine learning algorithms like bottom-up clustering could be useful as a basis to build a child speech categorization model. Meanwhile, in both years of Zero Resource Speech Challenge tasks, there are some unsupervised algorithms outperformed supervised ones, which might indicate that the mechanism in child speech categorization, similar unsupervised algorithms, requires no innate knowledge or structure.</p>
</div>
<div id="word-category-acquisition" class="section level2">
<h2><span class="header-section-number">9.4</span> Word Category Acquisition</h2>
<p>After successfully differentiating speech units from sound stream, young children also have to make sense out of the combination of speech units. This requires them to learn the grammatical word categories of the language. Past studies have investigated various hypothesis about how children learn grammatical categories. One way that children learn grammatical categories through statistical learning by tracking statistical information such as frequencies and co-occurence of certain sounds. Children could rely on distributional cues or sentence context to determine the category of a certain word (Mintz, Newport and Bever, 2002). In addition, studies on artificial language learning indicate that infants as young as 12 months of age can use distributional cues to group words that have no semantic meanings into categories (Gerken, Wilson and Lewis, 2005; Gomez and Lakusta, 2004; Lany and Gomez, 2008). Recent studies have suggested that infants use prosody and intonation information to determine syntactic and phrase boundaries in their first year of life (Pennekamp, Weber, and Friederici, 2006). Computational models are constructed by psycholinguists to further unravel the mechanisms in statistical learning. Mintz (2003) focused on the frequent frames in child-directed speech (e.g you __ it, the __one) and found that there is a strong pattern of frames that could enhance children’s category acquisition. Clair, Monaghan and Christiansen (2010) applied computational models further expanded the frames to investigate in child-directed speech. They combined fixed frames bigrams and trigrams (e.g aX, aXb) into flexible frames (e.g. aX + Xb), which increase the power of training data. The accuracy of combined flexible frames is largely higher than bigrams or trigrams, suggesting that a less rigid distributional form may provide more information on children learning language. Although these studies successfully contributed to the knowledge of children’s word category acquisition, they focused on particular structures that can not explain all the categories or the environment of all common words. Also, distributional models could not answer problems about ambiguous categories. In English, about 11% of word types in English are grammatically ambiguous (e.g cook (n./v.)) (DeRose, 1998). It is important to build a model that is able to assign more than one category to a word in order to represent categorical ambiguity in real world.</p>
</div>
<div id="word-categorization-as-a-machine-learning-problem" class="section level2">
<h2><span class="header-section-number">9.5</span> Word Categorization as a Machine Learning Problem</h2>
<p>Word Categorization, or part-of-speech tagging (POS tagging) is one of the most developed field in Natural Language Processing. POS tagging is the process of word categories in given input. The input to POS tagging tasks usually consist of word-tag pair sets. Most modern language processing on English uses Penn Treebank tags which has 45 tags (Marcus et al., 1993). Two most used algorithms for tagging are the Hidden Markov Model (HMM) and the Maximum Entropy Markov Model (MEMM). HMM is a generative model based on the probability of ngrams word combination. MEMM is a sequence model adaption on logistic regression, which is a discriminative sequence model. The accuracy of POS-tagging tasks highly depends on the labeled training dataset, which is not efficient enough.</p>
<p>Before discussing about how to build algorithms for word category, it is important to understand how grammatical categories are defined. Pike (1967) has discussed this question from etic and emic perspectives: whether the categories were created to fit words or words were sorted into different categories. This similar to the difference in supervised and unsupervised learning tasks. Supervised learning tasks, such as POS tagging which uses labeled data to train the algorithm, which is similar to “category before words” point of view. While in unsupervised learning tasks, categories are emerged from the words, which is in line with “words before category” point of view.</p>
<p>In addition to speech segmentation, young children are also able to abstract syntactic and semantic information from speech stream. The mechanism of syntactic acquisition is still open to debate. In the perspective of formal language theory and generative grammar, there are are finite set of words, which is denoted by Σ. A language is a subset of Σ, denoted as L. L can be defined as a set of grammatical and semantically well-formed sentences of a language. L, in principle, is an infinite set, as there are infinite sentences in any language. Grammar is a generative device to represent L in a finite way. In Chomsky’s terms, these grammars are i-languages, which are innate and universal to all speakers. He also assumed that there is a Language Acquisition Device (LAD), which takes input linguistic data and outputs a grammar of some kind. The existence of LAD has been evoked decades-long debates between empiricists and nativists. In the computational learning theory, LAD is not a mysterious black box any more; it is seen as an algorithm, which can be studied and interpreted using mathematical tools and computational modeling. To crack the internal structure of LAD, reverse engineering is needed. In the software engineering or machine learning tasks, an algorithm is designed to perform desired tasks. In the case of LAD, an algorithm that performs the desired tasks need to be interpreted and modeled for its internal structure.</p>
<p>In machine learning area, the generative approach could also be insightful., in the way that “generative models can be tested in its prevision and recall two customary measures of performance in natural language engineering, which can address perennial questions of model relevance and scalability” (Kolodyn, Lotem and Edelman, 2015). The algorithm wants to imitate the generative sense in language acquisition so that it can parse and produce new materials. The existing statistically based models are still struggling in parsing new materials, let along producing novel utterances.</p>
<p>In Zero Resource Speech Challenges, word categorization is addressed as the Spoken Term Discovery. As described in Versteegh et al (2016), spoken term discovery is the task of finding speech fragments, ideally the speech fragments could correspond to the word-like units in language. Unlike evaluating dissimilarity scores in speech categorization tasks, there are three steps to evaluate a spoken term discovery algorithm. The first is to examine pairwise fragment discovery in audio stream. Normalized Edit Distance (NED) and paired speech intervals and the coverage (COV) are evaluated the decide to acoustic/phonological discrimination of the pairwise fragment. The second step is to cluster all the discovered pairs into classes. The clusters are evaluated against the gold lexicon. Finally, all the classes are used as labels to “parse” the new input. This step is evaluated by how many word tokens were correctly segmented (Token scores) and how many word boundaries were correctly defined (Boundaries scores). The baseline is provided by a randomized matching algorithms (Jansen and Van Durme, 2011), which has a high NED score (good matching) but poor coverage.</p>
<p>In 2015 Zero Speech Resource Challenge, two papers on algorithms of spoken term discovery tasks were accepted for publication. The results of the algorithms in two papers are summarized in table 4. Rasanen et al. () proposed to use syllable segmentation for spoken term discovery. They compared three systems for segmenting speech stream into syllable units: Vseg, EnvMin and Osc (FIND RASANEN PAPER WRITE MORE ABOUT WHAT IS Vseg EnvMin and Osc). Using syllables to determine spoken terms is a highly original approach, since it relies on the prior knowledge about speech. As shown in table 3, this approach is effective, since it consistently beat baseline results. The Osc algorithm seem to be the most effective one among all syllable-based categorization. Lyzinski et al. focused on the second step of spoken term discovery process, which is clustering discovered pairwise into classes. The study used baseline algorithm for pairwise matching segments, and applied three algorithms to cluster the pairs segmented by baseline algorithm: one simple Connected Components (CC), and two modularity based algorithms, FG and Louvain. Although the performance of the three algorithms were similar to baseline or sometimes worse than bsseline, the results differ among three algorithms. This investigation provides insight on how clustering algorithm could impact performance of spoken term discovery system.</p>
</div>
<div id="references-7" class="section level2">
<h2><span class="header-section-number">9.6</span> References</h2>
<ol style="list-style-type: decimal">
<li>Ansari, T. K., Kumar, R., Singh, S., &amp; Ganapathy, S. (2017, December). Deep learning meth- ods for unsupervised acoustic modeling—Leap submission to ZeroSpeech challenge 2017. In Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE (pp. 754-761). IEEE.</li>
<li>Badino, L., Mereta, A., &amp; Rosasco, L. (2015). Discovering discrete subword units with bina- rized autoencoders and hidden-markov-model encoders. In Sixteenth Annual Conference of the International Speech Communication Association.</li>
<li>Baljekar, P., Sitaram, S., Muthukumar, P. K., &amp; Black, A. W. (2015). Using articulatory fea- tures and inferred phonological segments in zero resource speech processing. In Sixteenth Annual Conference of the International Speech Communication Association.</li>
<li>Chen, H., Leung, C. C., Xie, L., Ma, B., &amp; Li, H. (2015). Parallel inference of Dirichlet process Gaussian mixture models for unsupervised acoustic modeling: A feasibility study. In Sixteenth Annual Conference of the International Speech Communication Association.</li>
<li>Chen, H., Leung, C. C., Xie, L., Ma, B., &amp; Li, H. (2017). Multilingual bottle-neck feature learning from untranscribed speech.</li>
<li>Chomsky, N. (1986). Knowledge of language: Its nature, origin, and use. Greenwood Pub- lishing Group.</li>
<li>Clair, M. C. S., Monaghan, P., &amp; Christiansen, M. H. (2010). Learning grammatical cate- gories from distributional cues: Flexible frames for language acquisition. Cognition, 116(3), 341-360.</li>
<li>DeRose, S. J. (1998, September). XQuery: A unified syntax for linking and querying general XML documents. In QL.</li>
<li>de Vries, N., Davel, M., Badenhorst, J., Basson, W., de Wet, F., Barnard, E., et al. A smart- phone-based ASR data collection tool for under-resourced languages. Speech Communication 2014;56:119–131.</li>
<li>Ellis, N. C. (2017). Cognition, Corpora, and Computing: Triangulating Research in Usage- Based Language Learning. Language Learning, 67(S1), 40-65.</li>
<li>Gauthier, E., Besacier, L., Voisin, S., Melese, M., &amp; Elingui, U. P. (2016, May). Collecting resources in sub-saharan african languages for automatic speech recognition: a case study of wolof. In 10th Language Resources and Evaluation Conference (LREC 2016).</li>
<li>Gerken, L., Wilson, R., &amp; Lewis, W. (2005). Infants can use distributional cues to form syn- tactic categories. Journal of child language, 32(2), 249-268.</li>
<li>Glass, J. (2012, July). Towards unsupervised speech processing. In Information Science, Signal Processing and their Applications (ISSPA), 2012 11th International Conference on (pp. 1-4). IEEE.</li>
<li>Gómez, R. L., &amp; Lakusta, L. (2004). A first step in form-based category abstraction by 12- month-old infants. Developmental science, 7(5), 567-580.</li>
<li>Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., … &amp; Ng, A. Y. (2014). Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv 1412.5567. 21</li>
<li>Heck, M., Sakti, S., &amp; Nakamura, S. (2017, December). Feature optimized dpgmm cluster- ing for unsupervised subword modeling: A contribution to zerospeech 2017. In Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE (pp. 740-746). IEEE.</li>
<li>Jansen, A., Dupoux, E., Goldwater, S., Johnson, M., Khudanpur, S., Church, K., … &amp; JHU CLSP Mini-Workshop Research Team. (2013). A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition.</li>
<li>Jansen, A., &amp; Van Durme, B. (2011, December). Efficient spoken term discovery using ran- domized algorithms. In Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on (pp. 401-406). IEEE.</li>
<li>Kamper, H., Jansen, A., King, S., &amp; Goldwater, S. (2014, December). Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings. In Spoken Lan- guage Technology Workshop (SLT), 2014 IEEE(pp. 100-105). IEEE.</li>
<li>Kilgarriff, A. (2005). Language is never, ever, ever, random. Corpus linguistics and linguis- tic theory, 1(2), 263-276.</li>
<li>Kuhl, P. K. in Neonate Cognition: Beyond the Blooming Buzzing Confusion (eds Mehler, J. &amp; Fox, R.) 231–262 (Lawrence Erlbaum Associates, Hillsdale, New Jersey, 1985).</li>
<li>Kolodny, O., Lotem, A., &amp; Edelman, S. (2015). Learning a Generative Probabilistic Gram- mar of Experience: A Process-Level Model of Language Acquisition. Cognitive Science, 39(2), 227-267.</li>
<li>Lany, J., &amp; Gómez, R. L. (2008). Twelve-month-old infants benefit from prior experience in statistical learning. Psychological Science, 19(12), 1247-1252.</li>
<li>Manenti, C., Pellegrini, T., &amp; Pinquier, J. (2017, October). Unsupervised Speech Unit Dis- covery Using K-means and Neural Networks. In International Conference on Statistical Language and Speech Processing (pp. 169-180). Springer, Cham.</li>
<li>Marcus, M. P., Marcinkiewicz, M. A., &amp; Santorini, B. (1993). Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2), 313-330.</li>
<li>Miao, Y., Gowayyed, M., &amp; Metze, F. (2015, December). EESEN: End-to-end speech recog- nition using deep RNN models and WFST-based decoding. In Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on (pp. 167-174). IEEE.</li>
<li>Minagawa-Kawai, Y., Cristià, A., &amp; Dupoux, E. (2011). Cerebral lateralization and early speech acquisition: A developmental scenario. Developmental Cognitive Neuroscience, 1(3), 217-232.</li>
<li>Mintz, T. H., Newport, E. L., &amp; Bever, T. G. (2002). The distributional structure of grammat- ical categories in speech to young children. Cognitive Science, 26(4), 393-424.</li>
<li>Mintz, T. H. (2003). Frequent frames as a cue for grammatical categories in child directed speech. Cognition, 90(1), 91-117.</li>
<li>Mitchell, T. M. (1997). Machine learning. WCB.</li>
<li>Moon, C., Lagercrantz, H., &amp; Kuhl, P. K. (2013). Language experienced in utero affects vowel perception after birth: A two-country study. Acta Paediatrica, 102(2), 156-160.</li>
<li>Monaghan, P., &amp; Rowland, C. F. (2017). Combining language corpora with experimental and computational approaches for language acquisition research. Language Learning, 67(S1), 14-39. 22</li>
<li>Räsänen, O., Doyle, G., &amp; Frank, M. C. (2015). Unsupervised word discovery from speech using automatic segmentation into syllable-like units. In Sixteenth Annual Conference of the International Speech Communication Association.</li>
<li>Renshaw, D., Kamper, H., Jansen, A., &amp; Goldwater, S. (2015). A comparison of neural net- work methods for unsupervised representation learning on the zero resource speech chal- lenge. In Sixteenth Annual Conference of the International Speech Communication Associa- tion.</li>
<li>Schatz, T., Peddinti, V., Back, F., Jansen, A., Hermansky, H., Dupoux, E.. Evaluating speech features with the minimal-pair abx task (i): Analysis of the classical mfc/plp pipeline. In: Proceedings of Interspeech. 2013.</li>
<li>Schatz, T., Peddinti, V., Cao, X.N., Bach, F., Hermansky, H., Dupoux, E.. Evaluating speech features with the minimal-pair abx task (ii): Resistance to noise. In: Proceedings of Inter- speech. 2014.</li>
<li>Pike, K. L. (1967). Language in relation to a unified theory of the structure of human behav- ior (Vol. 24). Walter de Gruyter GmbH &amp; Co KG.</li>
<li>Pitt, M. A., Johnson, K., Hume, E., Kiesling, S., &amp; Raymond, W. (2005). The Buckeye cor- pus of conversational speech: labeling conventions and a test of transcriber reliability. Speech Communication, 45(1), 89-95.</li>
<li>Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., … &amp; Silovsky, J. (2011). The Kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding (No. EPFL-CONF-192584). IEEE Signal Processing Society.</li>
<li>Thiolliere, R., Dunbar, E., Synnaeve, G., Versteegh, M., &amp; Dupoux, E. (2015). A hybrid dy- namic time warping-deep neural network architecture for unsupervised acoustic modeling. In Sixteenth Annual Conference of the International Speech Communication Association.</li>
<li>Versteegh, M., Anguera, X., Jansen, A., &amp; Dupoux, E. (2016). The zero resource speech challenge 2015: Proposed approaches and results. Procedia Computer Science, 81, 67-72.</li>
<li>Wang, D., &amp; Zhang, X. (2015). Thchs-30: A free chinese speech corpus. arXiv preprint arX- iv:1512.01882.</li>
<li>Yuan, Y., Leung, C. C., Xie, L., Chen, H., Ma, B., &amp; Li, H. (2017). Extracting bottleneck fea- tures and word-like pairs from untranscribed speech for feature representation.</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="human-object-recognition-and-computational-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references-8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/CrumpLab/cognitivetechnologies/tree/master/book/09-xiaomeng-Lang.Rmd",
"text": "Edit"
},
"download": ["cog_tech.pdf", "cog_tech.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
