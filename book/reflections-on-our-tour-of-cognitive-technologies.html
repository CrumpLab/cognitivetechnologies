<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Cognitive Technologies: From Theory and Data to Application</title>
  <meta name="description" content="Papers on assorted cognitive technologies">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Cognitive Technologies: From Theory and Data to Application" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Papers on assorted cognitive technologies" />
  <meta name="github-repo" content="CrumpLab/OER_bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Cognitive Technologies: From Theory and Data to Application" />
  
  <meta name="twitter:description" content="Papers on assorted cognitive technologies" />
  

<meta name="author" content="Matthew Crump">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Cognitive Technologies</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html"><i class="fa fa-check"></i><b>1</b> Reflections on our tour of Cognitive Technologies</a><ul>
<li class="chapter" data-level="1.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#snake-oil-an-old-technology"><i class="fa fa-check"></i><b>1.1</b> Snake oil: An old technology</a></li>
<li class="chapter" data-level="1.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#tech-that-works"><i class="fa fa-check"></i><b>1.2</b> Tech that works</a></li>
<li class="chapter" data-level="1.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#connection-to-instance-theory"><i class="fa fa-check"></i><b>1.3</b> Connection to Instance Theory</a><ul>
<li class="chapter" data-level="1.3.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#procedures-of-mind"><i class="fa fa-check"></i><b>1.3.1</b> Procedures of Mind</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#exciting-directions"><i class="fa fa-check"></i><b>1.4</b> Exciting Directions</a><ul>
<li class="chapter" data-level="1.4.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#conversational-ai"><i class="fa fa-check"></i><b>1.4.1</b> Conversational AI</a></li>
<li class="chapter" data-level="1.4.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#decoding-brain-states"><i class="fa fa-check"></i><b>1.4.2</b> Decoding Brain states</a></li>
<li class="chapter" data-level="1.4.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#detecting-deception"><i class="fa fa-check"></i><b>1.4.3</b> Detecting Deception</a></li>
<li class="chapter" data-level="1.4.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#inner-voice-decoding-with-a-chinstrap"><i class="fa fa-check"></i><b>1.4.4</b> Inner Voice decoding with a chinstrap!</a></li>
<li class="chapter" data-level="1.4.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#image-memorability"><i class="fa fa-check"></i><b>1.4.5</b> Image Memorability</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#thats-all"><i class="fa fa-check"></i><b>1.5</b> That’s all</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><i class="fa fa-check"></i><b>2</b> Computational Classification Techniques for Biomedical and Clinical Big Data</a><ul>
<li class="chapter" data-level="2.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#abstract"><i class="fa fa-check"></i><b>2.1</b> Abstract</a></li>
<li class="chapter" data-level="2.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#introduction"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#previous-work"><i class="fa fa-check"></i><b>2.3</b> Previous Work</a><ul>
<li class="chapter" data-level="2.3.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#topic-modeling"><i class="fa fa-check"></i><b>2.3.1</b> Topic Modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#neural-networks"><i class="fa fa-check"></i><b>2.3.2</b> Neural Networks</a></li>
<li class="chapter" data-level="2.3.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clustering"><i class="fa fa-check"></i><b>2.3.3</b> Clustering</a></li>
<li class="chapter" data-level="2.3.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#word-sense-disambiguation"><i class="fa fa-check"></i><b>2.3.4</b> Word Sense Disambiguation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#medical-and-clinical-applications"><i class="fa fa-check"></i><b>2.4</b> Medical and Clinical Applications</a></li>
<li class="chapter" data-level="2.5" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clerical-applications"><i class="fa fa-check"></i><b>2.5</b> Clerical Applications</a></li>
<li class="chapter" data-level="2.6" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#real-world-applications-and-future-work"><i class="fa fa-check"></i><b>2.6</b> Real World Applications and Future Work</a></li>
<li class="chapter" data-level="2.7" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
<li class="chapter" data-level="2.8" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#references"><i class="fa fa-check"></i><b>2.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html"><i class="fa fa-check"></i><b>3</b> Sonification and augmented cognition: A brief overview</a><ul>
<li class="chapter" data-level="3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#types-of-sonification."><i class="fa fa-check"></i><b>3.1</b> Types of sonification.</a></li>
<li class="chapter" data-level="3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#why-sonify-non-sonic-information"><i class="fa fa-check"></i><b>3.2</b> Why sonify non-sonic information?</a></li>
<li class="chapter" data-level="3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#using-sonification-to-augment-cognition"><i class="fa fa-check"></i><b>3.3</b> Using sonification to augment cognition</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-attention-and-situational-awareness"><i class="fa fa-check"></i><b>3.3.1</b> Perception, attention, and situational awareness</a></li>
<li class="chapter" data-level="3.3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-and-action-in-motor-skill-learning"><i class="fa fa-check"></i><b>3.3.2</b> Perception and action in motor skill learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#data-analysis-and-pattern-recognition."><i class="fa fa-check"></i><b>3.3.3</b> Data analysis and pattern recognition.</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#references-1"><i class="fa fa-check"></i><b>3.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><i class="fa fa-check"></i><b>4</b> A Brief Review of Augmented Reality Display Technologies and Combination with Brain-Computer Interfaces</a><ul>
<li class="chapter" data-level="4.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#abstract-1"><i class="fa fa-check"></i><b>4.1</b> Abstract</a></li>
<li class="chapter" data-level="4.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#introduction-1"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#short-overview-brain-structure"><i class="fa fa-check"></i><b>4.3</b> Short overview: Brain Structure</a></li>
<li class="chapter" data-level="4.4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-technologies-and-basic-principles-of-brain-data-acquisition"><i class="fa fa-check"></i><b>4.4</b> BCI Technologies and Basic Principles of Brain Data Acquisition</a></li>
<li class="chapter" data-level="4.5" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#common-electroencephalography-methods"><i class="fa fa-check"></i><b>4.5</b> Common Electroencephalography Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electroencephalography-eeg-and-even-related-potential-erp"><i class="fa fa-check"></i><b>4.5.1</b> Electroencephalography (EEG) and Even-Related Potential (ERP)</a></li>
<li class="chapter" data-level="4.5.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#eeg-channel-selection-examples"><i class="fa fa-check"></i><b>4.5.2</b> EEG Channel Selection Examples</a></li>
<li class="chapter" data-level="4.5.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electromyography-emg-and-electrooculography-eog"><i class="fa fa-check"></i><b>4.5.3</b> Electromyography (EMG) and Electrooculography (EOG)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#brain-computer-interfaces-bci"><i class="fa fa-check"></i><b>4.6</b> Brain-Computer Interfaces (BCI)</a><ul>
<li class="chapter" data-level="4.6.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-functions"><i class="fa fa-check"></i><b>4.6.1</b> BCI Functions</a></li>
<li class="chapter" data-level="4.6.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#different-types-of-bci"><i class="fa fa-check"></i><b>4.6.2</b> Different Types of BCI</a></li>
<li class="chapter" data-level="4.6.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#applications-for-bci"><i class="fa fa-check"></i><b>4.6.3</b> Applications for BCI</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#augmented-reality-ar"><i class="fa fa-check"></i><b>4.7</b> Augmented Reality (AR)</a><ul>
<li class="chapter" data-level="4.7.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-technologies"><i class="fa fa-check"></i><b>4.7.1</b> AR Technologies</a></li>
<li class="chapter" data-level="4.7.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-devices"><i class="fa fa-check"></i><b>4.7.2</b> AR Devices</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#combining-ar-and-bci"><i class="fa fa-check"></i><b>4.8</b> Combining AR and BCI</a></li>
<li class="chapter" data-level="4.9" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#limitations-and-interpretations"><i class="fa fa-check"></i><b>4.9</b> Limitations and Interpretations</a></li>
<li class="chapter" data-level="4.10" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#references-2"><i class="fa fa-check"></i><b>4.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><i class="fa fa-check"></i><b>5</b> A Methodology for Microdosing Research: Cognitive behavioral tasks as investigative tools for tracking low-dose effects of psilocybin</a><ul>
<li class="chapter" data-level="5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#abstract-2"><i class="fa fa-check"></i><b>5.1</b> Abstract</a></li>
<li class="chapter" data-level="5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#introduction-2"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#the-third-wave-of-psychedelic-science"><i class="fa fa-check"></i><b>5.3</b> The Third Wave of Psychedelic Science</a><ul>
<li class="chapter" data-level="5.3.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#clinical-science"><i class="fa fa-check"></i><b>5.3.1</b> Clinical Science</a></li>
<li class="chapter" data-level="5.3.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#neuroscience"><i class="fa fa-check"></i><b>5.3.2</b> Neuroscience</a></li>
<li class="chapter" data-level="5.3.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#summary"><i class="fa fa-check"></i><b>5.3.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#microdosing"><i class="fa fa-check"></i><b>5.4</b> Microdosing</a></li>
<li class="chapter" data-level="5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#tasks-to-target-psychedelic-drug-effects"><i class="fa fa-check"></i><b>5.5</b> Tasks to Target Psychedelic Drug Effects</a><ul>
<li class="chapter" data-level="5.5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-effects"><i class="fa fa-check"></i><b>5.5.1</b> Cognitive Effects</a></li>
<li class="chapter" data-level="5.5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#working-memory-and-inhibition-tasks"><i class="fa fa-check"></i><b>5.5.2</b> Working-Memory and Inhibition Tasks</a></li>
<li class="chapter" data-level="5.5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-flexibility-tasks"><i class="fa fa-check"></i><b>5.5.3</b> Cognitive Flexibility Tasks</a></li>
<li class="chapter" data-level="5.5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#creativity-task"><i class="fa fa-check"></i><b>5.5.4</b> Creativity Task</a></li>
<li class="chapter" data-level="5.5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-effects"><i class="fa fa-check"></i><b>5.5.5</b> Perceptual Effects</a></li>
<li class="chapter" data-level="5.5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-processing-tasks"><i class="fa fa-check"></i><b>5.5.6</b> Perceptual Processing Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#discussion-and-concluding-remarks"><i class="fa fa-check"></i><b>5.6</b> Discussion and Concluding Remarks</a></li>
<li class="chapter" data-level="5.7" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#references-3"><i class="fa fa-check"></i><b>5.7</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><i class="fa fa-check"></i><b>6</b> Perceiving the World Around Us: How Divergent Methods Illustrate Convergent Perspectives</a><ul>
<li class="chapter" data-level="6.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#the-visual-system-and-present-controversy"><i class="fa fa-check"></i><b>6.2</b> The Visual System and Present Controversy</a><ul>
<li class="chapter" data-level="6.2.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#neurophysiological-evidence"><i class="fa fa-check"></i><b>6.2.1</b> Neurophysiological Evidence</a></li>
<li class="chapter" data-level="6.2.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#emotions-motivations-and-perception"><i class="fa fa-check"></i><b>6.2.2</b> Emotions, Motivations, and Perception</a></li>
<li class="chapter" data-level="6.2.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#zooming-in-on-fear"><i class="fa fa-check"></i><b>6.2.3</b> Zooming in on Fear</a></li>
<li class="chapter" data-level="6.2.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#other-emotions"><i class="fa fa-check"></i><b>6.2.4</b> Other Emotions</a></li>
<li class="chapter" data-level="6.2.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#motivated-perception"><i class="fa fa-check"></i><b>6.2.5</b> Motivated Perception</a></li>
<li class="chapter" data-level="6.2.6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-1"><i class="fa fa-check"></i><b>6.2.6</b> Conclusion</a></li>
<li class="chapter" data-level="6.2.7" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches"><i class="fa fa-check"></i><b>6.2.7</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-and-computational-approaches"><i class="fa fa-check"></i><b>6.3</b> Cognitive and computational approaches</a><ul>
<li class="chapter" data-level="6.3.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-approaches"><i class="fa fa-check"></i><b>6.3.1</b> Cognitive Approaches</a></li>
<li class="chapter" data-level="6.3.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#computational-approaches"><i class="fa fa-check"></i><b>6.3.2</b> Computational Approaches</a></li>
<li class="chapter" data-level="6.3.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-2"><i class="fa fa-check"></i><b>6.3.3</b> Conclusion</a></li>
<li class="chapter" data-level="6.3.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches-1"><i class="fa fa-check"></i><b>6.3.4</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-and-implications"><i class="fa fa-check"></i><b>6.4</b> Conclusion and Implications</a></li>
<li class="chapter" data-level="6.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#references-4"><i class="fa fa-check"></i><b>6.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html"><i class="fa fa-check"></i><b>7</b> Brain Training and Cognition</a><ul>
<li class="chapter" data-level="7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#abstract-3"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#introduction-4"><i class="fa fa-check"></i><b>7.2</b> Introduction</a></li>
<li class="chapter" data-level="7.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#chess-and-music"><i class="fa fa-check"></i><b>7.3</b> Chess and Music</a><ul>
<li class="chapter" data-level="7.3.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#near-and-far-transfer"><i class="fa fa-check"></i><b>7.3.1</b> Near and Far Transfer</a></li>
<li class="chapter" data-level="7.3.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study"><i class="fa fa-check"></i><b>7.3.2</b> A Meta-Analysis Study</a></li>
<li class="chapter" data-level="7.3.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#patterns-in-findings"><i class="fa fa-check"></i><b>7.3.3</b> Patterns in Findings</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training"><i class="fa fa-check"></i><b>7.4</b> Cognitive Training</a></li>
<li class="chapter" data-level="7.5" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training-programs-do-they-work"><i class="fa fa-check"></i><b>7.5</b> Cognitive Training Programs: Do They Work?</a><ul>
<li class="chapter" data-level="7.5.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#brain-training-games"><i class="fa fa-check"></i><b>7.5.1</b> Brain Training Games</a></li>
<li class="chapter" data-level="7.5.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#bilingual-brain-training"><i class="fa fa-check"></i><b>7.5.2</b> Bilingual Brain Training</a></li>
<li class="chapter" data-level="7.5.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#working-memory-training"><i class="fa fa-check"></i><b>7.5.3</b> Working Memory Training</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#fitnessphysical-activities"><i class="fa fa-check"></i><b>7.6</b> Fitness/Physical activities</a><ul>
<li class="chapter" data-level="7.6.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study-1"><i class="fa fa-check"></i><b>7.6.1</b> A Meta-Analysis Study</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#combining-cognitive-and-aerobic-training"><i class="fa fa-check"></i><b>7.7</b> Combining Cognitive and Aerobic Training</a><ul>
<li class="chapter" data-level="7.7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#problems-with-the-research"><i class="fa fa-check"></i><b>7.7.1</b> Problems with the research</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#limitations-and-future-work"><i class="fa fa-check"></i><b>7.8</b> Limitations and Future Work</a><ul>
<li class="chapter" data-level="7.8.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#long-term-effects"><i class="fa fa-check"></i><b>7.8.1</b> Long-Term Effects</a></li>
<li class="chapter" data-level="7.8.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#various-age-groups"><i class="fa fa-check"></i><b>7.8.2</b> Various Age Groups</a></li>
<li class="chapter" data-level="7.8.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#participants-with-health-problems"><i class="fa fa-check"></i><b>7.8.3</b> Participants with Health Problems</a></li>
<li class="chapter" data-level="7.8.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#individual-vs.group-setting"><i class="fa fa-check"></i><b>7.8.4</b> Individual vs. Group Setting</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#conclusion-3"><i class="fa fa-check"></i><b>7.9</b> Conclusion</a></li>
<li class="chapter" data-level="7.10" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#references-5"><i class="fa fa-check"></i><b>7.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html"><i class="fa fa-check"></i><b>8</b> Human Object Recognition and Computational Models</a><ul>
<li class="chapter" data-level="8.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#introduction-5"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#human-object-recognition-system"><i class="fa fa-check"></i><b>8.2</b> Human Object Recognition System</a><ul>
<li class="chapter" data-level="8.2.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-homology-of-human-and-macaques-visual-systems"><i class="fa fa-check"></i><b>8.2.1</b> The homology of human and macaque’s visual systems</a></li>
<li class="chapter" data-level="8.2.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#object-selective-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.2</b> Object-selective visual areas in the human brain</a></li>
<li class="chapter" data-level="8.2.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#facial-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.3</b> Facial visual areas in the human brain</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomena-of-interest-in-object-recognition"><i class="fa fa-check"></i><b>8.3</b> The behavioral phenomena of interest in object recognition</a></li>
<li class="chapter" data-level="8.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-other-race-effect"><i class="fa fa-check"></i><b>8.4</b> The behavioral phenomenon: Other-race effect</a><ul>
<li class="chapter" data-level="8.4.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model"><i class="fa fa-check"></i><b>8.4.1</b> Face space model</a></li>
<li class="chapter" data-level="8.4.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#perceptual-learning-theory"><i class="fa fa-check"></i><b>8.4.2</b> Perceptual learning theory</a></li>
<li class="chapter" data-level="8.4.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#neural-networks-evidence"><i class="fa fa-check"></i><b>8.4.3</b> Neural networks evidence</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-unfamiliar-face"><i class="fa fa-check"></i><b>8.5</b> The behavioral phenomenon: Unfamiliar face</a><ul>
<li class="chapter" data-level="8.5.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model-1"><i class="fa fa-check"></i><b>8.5.1</b> Face-space model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#how-we-deal-with-the-difficulties-of-computational-models"><i class="fa fa-check"></i><b>8.6</b> How we deal with the difficulties of computational models?</a><ul>
<li class="chapter" data-level="8.6.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#core-recognition"><i class="fa fa-check"></i><b>8.6.1</b> Core Recognition</a></li>
<li class="chapter" data-level="8.6.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#invariance-problem"><i class="fa fa-check"></i><b>8.6.2</b> Invariance problem</a></li>
<li class="chapter" data-level="8.6.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-explanation-of-it-neuronal-populations-on-object-recognition"><i class="fa fa-check"></i><b>8.6.3</b> The explanation of IT neuronal populations on object recognition</a></li>
<li class="chapter" data-level="8.6.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#shape-similarity-vs.semantic-category-information-in-it-neuronal-populations"><i class="fa fa-check"></i><b>8.6.4</b> Shape similarity vs. semantic category information in IT neuronal populations</a></li>
<li class="chapter" data-level="8.6.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#computational-models-accounting-for-the-it-representation"><i class="fa fa-check"></i><b>8.6.5</b> Computational models accounting for the IT representation</a></li>
<li class="chapter" data-level="8.6.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.6.6</b> 4.6. Deep Neural Networks</a></li>
<li class="chapter" data-level="8.6.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-advantages-of-hierarchical-features-in-computational-models"><i class="fa fa-check"></i><b>8.6.7</b> 4.7. The advantages of hierarchical features in computational models</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#conclusion-4"><i class="fa fa-check"></i><b>8.7</b> 5. Conclusion</a></li>
<li class="chapter" data-level="8.8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#references-6"><i class="fa fa-check"></i><b>8.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-7.html"><a href="references-7.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Cognitive Technologies: From Theory and Data to Application</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reflections-on-our-tour-of-cognitive-technologies" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Reflections on our tour of Cognitive Technologies</h1>
<p>Matthew Crump, Brooklyn College and Graduate Center of The City University of New York</p>
<p>In Spring 2018 a small group of doctoral and master’s students and I took a tour on the theme of Cognitive Technologies. We were interested in two sub themes that would connect cognition with technology: 1) examples where basic research and theory in cognition has led to applied technologies, and 2) examples where new tech has the potential to augment human cognition. Each week we covered varied topics from speed reading, computational models of semantics, machine learning for classification, computational models of object recognition, decoding brain states, brain training, apps for education, lie detection technology, augmented reality and smart spaces, cognition on drugs, and the use of big data for informing theories of cognition.</p>
<div id="snake-oil-an-old-technology" class="section level2">
<h2><span class="header-section-number">1.1</span> Snake oil: An old technology</h2>
<p>We found some “cognitive” technologies did not live to their hype. For example, speed-reading techniques do little more than train people how to skim text quickly. Too bad any gains in speed are accompanied with losses in comprehension <span class="citation">(Rayner et al. <a href="#ref-rayner_so_2016">2016</a>)</span>. There appears to be no strong evidence that brain training games do anything beyond training people on the games themselves <span class="citation">(Simons et al. <a href="#ref-simons_brain-training_2016">2016</a>)</span>. No far transfer here. Similarly, the evidence for cognition enhancing drugs (that do anything beyond enhancing alertness) is just as dissapointing <span class="citation">(Battleday and Brem <a href="#ref-battleday_modafinil_2015">2015</a>; Marraccini et al. <a href="#ref-marraccini_neurocognitive_2016">2016</a>; Mehlman <a href="#ref-mehlman_cognition-enhancing_2004">2004</a>)</span>.</p>
</div>
<div id="tech-that-works" class="section level2">
<h2><span class="header-section-number">1.2</span> Tech that works</h2>
<p>We found several promising technologies and endeavored to connect their successes to formative ideas in Cognitive Psychology. We focused a great deal on insights from instance-based view of cognition, and their connection to the many recent successes of machine learning applied to various classification problems. For example, problems that used to be identified as easy for for people and hard for machines, like face and object recognition, word and document similarity analysis, gesture, posture, and emotion recognition, voice transcription, and language translation, all have working computational solutions. In general, many of these problems were solved by the same kind of solution: gather a large database of examples, represent each example as a collection of digital features, then train a machine learning algorithm on the examples and measure whether the classifier can generalize to accurately classify new examples that were not from the training set. This big-data approach often works quite well. Why does it work? My view is that success was anticipated by instance theories.</p>
</div>
<div id="connection-to-instance-theory" class="section level2">
<h2><span class="header-section-number">1.3</span> Connection to Instance Theory</h2>
<p>Instance theories of cognition have been developed to account for a range of cognitive abilities, from learning <span class="citation">(Jamieson, Crump, and Hannah <a href="#ref-Jamiesoninstancetheoryassociative2012">2012</a>)</span> and memory <span class="citation">(D. L. Hintzman <a href="#ref-hintzman_judgments_1988">1988</a>; D. L. Hintzman <a href="#ref-hintzman_minerva_1984">1984</a>; D. Hintzman <a href="#ref-HintzmanSchemaabstractionmultipletrace1986">1986</a>)</span>, skill-acquisition <span class="citation">(Logan <a href="#ref-logan_toward_1988">1988</a>)</span>, categorization and concept formation <span class="citation">(Jacoby and Brooks <a href="#ref-JacobyNonanalyticcognitionMemory1984">1984</a>)</span>, to judgment and decision-making <span class="citation">(Dougherty, Gettys, and Ogden <a href="#ref-dougherty_minerva-dm:_1999">1999</a>)</span>. A common assumption among instance theories is that single experiences are a foundational unit of knowledge representation: People retain the details of their specific experiences. In some computational models, such as MINERVA <span class="citation">(D. Hintzman <a href="#ref-HintzmanSchemaabstractionmultipletrace1986">1986</a>)</span>, this idea is expressed in terms of a multiple-trace architecture. Every experience is represented as a feature vector coding the “gory detail” of the elemental features of that experience. These experiences are laid down in an instance-based memory, that has an ever expanding body of unique traces for each successive new experience. The notion that people have instance-based memory representation has been met with some skepticism, perhaps due to the incredulous idea that brains could have enough storage space to hold such a large repository of experiences: Instance theory must be wrong because you would receive a “hard-drive is too full message”. Regardless of the limits of brain-space, the more interesting implication from instance-theory is what can be done with a large pool of examples.</p>
<p>In Jacoby &amp; Brooks’ <span class="citation">Jacoby and Brooks (<a href="#ref-JacobyNonanalyticcognitionMemory1984">1984</a>)</span> view, a large pool of exmamples can provide a non-analytic basis for cognition. When people have a large number of examples, they can perform classification tasks on the basis of analogy by similarity. When presented with a face, rather than pulling up a feature-list for faces and inspecting each element in the stimulus to determine whether the rules say whether it can be called a face, people can look at face, find that they can spontaneiously recall many similar prior examples of this stimulus, then call it a face because it looks globally similar to other faces they have seen.</p>
<p>The essential ingredients for instance theory are 1) a large pool examples, and 2) sensitivity to similarity. In computational models of instance theory, like most global matching memory models <span class="citation">(Eich <a href="#ref-eich_composite_1982">1982</a>; D. L. Hintzman <a href="#ref-hintzman_minerva_1984">1984</a>; Humphreys et al. <a href="#ref-humphreys_global_1989">1989</a>; Murdock <a href="#ref-murdock_todam2:_1993">1993</a>)</span>, both of these ingredients are present. The models store items, experiences, or examples, as high-dimensional feature vectors in a memory. The models also assume that retrieval from memory is done by cue-driven similarity. Specifically, the cues or features present in the current environment are represented as a high-dimensional feature vector, and this cue-of-the-present-moment (probe) is used to retrieve similar traces from memory. This can be achieved mathematically by computing the correlation or cosine between the features of the probe, and the features of each trace in memory. Then, the model assumes that people might respond to the probe in the same way that they responded to the traces retrieved by memory.</p>
<p>Finally, instance theories work well when there is structure in the data. Structure refers to the fact that the features of our experiences are not random. They are correlated with themselves over time and space. If our experiences were random, our world would look and sound like the white noise from old TVs not capable of receiving a station. The insight from instance theory is that sensitivity to the structure of the world around us can be obtained by a reconstructive memory process capable of preserving the details of of our experiences. The experiences contain the structure, and we use similairity between the experiences to become sensitive to that structure. Reasoning by analogy, we see this idea being validated by the many successes of machine learning techniques applied to previously hard classification problems. Those solutions were obtained by harvesting enough examples to support accurate classifier generalization to new exemplars.</p>
<div id="procedures-of-mind" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Procedures of Mind</h3>
<p>Our review of instance theories also anticipated some of the lack-luster findings for cognitive enhancing technologies, like brain-training. The work of Kolers was particularly insightful <span class="citation">(Kolers and Roediger <a href="#ref-KolersProceduresmind1984">1984</a>)</span>. Among other things Kolers spilled a great deal of ink on the topic of learning to read upside down (and other geometric rotations of text). Unsurprisingly, he found that people are of course worse at reading weird rotated text compared to normal upright text, and that people can learn to get better at reading these unfamiliar rotations. More important, were his findings about what it was that people had learned: the details. For example, people learned about the very specific thing they were practicing. A subject learning to read upside-down text would not get better in general for reading just any upside down text, instead they get better at reading the specific letters, words, and sentences contained in the examples they were learning. In other words, there wasn’t much far transfer to be had. In Koler’s view, people were learning “procedures of mind” for solving the specific pattern-analyzing problems they were confronted in the training examples. Here, specificity is the rule and generalization can the exception. Generalization can occur when the specific procedures applied to one problem happen to be useful for another. Taking a broader view, it appears that the specificity and lack of far transfer associated with learning new skills is a kind of hegemonic principle. For example, brain-training games train the game in specific, not the brain in general <span class="citation">(Simons et al. <a href="#ref-simons_brain-training_2016">2016</a>)</span>. Luminosity should have read Kolers.</p>
</div>
</div>
<div id="exciting-directions" class="section level2">
<h2><span class="header-section-number">1.4</span> Exciting Directions</h2>
<p>Every week we read a few papers and everyone was assigned to find neat papers that we should all be aware of. We found some cool stuff. Here’s a few highlights in no particular order.</p>
<div id="conversational-ai" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Conversational AI</h3>
<p>Really hard problems like making computers capable being an engaging conversational partner are being tackled with some success. <a href="https://developer.amazon.com/alexaprize">Amazon created the Alexa prize</a>, which would be awarded to a research group who could produce a chatbot (installed in Amazon’s talking speaker Alexa) that could engage a person in chat for 20 minutes, without the person getting tired and cancelling the conversation. The challenge is still standing after the 2017 competition, but they are running it again in 2018, presumably with more training examples to improve the algorithms. It’s around the corner.</p>
</div>
<div id="decoding-brain-states" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Decoding Brain states</h3>
<p>Multi-voxel pattern classification analysis for classifying cognitive states based off of neural data has been applied to many problems. It often works. So, now we can get some idea of what dreams you were dreaming while you were lying in the scanner <span class="citation">(Horikawa et al. <a href="#ref-horikawa_neural_2013">2013</a>)</span>. There are too many other examples to list.</p>
</div>
<div id="detecting-deception" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Detecting Deception</h3>
<p>Lie detection has moved beyond the polygraph. People might be bad at detecting some of the signals of lying vs. telling the truth <span class="citation">(Vrij, Granhag, and Porter <a href="#ref-vrij_pitfalls_2010">2010</a>)</span>, but machine learning techniques are being thrown at the problem with some success. How about lie detection using fmri <span class="citation">(Langleben et al. <a href="#ref-langleben_telling_2005">2005</a>)</span>, videos of your face <span class="citation">(Meservy et al. <a href="#ref-meservy_automatic_2005">2005</a>)</span>, records of language production <span class="citation">(Matsumoto and Hwang <a href="#ref-matsumoto_identifying_2015">2015</a>)</span>, or typing <span class="citation">(Derrick et al. <a href="#ref-derrick_detecting_2013">2013</a>)</span>? It seems to work better than chance, often much better.</p>
</div>
<div id="inner-voice-decoding-with-a-chinstrap" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Inner Voice decoding with a chinstrap!</h3>
<p>It’s still in development, but this chinstrap tech called <a href="http://news.mit.edu/2018/computer-system-transcribes-words-users-speak-silently-0404">alterego</a> can be used to decode what your inner voice is saying. Umm what?</p>
</div>
<div id="image-memorability" class="section level3">
<h3><span class="header-section-number">1.4.5</span> Image Memorability</h3>
<p>What if there was machine that could tell you how intrinsically memorable something is? Ad agencies would be into this, they might want to know what pictures would stick most strongly in people’s memories. Some big data initiatives are now sorting this out by having loads of people do memory tasks for loads of pictures <span class="citation">(Isola et al. <a href="#ref-isola_what_2011">2011</a>)</span>. The result is a massive database of pictures normed for their memorability <span class="citation">(Khosla et al. <a href="#ref-khosla_understanding_2015">2015</a>)</span>. This can be used to predict memorability of pictures. Baby steps, but moving forward.</p>
</div>
</div>
<div id="thats-all" class="section level2">
<h2><span class="header-section-number">1.5</span> That’s all</h2>
<p>There’s alot going in the broad area of cognitive technologies. Researchers in cognition have growing opportunities to make use of computational models and big data to make theoretical and applied progress. It’ll be fun to see what happens.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-rayner_so_2016">
<p>Rayner, Keith, Elizabeth R. Schotter, Michael EJ Masson, Mary C. Potter, and Rebecca Treiman. 2016. “So Much to Read, So Little Time How Do We Read, and Can Speed Reading Help?” <em>Psychological Science in the Public Interest</em> 17 (1): 4–34. <a href="http://psi.sagepub.com/content/17/1/4.abstract" class="uri">http://psi.sagepub.com/content/17/1/4.abstract</a>.</p>
</div>
<div id="ref-simons_brain-training_2016">
<p>Simons, Daniel J., Walter R. Boot, Neil Charness, Susan E. Gathercole, Christopher F. Chabris, David Z. Hambrick, and Elizabeth A. L. Stine-Morrow. 2016. “Do ‘Brain-Training’ Programs Work?” <em>Psychological Science in the Public Interest</em> 17 (3): 103–86. doi:<a href="https://doi.org/10.1177/1529100616661983">10.1177/1529100616661983</a>.</p>
</div>
<div id="ref-battleday_modafinil_2015">
<p>Battleday, Ruairidh M., and A.-K. Brem. 2015. “Modafinil for Cognitive Neuroenhancement in Healthy Non-Sleep-Deprived Subjects: A Systematic Review.” <em>European Neuropsychopharmacology</em> 25 (11): 1865–81.</p>
</div>
<div id="ref-marraccini_neurocognitive_2016">
<p>Marraccini, Marisa E., Lisa L. Weyandt, Joseph S. Rossi, and Bergljot Gyda Gudmundsdottir. 2016. “Neurocognitive Enhancement or Impairment? A Systematic Meta-Analysis of Prescription Stimulant Effects on Processing Speed, Decision-Making, Planning, and Cognitive Perseveration.” <em>Experimental and Clinical Psychopharmacology</em> 24 (4): 269.</p>
</div>
<div id="ref-mehlman_cognition-enhancing_2004">
<p>Mehlman, Maxwell J. 2004. “Cognition-Enhancing Drugs.” <em>The Milbank Quarterly</em> 82 (3): 483–506. doi:<a href="https://doi.org/10.1111/j.0887-378X.2004.00319.x">10.1111/j.0887-378X.2004.00319.x</a>.</p>
</div>
<div id="ref-Jamiesoninstancetheoryassociative2012">
<p>Jamieson, Randall K., M. J. C. Crump, and Samuel D. Hannah. 2012. “An Instance Theory of Associative Learning.” <em>Learning &amp; Behavior</em> 40 (1): 61–82. doi:<a href="https://doi.org/10.3758/s13420-011-0046-2">10.3758/s13420-011-0046-2</a>.</p>
</div>
<div id="ref-hintzman_judgments_1988">
<p>Hintzman, Douglas L. 1988. “Judgments of Frequency and Recognition Memory in a Multiple-Trace Memory Model.” <em>Psychological Review</em> 95 (4): 528.</p>
</div>
<div id="ref-hintzman_minerva_1984">
<p>Hintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” <em>Behavior Research Methods, Instruments, &amp; Computers</em> 16 (2): 96–101.</p>
</div>
<div id="ref-HintzmanSchemaabstractionmultipletrace1986">
<p>Hintzman, D. 1986. “Schema Abstraction in a Multiple-Trace Memory Model.” <em>Psychological Review</em> 93 (4): 411–28.</p>
</div>
<div id="ref-logan_toward_1988">
<p>Logan, Gordon D. 1988. “Toward an Instance Theory of Automatization.” <em>Psychological Review</em> 95 (4): 492–527.</p>
</div>
<div id="ref-JacobyNonanalyticcognitionMemory1984">
<p>Jacoby, Larry L., and Lee R. Brooks. 1984. “Nonanalytic Cognition: Memory, Perception, and Concept Learning.” <em>The Psychology of Learning and Motivation</em> 18: 1–47.</p>
</div>
<div id="ref-dougherty_minerva-dm:_1999">
<p>Dougherty, Michael RP, Charles F. Gettys, and Eve E. Ogden. 1999. “MINERVA-DM: A Memory Processes Model for Judgments of Likelihood.” <em>Psychological Review</em> 106 (1): 180.</p>
</div>
<div id="ref-eich_composite_1982">
<p>Eich, Janet M. 1982. “A Composite Holographic Associative Recall Model.” <em>Psychological Review</em> 89 (6): 627–61.</p>
</div>
<div id="ref-humphreys_global_1989">
<p>Humphreys, Michael S., Ray Pike, John D. Bain, and Gerald Tehan. 1989. “Global Matching: A Comparison of the SAM, Minerva II, Matrix, and TODAM Models.” <em>Journal of Mathematical Psychology</em> 33 (1): 36–67.</p>
</div>
<div id="ref-murdock_todam2:_1993">
<p>Murdock, Bennet B. 1993. “TODAM2: A Model for the Storage and Retrieval of Item, Associative, and Serial-Order Information.” <em>Psychological Review</em> 100 (2): 183–203.</p>
</div>
<div id="ref-KolersProceduresmind1984">
<p>Kolers, Paul A., and Henry L. Roediger. 1984. “Procedures of Mind.” <em>Journal of Verbal Learning and Verbal Behavior</em> 23 (4): 425–49.</p>
</div>
<div id="ref-horikawa_neural_2013">
<p>Horikawa, Tomoyasu, Masako Tamaki, Yoichi Miyawaki, and Yukiyasu Kamitani. 2013. “Neural Decoding of Visual Imagery During Sleep.” <em>Science</em> 340 (6132): 639–42. <a href="http://science.sciencemag.org/content/340/6132/639.short" class="uri">http://science.sciencemag.org/content/340/6132/639.short</a>.</p>
</div>
<div id="ref-vrij_pitfalls_2010">
<p>Vrij, Aldert, Pär Anders Granhag, and Stephen Porter. 2010. “Pitfalls and Opportunities in Nonverbal and Verbal Lie Detection.” <em>Psychological Science in the Public Interest</em> 11 (3): 89–121. doi:<a href="https://doi.org/10.1177/1529100610390861">10.1177/1529100610390861</a>.</p>
</div>
<div id="ref-langleben_telling_2005">
<p>Langleben, Daniel D., James W. Loughead, Warren B. Bilker, Kosha Ruparel, Anna Rose Childress, Samantha I. Busch, and Ruben C. Gur. 2005. “Telling Truth from Lie in Individual Subjects with Fast Event-Related fMRI.” <em>Human Brain Mapping</em> 26 (4): 262–72.</p>
</div>
<div id="ref-meservy_automatic_2005">
<p>Meservy, Thomas O., Matthew L. Jensen, John Kruse, Judee K. Burgoon, and Jay F. Nunamaker. 2005. “Automatic Extraction of Deceptive Behavioral Cues from Video.” In <em>International Conference on Intelligence and Security Informatics</em>, 198–208. Springer.</p>
</div>
<div id="ref-matsumoto_identifying_2015">
<p>Matsumoto, David, and Hyisung C. Hwang. 2015. “Identifying Universal Linguistic Features Associated with Veracity and Deception.” HUMIN℡L LLC BERKELEY CA.</p>
</div>
<div id="ref-derrick_detecting_2013">
<p>Derrick, Douglas C., Thomas O. Meservy, Jeffrey L. Jenkins, Judee K. Burgoon, and Jay F. Nunamaker Jr. 2013. “Detecting Deceptive Chat-Based Communication Using Typing Behavior and Message Cues.” <em>ACM Transactions on Management Information Systems (TMIS)</em> 4 (2): 9.</p>
</div>
<div id="ref-isola_what_2011">
<p>Isola, Phillip, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. 2011. “What Makes an Image Memorable?” In <em>Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</em>, 145–52. IEEE.</p>
</div>
<div id="ref-khosla_understanding_2015">
<p>Khosla, Aditya, Akhil S. Raju, Antonio Torralba, and Aude Oliva. 2015. “Understanding and Predicting Image Memorability at a Large Scale.” In <em>Computer Vision (ICCV), 2015 IEEE International Conference on</em>, 2390–8. IEEE.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-Crump.Rmd",
"text": "Edit"
},
"download": ["OER_bookdown.pdf", "OER_bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
