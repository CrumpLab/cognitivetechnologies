<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Cognitive Technologies: From Theory and Data to Application</title>
  <meta name="description" content="Papers on assorted cognitive technologies">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Cognitive Technologies: From Theory and Data to Application" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Papers on assorted cognitive technologies" />
  <meta name="github-repo" content="CrumpLab/cognitivetechnologies" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Cognitive Technologies: From Theory and Data to Application" />
  
  <meta name="twitter:description" content="Papers on assorted cognitive technologies" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html">
<link rel="next" href="brain-training-and-cognition.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Cognitive Technologies</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html"><i class="fa fa-check"></i><b>1</b> Reflections on our tour of Cognitive Technologies</a><ul>
<li class="chapter" data-level="1.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#snake-oil-an-old-technology"><i class="fa fa-check"></i><b>1.1</b> Snake oil: An old technology</a></li>
<li class="chapter" data-level="1.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#tech-that-works"><i class="fa fa-check"></i><b>1.2</b> Tech that works</a></li>
<li class="chapter" data-level="1.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#connection-to-instance-theory"><i class="fa fa-check"></i><b>1.3</b> Connection to Instance Theory</a><ul>
<li class="chapter" data-level="1.3.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#procedures-of-mind"><i class="fa fa-check"></i><b>1.3.1</b> Procedures of Mind</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#exciting-directions"><i class="fa fa-check"></i><b>1.4</b> Exciting Directions</a><ul>
<li class="chapter" data-level="1.4.1" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#conversational-ai"><i class="fa fa-check"></i><b>1.4.1</b> Conversational AI</a></li>
<li class="chapter" data-level="1.4.2" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#decoding-brain-states"><i class="fa fa-check"></i><b>1.4.2</b> Decoding Brain states</a></li>
<li class="chapter" data-level="1.4.3" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#detecting-deception"><i class="fa fa-check"></i><b>1.4.3</b> Detecting Deception</a></li>
<li class="chapter" data-level="1.4.4" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#inner-voice-decoding-with-a-chinstrap"><i class="fa fa-check"></i><b>1.4.4</b> Inner Voice decoding with a chinstrap!</a></li>
<li class="chapter" data-level="1.4.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#image-memorability"><i class="fa fa-check"></i><b>1.4.5</b> Image Memorability</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="reflections-on-our-tour-of-cognitive-technologies.html"><a href="reflections-on-our-tour-of-cognitive-technologies.html#thats-all"><i class="fa fa-check"></i><b>1.5</b> That’s all</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><i class="fa fa-check"></i><b>2</b> Computational Classification Techniques for Biomedical and Clinical Big Data</a><ul>
<li class="chapter" data-level="2.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#abstract"><i class="fa fa-check"></i><b>2.1</b> Abstract</a></li>
<li class="chapter" data-level="2.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#introduction"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#previous-work"><i class="fa fa-check"></i><b>2.3</b> Previous Work</a><ul>
<li class="chapter" data-level="2.3.1" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#topic-modeling"><i class="fa fa-check"></i><b>2.3.1</b> Topic Modeling</a></li>
<li class="chapter" data-level="2.3.2" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#neural-networks"><i class="fa fa-check"></i><b>2.3.2</b> Neural Networks</a></li>
<li class="chapter" data-level="2.3.3" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clustering"><i class="fa fa-check"></i><b>2.3.3</b> Clustering</a></li>
<li class="chapter" data-level="2.3.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#word-sense-disambiguation"><i class="fa fa-check"></i><b>2.3.4</b> Word Sense Disambiguation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#medical-and-clinical-applications"><i class="fa fa-check"></i><b>2.4</b> Medical and Clinical Applications</a></li>
<li class="chapter" data-level="2.5" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#clerical-applications"><i class="fa fa-check"></i><b>2.5</b> Clerical Applications</a></li>
<li class="chapter" data-level="2.6" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#real-world-applications-and-future-work"><i class="fa fa-check"></i><b>2.6</b> Real World Applications and Future Work</a></li>
<li class="chapter" data-level="2.7" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
<li class="chapter" data-level="2.8" data-path="computational-classification-techniques-for-biomedical-and-clinical-big-data.html"><a href="computational-classification-techniques-for-biomedical-and-clinical-big-data.html#references"><i class="fa fa-check"></i><b>2.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html"><i class="fa fa-check"></i><b>3</b> Sonification and augmented cognition: A brief overview</a><ul>
<li class="chapter" data-level="3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#types-of-sonification."><i class="fa fa-check"></i><b>3.1</b> Types of sonification.</a></li>
<li class="chapter" data-level="3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#why-sonify-non-sonic-information"><i class="fa fa-check"></i><b>3.2</b> Why sonify non-sonic information?</a></li>
<li class="chapter" data-level="3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#using-sonification-to-augment-cognition"><i class="fa fa-check"></i><b>3.3</b> Using sonification to augment cognition</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-attention-and-situational-awareness"><i class="fa fa-check"></i><b>3.3.1</b> Perception, attention, and situational awareness</a></li>
<li class="chapter" data-level="3.3.2" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#perception-and-action-in-motor-skill-learning"><i class="fa fa-check"></i><b>3.3.2</b> Perception and action in motor skill learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#data-analysis-and-pattern-recognition."><i class="fa fa-check"></i><b>3.3.3</b> Data analysis and pattern recognition.</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sonification-and-augmented-cognition-a-brief-overview.html"><a href="sonification-and-augmented-cognition-a-brief-overview.html#references-1"><i class="fa fa-check"></i><b>3.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><i class="fa fa-check"></i><b>4</b> A Brief Review of Augmented Reality Display Technologies and Combination with Brain-Computer Interfaces</a><ul>
<li class="chapter" data-level="4.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#abstract-1"><i class="fa fa-check"></i><b>4.1</b> Abstract</a></li>
<li class="chapter" data-level="4.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#introduction-1"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#short-overview-brain-structure"><i class="fa fa-check"></i><b>4.3</b> Short overview: Brain Structure</a></li>
<li class="chapter" data-level="4.4" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-technologies-and-basic-principles-of-brain-data-acquisition"><i class="fa fa-check"></i><b>4.4</b> BCI Technologies and Basic Principles of Brain Data Acquisition</a></li>
<li class="chapter" data-level="4.5" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#common-electroencephalography-methods"><i class="fa fa-check"></i><b>4.5</b> Common Electroencephalography Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electroencephalography-eeg-and-even-related-potential-erp"><i class="fa fa-check"></i><b>4.5.1</b> Electroencephalography (EEG) and Even-Related Potential (ERP)</a></li>
<li class="chapter" data-level="4.5.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#eeg-channel-selection-examples"><i class="fa fa-check"></i><b>4.5.2</b> EEG Channel Selection Examples</a></li>
<li class="chapter" data-level="4.5.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#electromyography-emg-and-electrooculography-eog"><i class="fa fa-check"></i><b>4.5.3</b> Electromyography (EMG) and Electrooculography (EOG)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#brain-computer-interfaces-bci"><i class="fa fa-check"></i><b>4.6</b> Brain-Computer Interfaces (BCI)</a><ul>
<li class="chapter" data-level="4.6.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#bci-functions"><i class="fa fa-check"></i><b>4.6.1</b> BCI Functions</a></li>
<li class="chapter" data-level="4.6.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#different-types-of-bci"><i class="fa fa-check"></i><b>4.6.2</b> Different Types of BCI</a></li>
<li class="chapter" data-level="4.6.3" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#applications-for-bci"><i class="fa fa-check"></i><b>4.6.3</b> Applications for BCI</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#augmented-reality-ar"><i class="fa fa-check"></i><b>4.7</b> Augmented Reality (AR)</a><ul>
<li class="chapter" data-level="4.7.1" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-technologies"><i class="fa fa-check"></i><b>4.7.1</b> AR Technologies</a></li>
<li class="chapter" data-level="4.7.2" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#ar-devices"><i class="fa fa-check"></i><b>4.7.2</b> AR Devices</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#combining-ar-and-bci"><i class="fa fa-check"></i><b>4.8</b> Combining AR and BCI</a></li>
<li class="chapter" data-level="4.9" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#limitations-and-interpretations"><i class="fa fa-check"></i><b>4.9</b> Limitations and Interpretations</a></li>
<li class="chapter" data-level="4.10" data-path="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html"><a href="a-brief-review-of-augmented-reality-display-technologies-and-combination-with-brain-computer-interfaces.html#references-2"><i class="fa fa-check"></i><b>4.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><i class="fa fa-check"></i><b>5</b> A Methodology for Microdosing Research: Cognitive behavioral tasks as investigative tools for tracking low-dose effects of psilocybin</a><ul>
<li class="chapter" data-level="5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#abstract-2"><i class="fa fa-check"></i><b>5.1</b> Abstract</a></li>
<li class="chapter" data-level="5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#introduction-2"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#the-third-wave-of-psychedelic-science"><i class="fa fa-check"></i><b>5.3</b> The Third Wave of Psychedelic Science</a><ul>
<li class="chapter" data-level="5.3.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#clinical-science"><i class="fa fa-check"></i><b>5.3.1</b> Clinical Science</a></li>
<li class="chapter" data-level="5.3.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#neuroscience"><i class="fa fa-check"></i><b>5.3.2</b> Neuroscience</a></li>
<li class="chapter" data-level="5.3.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#summary"><i class="fa fa-check"></i><b>5.3.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#microdosing"><i class="fa fa-check"></i><b>5.4</b> Microdosing</a></li>
<li class="chapter" data-level="5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#tasks-to-target-psychedelic-drug-effects"><i class="fa fa-check"></i><b>5.5</b> Tasks to Target Psychedelic Drug Effects</a><ul>
<li class="chapter" data-level="5.5.1" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-effects"><i class="fa fa-check"></i><b>5.5.1</b> Cognitive Effects</a></li>
<li class="chapter" data-level="5.5.2" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#working-memory-and-inhibition-tasks"><i class="fa fa-check"></i><b>5.5.2</b> Working-Memory and Inhibition Tasks</a></li>
<li class="chapter" data-level="5.5.3" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#cognitive-flexibility-tasks"><i class="fa fa-check"></i><b>5.5.3</b> Cognitive Flexibility Tasks</a></li>
<li class="chapter" data-level="5.5.4" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#creativity-task"><i class="fa fa-check"></i><b>5.5.4</b> Creativity Task</a></li>
<li class="chapter" data-level="5.5.5" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-effects"><i class="fa fa-check"></i><b>5.5.5</b> Perceptual Effects</a></li>
<li class="chapter" data-level="5.5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#perceptual-processing-tasks"><i class="fa fa-check"></i><b>5.5.6</b> Perceptual Processing Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#discussion-and-concluding-remarks"><i class="fa fa-check"></i><b>5.6</b> Discussion and Concluding Remarks</a></li>
<li class="chapter" data-level="5.7" data-path="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html"><a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html#references-3"><i class="fa fa-check"></i><b>5.7</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><i class="fa fa-check"></i><b>6</b> Perceiving the World Around Us: How Divergent Methods Illustrate Convergent Perspectives</a><ul>
<li class="chapter" data-level="6.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#the-visual-system-and-present-controversy"><i class="fa fa-check"></i><b>6.2</b> The Visual System and Present Controversy</a><ul>
<li class="chapter" data-level="6.2.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#neurophysiological-evidence"><i class="fa fa-check"></i><b>6.2.1</b> Neurophysiological Evidence</a></li>
<li class="chapter" data-level="6.2.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#emotions-motivations-and-perception"><i class="fa fa-check"></i><b>6.2.2</b> Emotions, Motivations, and Perception</a></li>
<li class="chapter" data-level="6.2.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#zooming-in-on-fear"><i class="fa fa-check"></i><b>6.2.3</b> Zooming in on Fear</a></li>
<li class="chapter" data-level="6.2.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#other-emotions"><i class="fa fa-check"></i><b>6.2.4</b> Other Emotions</a></li>
<li class="chapter" data-level="6.2.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#motivated-perception"><i class="fa fa-check"></i><b>6.2.5</b> Motivated Perception</a></li>
<li class="chapter" data-level="6.2.6" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-1"><i class="fa fa-check"></i><b>6.2.6</b> Conclusion</a></li>
<li class="chapter" data-level="6.2.7" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches"><i class="fa fa-check"></i><b>6.2.7</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-and-computational-approaches"><i class="fa fa-check"></i><b>6.3</b> Cognitive and computational approaches</a><ul>
<li class="chapter" data-level="6.3.1" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#cognitive-approaches"><i class="fa fa-check"></i><b>6.3.1</b> Cognitive Approaches</a></li>
<li class="chapter" data-level="6.3.2" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#computational-approaches"><i class="fa fa-check"></i><b>6.3.2</b> Computational Approaches</a></li>
<li class="chapter" data-level="6.3.3" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-2"><i class="fa fa-check"></i><b>6.3.3</b> Conclusion</a></li>
<li class="chapter" data-level="6.3.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#limits-to-these-approaches-1"><i class="fa fa-check"></i><b>6.3.4</b> Limits to these approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#conclusion-and-implications"><i class="fa fa-check"></i><b>6.4</b> Conclusion and Implications</a></li>
<li class="chapter" data-level="6.5" data-path="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html"><a href="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives.html#references-4"><i class="fa fa-check"></i><b>6.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html"><i class="fa fa-check"></i><b>7</b> Brain Training and Cognition</a><ul>
<li class="chapter" data-level="7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#abstract-3"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#introduction-4"><i class="fa fa-check"></i><b>7.2</b> Introduction</a></li>
<li class="chapter" data-level="7.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#chess-and-music"><i class="fa fa-check"></i><b>7.3</b> Chess and Music</a><ul>
<li class="chapter" data-level="7.3.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#near-and-far-transfer"><i class="fa fa-check"></i><b>7.3.1</b> Near and Far Transfer</a></li>
<li class="chapter" data-level="7.3.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study"><i class="fa fa-check"></i><b>7.3.2</b> A Meta-Analysis Study</a></li>
<li class="chapter" data-level="7.3.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#patterns-in-findings"><i class="fa fa-check"></i><b>7.3.3</b> Patterns in Findings</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training"><i class="fa fa-check"></i><b>7.4</b> Cognitive Training</a></li>
<li class="chapter" data-level="7.5" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#cognitive-training-programs-do-they-work"><i class="fa fa-check"></i><b>7.5</b> Cognitive Training Programs: Do They Work?</a><ul>
<li class="chapter" data-level="7.5.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#brain-training-games"><i class="fa fa-check"></i><b>7.5.1</b> Brain Training Games</a></li>
<li class="chapter" data-level="7.5.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#bilingual-brain-training"><i class="fa fa-check"></i><b>7.5.2</b> Bilingual Brain Training</a></li>
<li class="chapter" data-level="7.5.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#working-memory-training"><i class="fa fa-check"></i><b>7.5.3</b> Working Memory Training</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#fitnessphysical-activities"><i class="fa fa-check"></i><b>7.6</b> Fitness/Physical activities</a><ul>
<li class="chapter" data-level="7.6.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#a-meta-analysis-study-1"><i class="fa fa-check"></i><b>7.6.1</b> A Meta-Analysis Study</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#combining-cognitive-and-aerobic-training"><i class="fa fa-check"></i><b>7.7</b> Combining Cognitive and Aerobic Training</a><ul>
<li class="chapter" data-level="7.7.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#problems-with-the-research"><i class="fa fa-check"></i><b>7.7.1</b> Problems with the research</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#limitations-and-future-work"><i class="fa fa-check"></i><b>7.8</b> Limitations and Future Work</a><ul>
<li class="chapter" data-level="7.8.1" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#long-term-effects"><i class="fa fa-check"></i><b>7.8.1</b> Long-Term Effects</a></li>
<li class="chapter" data-level="7.8.2" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#various-age-groups"><i class="fa fa-check"></i><b>7.8.2</b> Various Age Groups</a></li>
<li class="chapter" data-level="7.8.3" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#participants-with-health-problems"><i class="fa fa-check"></i><b>7.8.3</b> Participants with Health Problems</a></li>
<li class="chapter" data-level="7.8.4" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#individual-vs.group-setting"><i class="fa fa-check"></i><b>7.8.4</b> Individual vs. Group Setting</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#conclusion-3"><i class="fa fa-check"></i><b>7.9</b> Conclusion</a></li>
<li class="chapter" data-level="7.10" data-path="brain-training-and-cognition.html"><a href="brain-training-and-cognition.html#references-5"><i class="fa fa-check"></i><b>7.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html"><i class="fa fa-check"></i><b>8</b> Human Object Recognition and Computational Models</a><ul>
<li class="chapter" data-level="8.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#introduction-5"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#human-object-recognition-system"><i class="fa fa-check"></i><b>8.2</b> Human Object Recognition System</a><ul>
<li class="chapter" data-level="8.2.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-homology-of-human-and-macaques-visual-systems"><i class="fa fa-check"></i><b>8.2.1</b> The homology of human and macaque’s visual systems</a></li>
<li class="chapter" data-level="8.2.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#object-selective-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.2</b> Object-selective visual areas in the human brain</a></li>
<li class="chapter" data-level="8.2.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#facial-visual-areas-in-the-human-brain"><i class="fa fa-check"></i><b>8.2.3</b> Facial visual areas in the human brain</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomena-of-interest-in-object-recognition"><i class="fa fa-check"></i><b>8.3</b> The behavioral phenomena of interest in object recognition</a></li>
<li class="chapter" data-level="8.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-other-race-effect"><i class="fa fa-check"></i><b>8.4</b> The behavioral phenomenon: Other-race effect</a><ul>
<li class="chapter" data-level="8.4.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model"><i class="fa fa-check"></i><b>8.4.1</b> Face space model</a></li>
<li class="chapter" data-level="8.4.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#perceptual-learning-theory"><i class="fa fa-check"></i><b>8.4.2</b> Perceptual learning theory</a></li>
<li class="chapter" data-level="8.4.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#neural-networks-evidence"><i class="fa fa-check"></i><b>8.4.3</b> Neural networks evidence</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-behavioral-phenomenon-unfamiliar-face"><i class="fa fa-check"></i><b>8.5</b> The behavioral phenomenon: Unfamiliar face</a><ul>
<li class="chapter" data-level="8.5.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#face-space-model-1"><i class="fa fa-check"></i><b>8.5.1</b> Face-space model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#how-we-deal-with-the-difficulties-of-computational-models"><i class="fa fa-check"></i><b>8.6</b> How we deal with the difficulties of computational models?</a><ul>
<li class="chapter" data-level="8.6.1" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#core-recognition"><i class="fa fa-check"></i><b>8.6.1</b> Core Recognition</a></li>
<li class="chapter" data-level="8.6.2" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#invariance-problem"><i class="fa fa-check"></i><b>8.6.2</b> Invariance problem</a></li>
<li class="chapter" data-level="8.6.3" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-explanation-of-it-neuronal-populations-on-object-recognition"><i class="fa fa-check"></i><b>8.6.3</b> The explanation of IT neuronal populations on object recognition</a></li>
<li class="chapter" data-level="8.6.4" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#shape-similarity-vs.semantic-category-information-in-it-neuronal-populations"><i class="fa fa-check"></i><b>8.6.4</b> Shape similarity vs. semantic category information in IT neuronal populations</a></li>
<li class="chapter" data-level="8.6.5" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#computational-models-accounting-for-the-it-representation"><i class="fa fa-check"></i><b>8.6.5</b> Computational models accounting for the IT representation</a></li>
<li class="chapter" data-level="8.6.6" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.6.6</b> 4.6. Deep Neural Networks</a></li>
<li class="chapter" data-level="8.6.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#the-advantages-of-hierarchical-features-in-computational-models"><i class="fa fa-check"></i><b>8.6.7</b> 4.7. The advantages of hierarchical features in computational models</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#conclusion-4"><i class="fa fa-check"></i><b>8.7</b> 5. Conclusion</a></li>
<li class="chapter" data-level="8.8" data-path="human-object-recognition-and-computational-models.html"><a href="human-object-recognition-and-computational-models.html#references-6"><i class="fa fa-check"></i><b>8.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-7.html"><a href="references-7.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Cognitive Technologies: From Theory and Data to Application</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="perceiving-the-world-around-us-how-divergent-methods-illustrate-convergent-perspectives" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Perceiving the World Around Us: How Divergent Methods Illustrate Convergent Perspectives</h1>
<p>Jordan Wylie, The Graduate Center, CUNY, May 25 2018</p>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">6.1</span> Introduction</h2>
<p>Exogenous, sensory data helps us navigate through our daily lives. Our body’s specialized cells and tissues receive raw sensory information and translate it into signals that the mind and body can understand. The architecture of our brains is well-suited to handle, sort, and filter through the enormous amounts of sensory signals and noise that we encounter every day. One such sensory modality, vision, dominates phenomenological experience and has, in turn, dominated research on both bottom-up (or outside-in) and top-down (or inside-out) approaches to perception. The rich visual system literature spans domains and methodologies within psychology and related disciplines, many of which suggest a shared understanding of how the brain is able to integrate stored information, while continually processing new incoming stimuli. Though it may still be unclear, new computational approaches and computer-informed methodologies have shed light on an age-old debate. Namely, do our motivations and expectations inform conscious perception?</p>
<p>Research spanning decades has demonstrated the amazing capacity of the human visual system. Much of the brain’s posterior cortical structure is devoted in some way to processing visual information, with nearly half of the nonhuman primate neocortex being devoted to such processes (DiCarlo, Zoccolan, &amp; Rust, 2012; Felleman &amp; Van Essen, 1991). The dense visual network within the human brain is not contained within the occipital lobe, but recruits assistance from surrounding cortical areas. During visual processing, neural pathways work together to quickly discriminate between stimuli on a vast number of features; patterns, colors, motion, and many other structural features of our visual environments are registered on the retina and then integrated into ongoing neural and cognitive processing.</p>
<p>Despite a vast literature, complete mechanistic understanding of visual perception and recognition are still absent. Researchers continue to debate how exactly we interpret the world around us, and which methods are most appropriate for tackling that question. Some theorists have purposed a functionally impenetrable visual perception that is unadulterated by cognitive processes occurring elsewhere in the brain (Pylyshyn, 1999), while others disagree (see Friston, 2010), believing perception to be integrated with cognition similar to nearly all other functions within the brain. The present chapter will attempt to review this issue and relevant research findings guided by the predictive brain lens.</p>
<p>Specifically, the focus of the following chapter will be to examine how evidence of predictions inform visual recognition, as supported by neuroscientific and cognitive findings. How might social norms, our motivations and emotions, and informational assumptions influence what we see? Perceptual and recognition accuracy are fundamental to our visual experience, allowing us to interpret and make sense of the world around us. While research on the visual system touches many other important aspects to visual experiences (e.g., attention), those are not within the scope of the present review. Instead, I will focus on how neuroscientific, affective and motivational, and cognitive approaches can reveal important information about visual object recognition.</p>
</div>
<div id="the-visual-system-and-present-controversy" class="section level2">
<h2><span class="header-section-number">6.2</span> The Visual System and Present Controversy</h2>
<p>It is well documented that human beings have exceptional visual capabilities. While an owl may see with acuity at night, and the lizard may lack a visual blind spot, the human visual system, which has evolved from a shared primate brain, allows for flexibility and an emergent, powerful ability to predict. The combination of our physiology and cognitive abilities enables the integration of vast amounts of visual information to create perceptual experiences that do not deviate much from those around us. Indeed, visual experience requires some uniformity to ground humans in an agreed upon representation of reality. Perception is, therefore, rooted in this understanding and must be tethered to some similarity across people. By counter example, hallucinations demonstrate how perception without reality constraints lacks any observable order (see Clark, 2013). We must agree that a particular pattern of waves that hit the retina yield the color green, this is the first step in semantic development and abstracting away important ideas. But the question remains, how do we (or do we at all) use previous information, memories and states to inform and facilitate visual perception?</p>
<p>However, these questions are not novel. Beginning as far back as Descartes (1637), there has been a marked intrigue in how, mechanistically, humans are able to assimilate the extensive visual information present at any given moment to adequately traverse our social environments. This curiosity has not waned. Visual system-centered work has extended beyond m using of Da Vinci and Descartes to more modern-day science like Hubel (Polyak,1957; Schmolesky, 1995). Today, we capitalize on access to neurophysiological components of the visual system and a general template for information processing to inform questions and research concerning vision. Research done with similar primate visual systems, the macaque monkey, illuminated some key neural structures that process visual information (Fitzpatrick, Itoh, &amp; Diamond, 1983; Shipp &amp; Zeki, 1989). By mapping specific connections across brain regions, we have begun to piece together where the visual system is distributed in the brain, which regions are most crucial for processing visual information, and the cascade of processes and networks responsible for the bringing visual information to conscious awareness.</p>
<p>However, correlating activity and locating areas within the brain can only answer so many questions. It is necessary to extend these models to other domains to understanding how these areas work. Are they running in parallel or serially? Modular accounts of visual recognition suggest that distinct visual cortical areas (V1-V5) process different types of visual information and together makeup the primary cascade for recognizing objects (Ungerleider &amp; Haxby, 1994). Visual information first hits the retina and is pooled by ganglion cells. It is here that the other important facets of the visual system are most salient. Namely, attending to specific areas allows for information to hit the field of vision, with information situated close to the fovea most effectively represented. After this, information is passed to the LGN for transduction, which finally sparks the cascade of processing to create the final representation (Van Essen, Anderson, &amp; Felleman, 1992; Felleman &amp; Van, 1991).</p>
<p>Beginning at the primary visual cortex, or V1, rudimentary object features are calculated from raw visual information (Desimone &amp; Ungerleider, 1989). From there, increasingly specialized areas selectively fill in missing information that builds up to a representation of the object at the conscious level (Kastner &amp; Ungerleider 2000). Moreover, computational evidence suggests that the cascade of processing within the visual system that leads to the conscious categorization and subsequent identification of objects proceeds in a hierarchical fashion. Again, using the macaque monkey’s visual system as a proxy, findings suggest that the specific cortical areas (e.g., V1, V4) process distinct components of the overall sensory input. This processing occurs through a series by which processing low-level features of the two-dimensional space eventually producing a three-dimensional (3-D) object representation (Perrett &amp; Oram, 1993). Other theorists have extended this work to evaluate the applicability given human biological constraints, finding that physiological evidence that implicates the inferior temporal cortex (IT) can be modelled for basic performance (Riesenhuber &amp; Poggio, 1999). Essential to this line of reasoning is the feed-forward building of complexity.</p>
<p>Neurophysioloigcal mapping has established specific subdivisions, each of which contribute to the overall functioning. These subdivisions are largely made up of different cellular signal, including the magnocellular (M-pathway) and the parvocellular (P-pathway), research that has been spearheaded by studies of the macaque monkey visual system (Maunsell, Nealey, &amp; DePriest, 1990). The M-pathway and P-pathway have also been linked to specific spatial frequency information, whereby the M-pathway, situated primarily in the dorsal stream, is sensitive to low-spatial frequency information, and the P-pathway, primarily located in the ventral stream, is sensitive to high-spatial frequency information (Burr, Morrone, &amp; Ross, 1994; Goffaux et al., 2005). Whereas the P-pathway primarily facilitates perception of contrast and color, notably higher-order features of visual perception, the M-pathway facilitates perception of motion and coarse greyscale information (Merigan and Maunsell, 1993; Vuilleumier et al., 2003). Interestingly, these pathways are also thought to map onto unconscious (M-pathway) and conscious (P-pathway) visual processing (Tapia &amp; Breitmeyer, 2011). These divergent features of cellular channels within the visual system highlights important characteristics of our primate visual system; namely, the parallel, coordinated nature of visual processing.</p>
<p>Whereas these approaches are founded on the bottom-up nature of visual perception and object recognition, recent research has begun to challenge this view. For instance, the subjective value of an object may affect the proximity in which said object is perceived. Subjectively more desired objects (e.g., money) are estimated as closer in proximity than less desirable objects (Balcetis &amp; Dunning, 2010). Further, research on action potential suggests that perception of hill steepness is influenced by metabolic costs (Proffitt, 2006). This line of research has revived the New Look debate, which claims that our beliefs, motivations, affordances, and more directly affect how we interpret incoming visual data (Balcetis, 2016). The updated New Look advocates a penetrability of perception by cognition, while other researchers continue to advocate a cognitively “impenetrable” V1, suggesting methodological limitations have stymied legitimate challenges to conventional conceptions of bottom-up processing (see Firestone &amp; Scholl, 2016). Are the differences in observed behavior a function of response biases or actual perceptual modulation? Does the money actually appear closer? Or is it just a function of the relative desirability? If a judgement is driving the observed differences in responses, early regions of the visual process cascade may be independent of cognitive influence. While evidence is mounting, these questions remain to be answered.</p>
<p>In the following sections, I will attempt to answer these questions through the predictive lens. First, I will touch on prominent approaches to studying object recognition in humans. While the lens from which object recognition is studied varies greatly, an overarching goal is to better understand how human brains utilize prior information to inform ongoing processing of visual information. How might stimulus-driven conceptions of vision be limiting? What units or features of visual stimuli make up the foundation for understanding complex objects encountered? The scope of our knowledge about our environments is vast, informed by physiological states, memory, and sensory signals across modalities. How all of this information is integrated represents a critical question within vision research. Beginning with neuropsychological approaches to vision and then moving to cognitive science methods, the following sections will attempt to consider the relative advances and limitations to studying vision through the predictive lens.</p>
<p>To answer these questions of general perception and more specific questions of object recognition, methodological approaches began by neural pathway mapping. This was useful in identifying particular neurons that process specific information, but it does not get us closer to understanding how this happens. Computational approaches are currently gaining momentum. These approaches differ slightly from traditional neurobiological study of perception, instead assessing how to maximize information processing model fits.</p>
<div id="neurophysiological-evidence" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Neurophysiological Evidence</h3>
<p>Neural approaches have guided much of the research on the visual system and predictive vision. These approaches capitalize on the physical accessibility of neuroscientific methodology, enabled by the similarity in the visual cortices of other primates (Milner &amp; Goodale 1995). Early work has isolated two major pathways by which our visual system preferentially processes distinct components of what we see (Milner &amp; Goodale, 2008). Specifically, the ventral stream tells us what an object is, while the dorsal tells us where it is (Goodale &amp; Milner, 1992; Goodale &amp; Westwood, 2004). Moreover, the ventral stream is a low-level visual pathway, made up of descending and ascending routes, that are necessary for detailed visual information (Bar, 2000). Much of the research on ventral stream processing has focused on the traditional bottom-up approach, or on the ascending pathways, whereby visual information hits the V1, V2, and V4 cortical areas and then projects onto the high-level regions that are implicated in object perception like the IT (Bar, 2000). Generally, it is through these visual cortices (V1-V4) that the ventral visual stream hierarchically creates the visual representation (Hong, et al., 2016). Contrastingly, the dorsal stream is implicated in movement-based vision, aiding in the calibration of motor functions and detection of movement in the periphery. This pathway is less implicated in the process of object recognition but rather, fine-tuned for perception for action (Goodale &amp; Milner, 1992). However, some research has suggested that our brain may rely on predictions from gross low-level features to supply an initial guess for other parts of the visual system to inform (Bar, 2003; Bar et al., 2006; Kveraga, Boshyan, &amp; Bar, 2007). These findings highlight the potentially important contribution of the dorsal stream to complex visual object recognition and have opened the door to new explorations within the purview of the predictive lens.</p>
<p>The dual-system process model posits that ascending neural pathways in the ventral stream hierarchically build the representation of a given object. Though this is undoubtedly a piece of the puzzle, there is much to be gleaned from incorporating feedback loops into the model of visual object recognition. Specifically, research has demonstrated the importance of context in object perception (Bar, 2004; Fenske, Aminoff, Gronau, &amp; Bar, 2006), engagement of higher-order structures in processing degraded or ambiguous stimuli (Wyatte, Curran, &amp; O’Reilly, 2012), and perceptibility of low spatial frequency objects (Kverega, 2007). Recent evidence has corroborated these findings, demonstrating context-dependent oscillation patterns within the prefrontal and parietal cortices (Helfrich, Huang, Wilson, &amp; Knight, 2017). These findings emphasize the role of expectation in guiding and enhancing visual perception. Contextual factors provide cues to associate with similar objects, facilitating and even biasing visual processing.</p>
<p>The two-system approach garnered a great deal of support, engendering a flood of scientific research. However, there are fundamental limitations to such approaches. Namely, these approaches rely on isolating pathways in the brain, ostensibly overlooking the complexities and interdependences that exist between these pathways. It is well documented that the brain is an iterative, dynamic organ (Cunningham &amp; Zelazo, 2007). Research in this domain has implicated the PFC and reflective processing to extricate human neural processing from the more automatic associative processing seen in animals, favoring a hierarchical brain architecture that allows for afferent and efferent connections between brain regions (see Zelazo &amp; Cunningham, 2007). This approach has also been situated within the object recognition literature. Work done by Bar (2000) underscores how studying the pathways separately may have neglected important contributions from prefrontal brain areas to ongoing visual processing.</p>
<p>To fully understand how neurobiology might suggest a top-down, cognitive penetrability, it is important to reconcile the role of the orbitofrontal cortex (OFC) in the processing of affective information. Specifically, the amygdala has established connectivity to the OFC, which implicates it in encoding emotionally salient information (Pessoa &amp; Adolphs, 2010). Studies have implicated the OFC in representing threat and reward (Kringelbach &amp; Rolls 2004), as well as in processing and representing auditory and visual information (Kringelbach 2005). This research has further linked the OFC to visual processing by demonstrating that the OFC is activated around 80-130ms after stimulus onset (Lamme &amp; Roelfsema 2000). While this is not the earliest component of visual processing (&lt;100ms), utilization of fMRI and MEG imaging has established the temporal activity during the short latency period, a time early enough to modulate ongoing processing (Barrett &amp; Bar, 2009; O’Callaghan, Kveraga, Shine, Adams, &amp; Bar, 2016).</p>
<p>Finally, while not the primary focus of the present chapter, attention is an important part of visual perception that often obfuscates the interpretation of visual system penetrability. Attention is an obviously critical facet of the visual system and is important to understanding the ways in which higher-order cognitive processes bias visual processing. We can only process what we attend to, and as such, visual attention operates as a sort of gate keeper in the cascade of conscious object representation. Specific stimulus properties, like emotion, are prioritized, which increases the likelihood that they will be attended to (Öhman, Flykt, &amp; Esteves, 2001). Indeed, like a perceptual bias or expectation, attentional biases increase the probability of recognition. This makes parsing attentional versus perceptual biases difficult to disentangle. However, even at the attentional level, some findings are suggesting an influence of visual expectations (Gantman &amp; Van Bavel, 2014). Attentional control refers to processes that facilitate both suppression of irrelevant stimuli (temporal attention) and broaden the breadth of visual field input (spatial attention), which research has shown are attentional systems differentially biased based on emotional states (Clore &amp; Huntsinger, 2007; Gable &amp; Harmon-Jones, 2008, 2010). Spatial, or broadened attention, increases target detection in peripheral locations, yet increases inaccurate responses due to the costs associated with unfocused processing of visual stimuli. Conversely, temporal or flexible attention requires a focused lens, making irrelevant and peripheral targets difficult to process, but increases the accuracy of target identification. Specifically, previous research has shown that high arousal emotions increase local target detection compared to happiness and sadness (Easterbrook, 1959; Eysenck et al., 2007; Gable &amp; Harmon-Jones, 2008, 2010; Clore &amp; Huntsinger, 2007; Wells &amp; Matthews, 1996). These results highlight the ways in which motivations and emotions may influence attentional processing, subsequently influencing perception. Therefore, even on the attentional level, top-down expectancies can produce influences on visual processing.</p>
</div>
<div id="emotions-motivations-and-perception" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Emotions, Motivations, and Perception</h3>
<p>A critical piece to the present argument is that emotions, often referred to as a part of system-one (Kahneman, 2011), create powerful preferences from which we see the world. Emotions are a crucial component of our capacity to navigate social systems. As mentioned above, emotion saliency provides an interesting intersection between attention and prediction. Emotions guide the way we see our worlds, interact with others, and motivate goal-directed behavior. Research highlights the impact of emotions on our lives, demonstrating that emotions influence our attitudes (DeSteno, Dasgupta, Bartlett, &amp; Cajdric, 2004; Esses, &amp; Dovidio, 2002), our decisions (Lerner &amp; Keltner, 2001; Lerner, Small, &amp; Loewenstein, 2004), and our judgements (Forgas, 2013; Clore &amp; Huntsinger, 2007).</p>
<p>As mentioned above, recent studies have illustrated that there is a top-down contribution to object recognition stemming from the dorsal stream (Kveraga et al., 2007). One prominent theory, the Frame and Fill Theory (FnF), posits that object processing within the ventral stream relies on contributions from the dorsal stream (magnocellular connections; M-pathway), which contributes global outlines of visual input and estimates of what the object is via the OFC. The ventral visual stream (parvocellular channels; P-pathway) relies on the global template and ‘fills’ in necessary details for accurate object recognition (Bullier, 2001;Chen et al.,2007). Further, by introducing the dorsal stream as a mechanism through which emotions may bias object recognition, there may also be important implications for the biasing of attention. Research suggests that the dorsal stream governs the shifting of attention (Siegel, Donner, Oostenveld, Fries, &amp; Engel, 2008). Thus, the FnF theory provides a cohesive model of attention and object recognition for studying biases that influence early processing, specifically in relation to biases caused by emotional content. Through this lens, the relationships between emotion and object recognition can be better tested, by biasing processing toward the M-pathway and the dorsal stream object recognition (and flexible attention) may be facilitated.</p>
<p>Findings from the lens of affective neuroscience also suggest that the primacy of emotion may guide gating mechanisms of early visual inputs as well as recruit the engagement of the OFC, altering ongoing perceptual and visual processing. (Feldman-Barrett &amp; Bar, 2009; Schmitz et al. 2009). Emotions and affective states are informationally rich, intimately interacting with cognitive processes (see Clore, Gasper, &amp; Garvin, 2001). Together, emotion and perception optimize the visual identification process. Specific evidence has implicated positive and negative affect in the encoding of visual information, suggesting that differences between positive and negative states interact with the encoding of peripheral information by altering the field of vision (Schmitz, De Rosa, Anderson, 2009; Rowe et al., 2007). Moreover, binocular rivalry studies have unlocked interesting insights into what achieves perceptual dominance. Emotional faces (Alpers &amp; Gerdes, 2007), affectively conditioned (Alpers et al., 2005), and motivationally valued (Balcetis, Dunning, &amp; Granot, 2012) stimuli all overtake the perceptual experience compared to neutral and control stimuli. Collectively, these findings hint at important relationships between emotional or motivational value and visual prioritization. If emotions interact with value representations of stimuli, expectations and predictions may be reflected in the processing of visual information. However, within the dominant framing of visual processing, connectivity between emotion “centers” in the brain (e.g., the amygdala) and top-down contributors (e.g., OFC), are not well established.</p>
</div>
<div id="zooming-in-on-fear" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Zooming in on Fear</h3>
<p>Fear has emerged as an important emotion to assess different processing during visual perception. A surfeit of research has investigated the link between amygdala functioning and emotions, much of which has focused on amygdala activation in the recognition and response to potential threats through feedback from the visual cortex (Amaral et al., 1992; LeDoux, 1998; Ledoux, 2002; Pessoa et al., 2002; Adolphs &amp; Spezio, 2006). Further, studies have shown automatic detection of threatening stimuli presented outside of conscious awareness (Öhman &amp; Mineka, 2001; Öhman, 2005), enhanced attention in visual searches when in fearful states (Öhman, Flykt, &amp; Esteves, 2001), and biased perceptions when afraid (Stefanucci, Proffitt, Clore, &amp; Parekh, 2008). Such results suggest a unique role emotion, and specifically fear, may have on initial attention and perception and may influence higher-level processes like object-identification.</p>
<p>While the role of the amygdala in the processing of emotion is well established, the purported mechanisms that affect cognitive processes in fear states are incompatible, relying on conflicting top-down and bottom-up processing models to explain a variety of phenomena (Pessoa &amp; Adolphs, 2010). Specifically, two routes have been posited for amygdala directed processing. The low route, which has the advantage of speed, suggests a direct subcortical route from the thalamus, and the higher route, from the thalamus to the visual cortex to the amygdala (Rolls, 1999). Indeed, the amygdala provides a critical source of input for affective processing. Yet, evidence of a low route existing in high order species is lacking (Shi &amp; Davis, 2001) and high route processing has yet to reconcile speed issues (Shi &amp; Davis, 2001). This inconsistency diminishes understanding of how the amygdala gets information to the level of consciousness quickly enough to incite action.</p>
<p>It is well established that fear is associated with facilitating attention toward and perception of dangerous entities compared to other emotional states. Again, the common framework for explaining these findings are primary cortical visual pathways that send low grade visual information to the amygdala, which then identifies threatening entities. Mixed findings and the lack of a unifying theory have limited understanding of how cognition might influence ongoing processing. Moreover, these approaches rely on the primacy of affect, suggesting that cognition has little to do with initial processing of affective information, a notion that continues to be contended (Lazarus, 1982; Storbeck &amp; Clore, 2007).</p>
<p>The emphasis on subcortical processing of information, restricts top-down contributions of processing emotional stimuli such as motivations, perceptions, and attitudes. For instance, the visual system is sensitive to and biased by endogenous (Balcetis &amp; Dunning, 2006; Tiedens, Unzueta, &amp; Young, 2007; Skelly, &amp; Decety, 2012) and exogenous (environmental cues) factors (Proffit, 2006; Cole, Balcetis, &amp; Dunning, 2013), which can change the nature of processing of visual features and perceptions of such objects. Similarly, endogenous and exogenous factors may even bias attention, like fear increasing attentional flexibility, enhancing the ability to detect peripheral objects, which is contrary to the standard assumption that fear only narrows attention (Awh &amp; Pashler, 2010). Current paradigms examining such rapid detection of objects and subsequent object recognition rely on assuming the independence of dorsal and ventral streams, with scarce focus on how they interact with one another and how emotion may modulate such interactions.</p>
</div>
<div id="other-emotions" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Other Emotions</h3>
<p>The majority of visual perception research examines how fear interacts with processing, though some research has suggested that other emotions (particularly negative valence) evidence biases. One such study investigated how faces are perceived as fundamentally different depending on context in which the face was presented (Aviezer et al., 2008). Although this is notably not a study on object recognition, it nonetheless highlights how the visual system is context dependent. This occurs with something as vital as the accurate recognition and identification of other human faces. Emotion has marked effects on a number of cognitive processes and may have dissociable effects on object recognition during instances in which emotion states are congruent with predicted sensory input compared to when they are not.</p>
</div>
<div id="motivated-perception" class="section level3">
<h3><span class="header-section-number">6.2.5</span> Motivated Perception</h3>
<p>In addition, research within the motivational domain has suggested an influence of motivational drive on perception. The implications of goals in biasing visual system processing is a particularly consequential possibility (Weber, 1996; Inbar, &amp; Pizarro, 2009). For instance, within the social framework, moral goals have been shown to influence our decisions (Haidt, 2007), our attitudes (Helzer &amp; Pizarro, 2011), our emotions (Haidt, 2003), and even the ‘popping out’ of salient words (Gantman &amp; Van Bavel, 2014). Through a motivated perceptual lens, moral goals may facilitate object recognition of salient images. Research has also shown race-based processing may also rely on low-spatial frequency cues (Correll, Hudson, Guillermo, &amp; Earls, 2017), suggesting some integration of social conceptualizations and expectancies into the ongoing visual perceptual process.</p>
<p>The effect of motivation on perception is not limited to the moral sphere. Research has also demonstrated goal- and action- driven effects on perception. For example, externally incentivizing a specific construal of ambiguous figures drives differences in reported encounters with the target construal (Balcetis &amp; Dunning, 2006). A number of psychological studies have evidenced top-down effects informed by subjective value, race and stereotypes, and political climate. These types of social knowledge mark a high-level form of context, which has previously been implicated in object perception under isolated laboratory conditions. One study has suggested that outside of frequency, learning and biases in responses, perceptual dominance is explained by subjectively valued influences (Balcetis, Dunning, &amp; Granot, 2012). Further, research done by Levin and Banaji (2006) revealed differences in the perceived lightness of faces matched on luminance. African Americans were seen as darker skinned compared to European-descent faces, a finding that has been attributed to top-down knowledge of facial featural differences between these two races (Levin &amp; Banaji, 2006). Unfortunately, other research has corroborated findings suggesting racial stereotyping modulatory effects on perception. For instance, one study found that race of the target predicted erroneously firing a gun in a computer game, even when the incentive was structured to be accurate and shoot only targets who were holding a gun, not a tool (Correll, Park, Judd, &amp; Wittenbrink, 2002). Another set of findings has demonstrated how a self-identified political group and government stability can alter perception of skin color, favoring lighter skin representations when the target is identified as a member of your political in-group (Caruso, Mead, &amp; Balcetis, 2009) and under instances of in-group instability (Stern et al., 2016). Acquiring accurate social knowledge is important to individual functioning. However, this means that our socially determined biases may permeate into cognition and perception. These findings highlight how such knowledge constrains visual processing and biases in the direction of one’s goals or beliefs.</p>
</div>
<div id="conclusion-1" class="section level3">
<h3><span class="header-section-number">6.2.6</span> Conclusion</h3>
<p>To synthesize the dominant themes so far, the amygdala is crucial for processing of emotional stimuli and is specifically sensitive to fear. Motivational works also provides an informative perspective on how individual internal states or internally valued goals can fundamentally alter perception. Research on the mechanisms that govern how fear impacts attention and object recognition rely on conflicting cortical processing routes, routes which preclude top-down contributions, and omit the early activation on prefrontal cortical areas of the brain. Consequently, more research is needed to establish and extend mechanistic understanding of the influence of fear. Fear imparts biases onto what we see, biases which can produce and reinforce maladaptive behavioral responses (e.g., always seeing a snake instead of a sock increases stress response, a process that is ultimately corrosive for the body). The biases and predictions we bring into our phenomenological experiences constrains what we see, especially in instances in which relevant objects or scenes are obfuscated in some way.</p>
<p>On a broader level, this literature pulls at intuition because we know the human brain to be a remarkable predictor (though we are objectively bad lay statisticians). We have adapted the ability to quickly detect threatening objects and to do so in the direction of greater false positives than negatives. Given what is known about the adaptability of the brain, its proclivity to make predictions (in terms of lessening energy costs), and the false positive bias, learned associations may be driving many of these predictions. Such predictions require descending neural processing, and like our conscious navigating of our complex, social environments, they are susceptible to errors and heuristically biased assumptions.</p>
</div>
<div id="limits-to-these-approaches" class="section level3">
<h3><span class="header-section-number">6.2.7</span> Limits to these approaches</h3>
<p>While some evidence has converged on the predictive advantage of both emotional and motivational states, there is still an ongoing debate as to where exactly these differences exist. Attention, response biases and demand characteristics are each potentially contributing to findings (for a detailed review see Firestone &amp; Scholl, 2016). Indeed, parsing through whether prior information biases early visual processing (e.g., V1) is a controversial topic. Though scientists would agree on many substantive evaluations of visual processing (e.g., parallel processing), prefrontal cortical access to V1 is one discrepancy that is challenging to resolve using reverse inference (imaging studies) and ineffectually controlled behavioral paradigms. Critics of the descending neural pathway view of object recognition suggest that evidence cited above may be primarily attributed to judgements, and that the scope of our current technologies limits the claims that can be made. For instance, imaging studies using functional magnetic resonance imaging (fMRI) often rely on patterns of activation and lack temporal resolution.</p>
<p>Moreover, it is completely uncontroversial to not the interconnected nature of the brain. A large number of studies utilizing neurophysiological or emotion/motivational methods have emphasized specific “centers” in the brain. However, it still remains unclear how selective these neural regions are. For instance, the amygdala was once thought to selectively attend to fear/threat stimuli (Davis, 1997). Recent research has suggested this may not be the case. Instead, the amygdala seems to come online for a number of stimulus features, including emotional saliency (not just fear) and novelty (Sander, Grafman, &amp; Zalla, 2003). What about how we process faces? What makes this different than how we process objects? Additionally, dealing with assessing differences in perception versus judgement findings still marks an important and difficult task.</p>
</div>
</div>
<div id="cognitive-and-computational-approaches" class="section level2">
<h2><span class="header-section-number">6.3</span> Cognitive and computational approaches</h2>
<p>Again, the primary aim of the present chapter is to synthesize evidence across domains that hint at the penetrability of perception. Instead of being able to draw a hard line between cognition and perception, evidence continues to mount suggesting perceptual experiences are shaped by temporal and spatial predictions (Rohenkohl, Gould, Pessoa, &amp; Nobre, 2014). The location of this influence, then, is the critical component in debate. Indeed, it would be uncontroversial to find that expectations shape judgements.</p>
<p>Instead of focusing on the physical regions of the brain that may allow for a particular sequence of neuronal firing, cognitive approaches are rooted in information processing theory. Namely, exogenous stimuli provide particular information that is then transduced and processed through the brain, much like a metaphorical computer. These approaches to object recognition have, like with models of instance theory (Hintzman, 1986), back-propagation (Rumelhart, Hinton, &amp; Williams, 1986) and hierarchical models of associative memory (Fukushima, 1984), illustrated the benefit of representing structures in a way that makes mapping and generalizing patterns feasible. Though it is unlikely that human object recognition and computer vision converge, utilizing these models to combine them with theory of human vision may reveal interesting predictions and advances in the understanding of perception.</p>
<div id="cognitive-approaches" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Cognitive Approaches</h3>
<p>Before diving into cognitive theories of object recognition, it is first important to situate visual perception within the cognitive domain. Research within this domain focused on characteristics and patterns of visual perception (Gibson,1950) and dominate processing styles (Navon, 1977). These research enterprises have spurred a great deal of subsequent visual research and continue to inform models today. For instance, Navon (1977) explicated how processing of global features precedes that of local features. This perspective directly maps onto other theories that consider the predictive mind and the quick processing of coarse information through M-pathway channels.</p>
<p>Early cognitive theories of object recognition were grounded in information processing perspectives. One such perspective came from Biederman (1987) where he posited that objects are made up of reducible units called ‘geons’. These units provide that foundation for the visual system to build up and recognize more complex objects and scenes. This view, much like early semantic models (see McClelland &amp; Rumerlhart, 1981), relied on basic feature detection as the mechanism that allows for complex combinations and processing of visual information. From this perspective, object attributes like size and location do not appear to be integral to the recognition process. For example, researchers in a priming study found that object representations were not affected by removal of attributes or alteration of left-right orientation, suggesting that identification is occurring on the geon level (Biederman &amp; Cooper, 1991; Hummel &amp; Biederman, 1992). Further, controlled cognitive processes (e.g., semantic) cannot account for differences in priming effects. Instead, matched exemplars do not see a recognition advantage (Biederman &amp; Cooper, 1991). While these approaches have yielded important results, there are still limitations in the organization of these attributes and geons, perspective and field of vision variations, and the concentration on a traditional bottom-up sequence that is triggered by individual stimulus properties.</p>
<p>Several other factors contribute to and moderate successful object recognition. Some work has attempted to extend basic units for recognition approaches by including subliminal priming. These approaches allow for the more specific understanding of how object representations reach the conscious level. For example, without inclusion of semantic effects, visual subliminal priming facilitated object recognition, even in instances in which the objects location had changed (Bar &amp; Biederman, 1998). Other work has demonstrated the effects of color on object recognition, which highlight some important features. Namely, color did not facilitate the identification and recognition of objects that were manufactured or that did not naturally occur in the presented color scheme (Humphrey, Goodale, Jakobson, &amp; Servos, 1994). These findings underscore two important concepts; first, the cascade of processing that allows for integration of more complex information (including color) over the series, and second, the importance of expectation in the identification process. Colors that did not match their typical or predicted form did not facilitate the recognition process (Humphrey et al., 1994). Evidence in this direction supports the notion that prior and expectancies are informing ongoing visual perception. Encountering a mismatch of expectation requires more effortful, controlled processing to make sense of the prediction error.</p>
</div>
<div id="computational-approaches" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Computational Approaches</h3>
<p>Computational models of object recognition include a variety of methods and purposes. Many of these recent models primarily focus on error-reduction or variance mapping as a means to achieve a specific outcome, with little care for cognitive or neurophysiological theories. However, even these models still enable interesting, fruitful tests of object perception and cognitive penetration.</p>
<p>Attneave (1954) first emphasized that the primary role of visual perception is to process relevant information. From there, he claimed, it becomes clear how repetitive and interdependent the majority of our visual experience is, and how these associations allow for perceptual processes to incorporate higher-order information, as it is purely economical to do so. In spite of this perspective, much of the literature still focused on ascending, feed-forward processing. For example, Marr (1982) developed a prominent theoretical approach to object recognition that emphasized computational methods, the complexity of constructing 3-D representations, and the bottom-up nature of processing stages. Additionally, Marr (1982) emphasized the influence of viewpoint on object perception. Indeed, it seems that viewpoint is an important factor for the object identification and not the object categorization process (for review see Milivojevic, 2012). Again, the bottom-up approach has illuminated a number of facets of the visual perceptual process that are important but is fundamentally excluding how top-down processes interact. To understand how we utilize prior knowledge, how it is integrated, descending neural pathways and the corresponding information must be included.</p>
<p>As mentioned above, some computational models have focused less on updating or integrating a cognitive theory of visual perception, instead favoring an outcome-related, engineering approach. Machine learning and computer algorithms have given rise to research devoted to creation of technological advances in the categorization and decoding of objects. One study found that stimulus representation cortical patterns can predict the contents of sleep imagery by correlating patterns of hallucinations during sleep with specific patterns of stimuli representations while awake (Horikawa, Tamaki, Miyawaki, &amp; Kamitani, 2013). Another interesting study reconstructed faces by correlating trained faces with patterns of voxel activity (Cowen, Chun, &amp; Kuhl, 2014). However, unlike objects, faces do not contain much variance in the general structure, which makes conservation of integral information after a primary component analysis much more straightforward. These studies highlight how computer-driven methodology can produce meaningful results, but without a theoretical foundation, how these results can inform ongoing debates on human vision is often obfuscated. How are these patterns of neural activity representing different features of objects or faces? How does viewpoint and an object’s position in space and time affect perception and recognition? These questions are fundamental to the understanding of complex object recognition, and necessary to the question of integration of expectation and prediction.</p>
<p>Other computational models have focused more specifically on the combination of theory and empirical evidence. Such models, like predictive coding models (Friston, 2010; Clark 2013), utilize Bayesian theory, and generative models to propose hierarchical perceptual processing that is integrated with descending connections from high-order cortical structures. Instead of purely processing in a feed-forward manner, the human brain is constantly maintaining a representation of the external environment that is informed by past experience, motivations and emotions, memory, and object values. Predictive coding purposes that optimization of perception and action relies on the minimization of prediction errors with recurrent loops (Friston, 2008; Friston, 2010). This idea harkens back to neurophysiological models of the dynamic, iterative brain (Cunningham &amp; Zelazo, 2007). Here, the predictive coding model informs both computational theories by introducing ways in which information is represented (e.g., prediction error) and how those signals are integrated into ongoing processes.</p>
<p>Predictive coding reveals how the brain may be economically reducing the processing power required to manage the massive amounts of sensory information. This information is quickly assessed to allow appropriate responding to environment momenta. By understanding that the brain is relying on presuppositions about the organization and probabilities of specific pieces of information, we can begin to make sense of the disparate findings within the psychological study of vision. Whereas bottom-up, hierarchical processes explain how representation units are passed from one area to the next when encountering input, descending pathways hold information about expectations, predictions, and incorporate error (Clark, 2013). This approach parallels models of associative learning whereby bottom-up learning provides necessary cues for encoding, but retrieval is not perfect. Instead, retrieval is related to a number of environmental aspects of the encounter. Research suggests that frequency (Tulving, 1972), emotional value (Carstensen, Fung &amp; Charles, 2003; Teasdale &amp; Russell, 1983), and many other features (for another example, see Storbeck &amp; Clore, 2005) of both internal and external experience overlap and account for variance in accurate retrieval.</p>
<p>Still evidence has not adequately delineated to what extent and at what level priors and expectations may influence perception and object recognition. Bayesian priors have oft been utilized as a means for incorporating high-level information into processing. But the question of where the priors interact still remains. Moreover, the distinction between types of top-down influences and cognitive penetration may be an important feature to explore (Hohwy, 2017). What kind of information are they holding? Hohwy (2017) describes the ways in which the minimization of prediction error coupled with Bayesian priors can lead to instances of cognitive penetrability. Indeed, there is a theoretical requirement and a practical one to the inclusion of error minimization processes in cases in which expectations are especially strong to encounter particular stimuli. Otherwise, it is difficult to reconcile how information is learned so that it may be integrated into an expectation.</p>
<p>Further, the iterative nature of processing allows for prediction and corroborating (or not) evidence to occur time and time again. Much like associative memory models, the very repetition across contexts can allow for the decoupling of the stimulus in the original environment but holds onto the cooccurrence of an object and the surrounding environment. The goal of perception is to be accurate on a global level, meaning prediction errors that occur in response to visual illusions should be considered functional for the minimization of error over time (Lupyan, 2015; Purves et al., 2011). Although it is indeed true that low-level, sensory signals are necessary input for the process, the synthesizing of visual input is not passive. Instead, perception has been called a “constructive process of turning various forms of energy (mechanical, chemical, electromagnetic) into<em>information</em>useful for guiding behavior” (Lupyan, 2015). Moreover, recent research in the emotion field has suggested that language is a major contributor to the emotional cascade. This sentiment has been paralleled in other perceptional processes (Nook et al., 2017). Lupyan and Clark (2015) have proposed that language plays a vital role in visual perceptual processes as well. If language is one top-down constraint on perception, a number of mixed findings within cultural psychology can begin to merge. This represents an informative and interesting perspective for the many cognitive processes that require prediction and perception.</p>
</div>
<div id="conclusion-2" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Conclusion</h3>
<p>In sum, cognitive and computational approaches have incorporated findings from neurophysiology to support understanding the process by which visual object recognition occurs. Cognitive approaches have demonstrated specific units that are represented in the cascade, uncovering the increasingly complex series. Two-dimensional surfaces, patterns, colors, edges, and many other aspects of objects all inform the visual system and aid in the overall identification process. Computational approaches allow for modelling of unobserved phenomena that has extended what was previously understood about the hierarchical nature of the brain. These models have demonstrated how prediction and prediction error can make sense of our complex perceptual system.</p>
</div>
<div id="limits-to-these-approaches-1" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Limits to these approaches</h3>
<p>However, these approaches are also subject to limitations. Namely, cognitive studies which reduce objects to basic units suffer much of the same problem that neurophysiological findings do, they are constrained by only studying aspects in isolation. For instance, Biederman (1991) cannot account for a number of naturally occurring visual “environments”. Point of view, field of vision, emotional or motivational state, attentional biases, and more all interact with the most fundamental features of visual object recognition. Computational cognitive approaches run into slightly different issues. Most models require training from set of stimuli, which can lead to biases. Further, this research has given rise to the technologies that have creeped into nearly every facet of our daily lives. Facial recognition is used to unlock smartphones, and while this may be an efficient means for accessing a handheld device, there are still a number of implications. Importantly, the machine learning training sets may be systematically biased, leading to a biased algorithm and subsequent codification systems that rely on this type of data synthesis.</p>
</div>
</div>
<div id="conclusion-and-implications" class="section level2">
<h2><span class="header-section-number">6.4</span> Conclusion and Implications</h2>
<p>Both the cognitive and the neuropsychological approaches to studying the visual system and object recognition have revealed unique solutions and more interesting questions. Yet, there is much still to be learned and integrated across these domains. Indeed, computational cognitive models have been exceedingly informative, particularly in regard to application in computer vision. What can be gleaned from integrating these approaches to object recognition?</p>
<p>The models and approaches mentioned were by no means exhaustive. Instead, the purpose of the present chapter was to investigate what perceptual and object recognition evidence exists within both neurophysiological and cognitive studies that provides a foundation for conceptualizing the human brain as a predictive machine. Neurophysiology has demonstrated where information is entering, how it proceeds, and demarcated what cells and areas are doing distinct work. However, a number of these models do not incorporate theory on dynamic, integrated processing, instead focusing on isolated units. Cognitive theories diverge on this point. The center around extensive information processing formulations. Though some still consider modular accounts of cognition, the orientation toward process is valuable and something to be incorporated in future research.</p>
<p>Informed by neurophysiological findings, we see advances in abnormal mind perception. From schizophrenia (Butler, Silverstein, &amp; Dakin, 2008) to autism spectrum disorder (Dakin &amp; Frith, 2005; Grice et al., 2001), the etiological underpinnings of nonconforming minds may be better understood by defining specializations of specific cortical areas in the brain. Studies that are concerned with representations and information processing have been particularly helpful in the technological domain as computers work like this. For example, major advances in computer vision have been spearheaded by cognitive computational models (Brown, 1985; Ullman et al., 2016). Recently, attempts at computer vision have shifted to convolution networking to better match the complexity and accuracy achieved by the human brain (Simonyan &amp; Zisserman, 2014). Again, there are clear applications of this work and the theories that emerge through trials are potentially informative for a number of other phenomena. Namely, perception is one link to the consciousness. Understanding how prediction and sensory information are integrated in our brain can provide a meaningful step toward solving problems of consciousness.</p>
<p>The biases that predictions and expectations produce exist elsewhere (such as visual attention) and have profound effects on downstream cognitive processes like judgments and behavior. For instance, evidence clearly shows that biases influence important social customs like eye-witness reporting (MacLeod, 2002; Storbeck &amp; Clore, 2005). The consequences of these decisions are clear and understanding how emotion regulates and biases affective feelings and associations may contribute to understanding how to recalibrate social systems where a great deal of the ultimate decision depend on individual accounts.</p>
</div>
<div id="references-4" class="section level2">
<h2><span class="header-section-number">6.5</span> References</h2>
<p>Adolphs, R., &amp; Spezio, M. (2006). Role of the amygdala in processing visual social stimuli.<em>Progress in brain research</em>,<em>156</em>, 363-378.</p>
<p>Alpers, G. W., &amp; Gerdes, A. (2007). Here is looking at you: emotional faces predominate in binocular rivalry.<em>Emotion</em>,<em>7</em>(3), 495.</p>
<p>Alpers, G. W., Ruhleder, M., Walz, N., Mühlberger, A., &amp; Pauli, P. (2005). Binocular rivalry between emotional and neutral stimuli: A validation using fear conditioning and EEG.<em>International Journal of Psychophysiology</em>,<em>57</em>(1), 25-32.</p>
<p>Amaral,J.L.Price,A.Pitkänen,S.T.Carmichael (1992). Anatomical organization of the primate amygdaloid complex. J.P.Aggleton(Ed.)<em>, The Amygdala:Neurobiological Aspects of Emotion, Memory, and Mental Dysfunction,Wiley-Liss</em>,New York(1992), pp.1-66</p>
<p>Amodio, D. M., Zinner, L. R., &amp; Harmon-Jones, E. (2007). Social psychological methods of emotion elicitation.<em>Handbook of emotion elicitation and assessment</em>, 91.</p>
<p>Attneave, F. (1954). Some informational aspects of visual perception.<em>Psychological review</em>,<em>61</em>(3), 183.</p>
<p>Aviezer, H., Hassin, R. R., Ryan, J., Grady, C., Susskind, J., Anderson, A., … &amp; Bentin, S. (2008). Angry, disgusted, or afraid? Studies on the malleability of emotion perception. Psychological science, 19(7), 724-732.</p>
<p>Awh, E. &amp; Pashler, H. (2000). Evidence for split attentional foci. <em>Journal of Experimental Psychology: Human Perception and Performance, 26</em>, 834-846.</p>
<p>Balcetis, E. &amp; Dunning, D. (2006). See what you want to see: Motivational influences on visual perception. <em>Journal of Personality and Social Psychology, 91</em>, 612-625.</p>
<p>Balcetis, E., Dunning, D., &amp; Granot, Y. (2012). Subjective value determines initial dominance in binocular rivalry.<em>Journal of Experimental Social Psychology</em>,<em>48</em>(1), 122-129.</p>
<p>Balcetis, E. (2016). Approach and avoidance as organizing structures for motivated distance perception.<em>Emotion Review</em>,<em>8</em>(2), 115-128.</p>
<p>Balcetis, E., &amp; Dunning, D. (2010). Wishful seeing: More desired objects are seen as closer.<em>Psychological science</em>,<em>21</em>(1), 147-152.</p>
<p>Bar, M. (2000). A Cortical Mechanism for Triggering Top-Down, <em>Journal of Cognitive Neuroscience.</em> 15:4, pp. 600–609.</p>
<p>Bar, M. (2003). A cortical mechanism for triggering top-down facilitation in visual object recognition. <em>Journal of Cognitive Neuroscience, 15</em>, 600-609.</p>
<p>Bar, M., Kassam, K. S., Ghuman, A. S., Boshyan, J., Schmid, A. M., Dale, A. M., Hamalainen, M. S., Marinkovic, K., Schacter, D. L., Rosen, B. R., &amp; Halgren, E. (2006). Top-down facilitation of visual recognition. <em>Proceedings of the National Academy of Sciences, 103</em>, 449-454.</p>
<p>Bar, M., &amp; Biederman, I. (1998). Subliminal visual priming.<em>Psychological Science</em>,<em>9</em>(6), 464-468.</p>
<p>Barrett, L. F., Bar, M., (2009). See it with feeling: affective predictions during object perception. <em>Philosophical Transactions of The Royal Society.</em> 1325–1334.</p>
<p>Bradley, M. M., Sabatinelli, D., Lang, P. J., Fitzsimmons, J. R., King, W., &amp; Desai, P. (2003). Activation of the visual cortex in motivated attention.<em>Behavioral neuroscience</em>,<em>117</em>(2), 369.</p>
<p>Brown, S. W. (1985). Time perception and attention: The effects of prospective versus retrospective paradigms and task demands on perceived duration.<em>Perception &amp; Psychophysics</em>,<em>38</em>(2), 115-124.</p>
<p>Biederman, I. (1987). Recognition-by-components: a theory of human image understanding.<em>Psychological review</em>,<em>94</em>(2), 115.</p>
<p>Biederman, I., &amp; Cooper, E. E. (1991). Priming contour-deleted images: Evidence for intermediate representations in visual object recognition.<em>Cognitive psychology</em>,<em>23</em>(3), 393-419.</p>
<p>Britton, J. C., Taylor, S. F., Sudheimer, K. D., &amp; Liberzon, I. (2006). Facial expressions and complex IAPS pictures: Common and differential networks. <em>NeuroImage</em>, <em>31</em>(2), 906–919. <a href="https://doi.org/10.1016/j.neuroimage.2005.12.050" class="uri">https://doi.org/10.1016/j.neuroimage.2005.12.050</a></p>
<p>Bullier, J. (2001). Integrated model of visual processing. Brain Research. Brain Research Reviews, 36(2/3), 96–107.</p>
<p>Burr, D. C., Morrone, M. C., &amp; Ross, J. (1994). Selective suppression of the magnocellular visual pathway during saccadic eye movements.<em>Nature</em>,<em>371</em>(6497), 511.</p>
<p>Butler, P. D., Silverstein, S. M., &amp; Dakin, S. C. (2008). Visual perception and its impairment in schizophrenia.<em>Biological psychiatry</em>,<em>64</em>(1), 40-47.</p>
<p>Campbell, J. I., &amp; Thompson, V. A. (2012). MorePower 6.0 for ANOVA with relational confidence intervals and Bayesian analysis.<em>Behavior research methods</em>,<em>44</em>(4), 1255-1265.</p>
<p>Carstensen, L. L., Fung, H. H., &amp; Charles, S. T. (2003). Socioemotional selectivity theory and the regulation of emotion in the second half of life.<em>Motivation and emotion</em>,<em>27</em>(2), 103-123.</p>
<p>Caruso, E. M., Mead, N. L., &amp; Balcetis, E. (2009). Political partisanship influences perception of biracial candidates’ skin tone.<em>Proceedings of the National Academy of Sciences</em>,<em>106</em>(48), 20168-20173.</p>
<p>Chen, C. M., Lakatos, P. S., Shah, A. S., Mehta, A. D., Givre, S. J., Javitt, D. C., &amp;</p>
<p>Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral and brain sciences, 36(3), 181-204.</p>
<p>Clore, G. L., &amp; Huntsinger, J. R. (2007). How emotions inform judgment and regulate thought. <em>Trends in Cognitive Sciences</em>, <em>11</em>(9), 393–399. <a href="https://doi.org/10.1016/j.tics.2007.08.00" class="uri">https://doi.org/10.1016/j.tics.2007.08.00</a></p>
<p>Clore, G. L., Gasper, K., &amp; Garvin, E. (2001). Affect as information.<em>Handbook of affect and social cognition</em>, 121-144.</p>
<p>Cole, S., Balcetis, E., &amp; Dunning, D. (2013). Affective signals of threat increase perceived proximity.<em>Psychological science</em>,<em>24</em>(1), 34-40.</p>
<p>Correll, J., Park, B., Judd, C. M., &amp; Wittenbrink, B. (2002). The police officer’s dilemma: Using ethnicity to disambiguate potentially threatening individuals.<em>Journal of personality and social psychology</em>,<em>83</em>(6), 1314.</p>
<p>Correll, J., Hudson, S. M., Guillermo, S., &amp; Earls, H. A. (2017). Of kith and kin: Perceptual enrichment, expectancy, and reciprocity in face perception.<em>Personality and Social Psychology Review</em>,<em>21</em>(4), 336-360.</p>
<p>Cowen, A. S., Chun, M. M., &amp; Kuhl, B. A. (2014). Neural portraits of perception: reconstructing face images from evoked brain activity. Neuroimage, 94, 12–22.</p>
<p>Cunningham, W. A., &amp; Zelazo, P. D. (2007). Attitudes and evaluations: A social cognitive neuroscience perspective.<em>Trends in cognitive sciences</em>,<em>11</em>(3), 97-104.</p>
<p>Dakin, S., &amp; Frith, U. (2005). Vagaries of visual perception in autism.<em>Neuron</em>,<em>48</em>(3), 497-507.</p>
<p>Davis, M. (1997). Neurobiology of fear responses: the role of the amygdala.<em>The Journal of neuropsychiatry and clinical neurosciences</em>.</p>
<p>Descartes, R. (1984).<em>The philosophical writings of Descartes: Volume 3, the correspondence</em>(Vol. 3). Cambridge University Press.</p>
<p>DeSteno, D., Dasgupta, N., Bartlett, M. Y., &amp; Cajdric, A. (2004). Prejudice from thin air: The effect of emotion on automatic intergroup attitudes. Psychological Science, 15(5), 319-324.</p>
<p>DiCarlo, J. J., Zoccolan, D., &amp; Rust, N. C. (2012). How does the brain solve visual object recognition?.<em>Neuron</em>,<em>73</em>(3), 415-434.</p>
<p>Easterbrook, J. A. (1959). The effect of emotion on cue utilization and the organization of behavior. <em>Psychological Review</em>, <em>66</em>, 183-201.</p>
<p>Esses, V. M., &amp; Dovidio, J. F. (2002). The role of emotions in determining willingness to engage in intergroup contact. Personality and Social Psychology Bulletin, 28(9), 1202-1214.</p>
<p>Felleman, D. J., &amp; Van, D. E. (1991). Distributed hierarchical processing in the primate cerebral cortex.<em>Cerebral cortex (New York, NY: 1991)</em>,<em>1</em>(1), 1-47.</p>
<p>Fenske, M. J., Aminoff, E., Gronau, N., &amp; Bar, M. (2006). Top-down facilitation of visual object recognition: object-based and context-based contributions.<em>Progress in brain research</em>,<em>155</em>, 3-21</p>
<p>Firestone, C., &amp; Scholl, B. J. (2016). Cognition does not affect perception: Evaluating the evidence for&quot; top-down&quot; effects.<em>Behavioral and brain sciences</em>,<em>39</em>.</p>
<p>Fitzpatrick D, Itoh K, Diamond IT. The laminar organization of the lateral geniculate body and the striate cortex in the squirrel monkey (Saimiri sciureus). J Neurosci. 1983;3:673–702. PubMed PMID: 6187901.</p>
<p>Forgas, J. P. (2013). Don’t worry, be sad! On the cognitive, motivational, and interpersonal benefits of negative mood.<em>Current Directions in Psychological Science</em>,<em>22</em>(3), 225-232.</p>
<p>Friston, K. (2008). Hierarchical models in the brain. PLoS computational biology, 4(11), e1000211.</p>
<p>Friston, K. (2010). The free-energy principle: a unified brain theory?. Nature Reviews Neuroscience, 11(2), 127.</p>
<p>Friston and Clark</p>
<p>Gable, P. A. &amp; Harmon-Jones, E. (2010). The blues broaden, but the nasty narrows: Attentional consequences of negative affects low and high in motivational intensity. <em>Psychological Science, 21</em>, 211-215.</p>
<p>Gantman, A. P., &amp; Van Bavel, J. J. (2014). The moral pop-out effect: Enhanced perceptual awareness of morally relevant stimuli.<em>Cognition</em>,<em>132</em>(1), 22-29.</p>
<p>Gibson, J. J. (1950). The perception of the visual world.</p>
<p>Goodale, M. A., &amp; Milner, A. D. (1992). Separate visual pathways for perception and action.<em>Trends in neurosciences</em>,<em>15</em>(1), 20-25.</p>
<p>Goodale, M. A., &amp; Westwood, D. A. (2004). An evolving view of duplex vision: separate but interacting cortical pathways for perception and action.<em>Current opinion in neurobiology</em>,<em>14</em>(2), 203-211.</p>
<p>Grice, S. J., Spratling, M. W., Karmiloff-Smith, A., Halit, H., Csibra, G., de Haan, M., &amp; Johnson, M. H. (2001). Disordered visual processing and oscillatory brain activity in autism and Williams syndrome. Neuroreport, 12(12), 2697-2700.</p>
<p>Haidt, J. (2003). The moral emotions.<em>Handbook of affective sciences</em>,<em>11</em>(2003), 852-870.</p>
<p>Haidt, J. (2007). The new synthesis in moral psychology.<em>science</em>,<em>316</em>(5827), 998-1002.</p>
<p>Harmon-Jones, E., &amp; Gable, P. A. (2008). Incorporating motivational intensity and direction into the study of emotions: Implications for brain mechanisms of emotion and cognition-emotion interactions.<em>Netherlands Journal of Psychology</em>,<em>64</em>(4), 132-142.</p>
<p>Harmon-Jones, E., Gable, P. A., &amp; Price, T. F. (2013). Does Negative Affect Always Narrow and Positive Affect Always Broaden the Mind? Considering the Influence of Motivational Intensity on Cognitive Scope. <em>Current Directions in Psychological Science</em>, <em>22</em>(4), 301–307. <a href="https://doi.org/10.1177/0963721413481353" class="uri">https://doi.org/10.1177/0963721413481353</a></p>
<p>Harmon-Jones, E., &amp; Gable, P. A. (2017). On the role of asymmetric frontal cortical activity in approach and withdrawal motivation: An updated review of the evidence. <em>Psychophysiology</em>, (May 2016). <a href="https://doi.org/10.1111/psyp.12879" class="uri">https://doi.org/10.1111/psyp.12879</a></p>
<p>Haxby, J. V., Hoffman, E. A., &amp; Gobbini, M. I. (2002). Human neural systems for face recognition and social communication. <em>Biological Psychiatry</em>, <em>51</em>(1), 59–67.</p>
<p>Helfrich, R. F., Huang, M., Wilson, G., &amp; Knight, R. T. (2017). Prefrontal cortex modulates posterior alpha oscillations during top-down guided visual perception.<em>Proceedings of the National Academy of Sciences</em>,<em>114</em>(35), 9457-9462.</p>
<p>Helzer, E. G., &amp; Pizarro, D. A. (2011). Dirty liberals! Reminders of physical cleanliness influence moral and political attitudes.<em>Psychological science</em>,<em>22</em>(4), 517-522.</p>
<p>Hintzman, D. (1986). Schema abstraction in a multiple-trace memory model. Psychological Review, 93, 411–428. Fukushima, K. (1984). A hierarchical neural network model for associative memory.<em>Biological cybernetics</em>,<em>50</em>(2), 105-113.</p>
<p>Hummel, J. E., &amp; Biederman, I. (1992). Dynamic binding in a neural network for shape recognition.<em>Psychological review</em>,<em>99</em>(3), 480.</p>
<p>Horikawa, T., Tamaki, M., Miyawaki, Y., &amp; Kamitani, Y. (2013). Neural decoding of visual imagery during sleep. Science, 340, 639–642.</p>
<p>Hohwy, J. (2017). Priors in perception: Top-down modulation, Bayesian perceptual learning rate, and prediction error minimization.<em>Consciousness and cognition</em>,<em>47</em>, 75-85.</p>
<p>Humphrey, G. K., Goodale, M. A., Jakobson, L. S., &amp; Servos, P. (1994). The role of surface information in object recognition: Studies of a visual form agnosic and normal subjects.<em>Perception</em>,<em>23</em>(12), 1457-1481.</p>
<p>Inbar, Y., Pizarro, D. A., Knobe, J., &amp; Bloom, P. (2009). Disgust sensitivity predicts intuitive disapproval of gays.<em>Emotion</em>,<em>9</em>(3), 435.</p>
<p>Johnson, M. H. (2001). Disordered visual processing and oscillatory brain activity in autism and Williams syndrome.<em>Neuroreport</em>,<em>12</em>(12), 2697-2700.</p>
<p>Kahneman, D. (2011).<em>Thinking, fast and slow</em>. Macmillan.</p>
<p>Kastner, S., Ungerleider, L. G. (2000). Mechanisms of visual attention in the human cortex.<em>Annual review of neuroscience</em>,<em>23</em>(1), 315-341.</p>
<p>Kringelbach, M. L., &amp; Rolls, E. T. (2004). The functional neuroanatomy of the human orbitofrontal cortex: evidence from neuroimaging and neuropsychology.<em>Progress in neurobiology</em>,<em>72</em>(5), 341-372.</p>
<p>Kringelbach, M. L. (2005). The human orbitofrontal cortex: linking reward to hedonic experience.<em>Nature Reviews Neuroscience</em>,<em>6</em>(9), 691-702.</p>
<p>Kveraga, K., Boshyan, J., &amp; Bar, M. (2007). Magnocellular projections as the trigger of top-down facilitation in recognition. <em>Journal of Neuroscience, 27</em>, 13232-13240</p>
<p>Lamme, V. A. &amp; Roelfsema, P. R. (2000). The distinct modes of vision offered by feedforward and recurrent processing. Trends Neurosci. 23, 571–579.</p>
<p>Lang, P., Bradley, M., &amp; Cuthbert, B. (1999). <em>International Affective Picture System (IAPS): Technical manual and affective ratings</em>. Gainesville, FL: The Center for Research in Psychophysiology, University of Florida.</p>
<p>Lang, P., &amp; Bradley, M. M. (2007). The International Affective Picture System (IAPS) in the study of emotion and attention.<em>Handbook of emotion elicitation and assessment</em>,<em>29</em>.</p>
<p>Lazarus, R. S. (1982). Thoughts on the relations between emotion and cognition.<em>American psychologist</em>,<em>37</em>(9), 1019.</p>
<p>LeDoux, J. (1998). Fear and the brain: where have we been, and where are we going?.<em>Biological psychiatry</em>,<em>44</em>(12), 1229-1238.</p>
<p>LeDoux, S. F. (2002). Defining natural sciences.<em>Behaviorology Today</em>,<em>5</em>(1), 34-36.</p>
<p>Lerner, J. S., &amp; Keltner, D. (2001). Fear, anger, and risk.<em>Journal of personality and social psychology</em>,<em>81</em>(1), 146.</p>
<p>Lerner, J. S., Small, D. A., &amp; Loewenstein, G. (2004). Heart strings and purse strings: Carryover effects of emotions on economic decisions.<em>Psychological science</em>,<em>15</em>(5), 337-341.</p>
<p>Lupyan, G. (2015). Cognitive penetrability of perception in the age of prediction: Predictive systems are penetrable systems.<em>Review of philosophy and psychology</em>,<em>6</em>(4), 547-569.</p>
<p>Lupyan, G., &amp; Clark, A. (2015). Words and the world: Predictive coding and the language-perception-cognition interface.<em>Current Directions in Psychological Science</em>,<em>24</em>(4), 279-284.</p>
<p>MacLeod MD. (2002) Retrieval-induced forgetting in eyewitness memory: forgetting as a consequence of remembering.Appl. Cognit. Psychol. 16:135–149.</p>
<p>Marr, D. (1982).<em>Vision: A Computational Investigation Into</em>. WH Freeman.</p>
<p>Maunsell, J. H., Nealey, T. A., &amp; DePriest, D. D. (1990). Magnocellular and parvocellular contributions to responses in the middle temporal visual area (MT) of the macaque monkey.<em>Journal of Neuroscience</em>,<em>10</em>(10), 3323-3334.</p>
<p>McClelland, J. L., &amp; Rumelhart, D. E. (1981). An interactive activation model of context effects in letter perception: I. An account of basic findings.<em>Psychological review</em>,<em>88</em>(5), 375.</p>
<p>Merigan, W. H., &amp; Maunsell, J. H. (1993). How parallel are the primate visual pathways?.<em>Annual review of neuroscience</em>,<em>16</em>(1), 369-402.</p>
<p>Milivojevic, B. (2012). Object Recognition Can Be Viewpoint Dependent or Invariant–It’s Just a Matter of Time and Task. Frontiers in computational neuroscience, 6, 27.</p>
<p>Milner, A. D., &amp; Goodale, M. A. (1995). The visual brain in action. Oxford: Oxford University Press.</p>
<p>Milner, A. &amp; Goodale, M. (2008). Two visual systems re-viewed. <em>Neuropsychologia, 46</em>, 774-785.</p>
<p>Navon, D. (1977). Forest before trees: The precedence of global features in visual perception.<em>Cognitive psychology</em>,<em>9</em>(3), 353-383.</p>
<p>Nook, E. C., Sasse, S. F., Lambert, H. K., McLaughlin, K. A., &amp; Somerville, L. H. (2017). Increasing verbal knowledge mediates development of multidimensional emotion representations.<em>Nature Human Behaviour</em>,<em>1</em>(12), 881.</p>
<p>O’Callaghan, C., Kveraga, K., Shine, J. M., Adams, R. B., &amp; Bar, M. (2016). Convergent evidence for top-down effects from the “predictive brain”.<em>Behavioral and Brain Sciences</em>,<em>39</em>.</p>
<p>Öhman, A., &amp; Mineka, S. (2001). Fears, phobias, and preparedness: Toward an evolved module of fear and fear learning. <em>Psychological Review, 108</em>, 483-522.</p>
<p>Öhman, A., Flykt, A., &amp; Esteves, F. (2001). Emotion drives attention- Snakes in the grass. <em>Journal of Experiemntal Psychology: General</em>, <em>130</em>(3), 466–478. <a href="https://doi.org/10.1037/AXJ96-3445.130.3.466" class="uri">https://doi.org/10.1037/AXJ96-3445.130.3.466</a></p>
<p>Öhman, A. (2005). The role of the amygdala in human fear: automatic detection of threat.<em>Psychoneuroendocrinology</em>,<em>30</em>(10), 953-958.</p>
<p>Perrett, D. I., &amp; Oram, M. W. (1993). Neurophysiology of shape processing.<em>Image and Vision Computing</em>,<em>11</em>(6), 317-333.</p>
<p>Pessoa, L. &amp; Adolphs, R. (2010). Emotion processing and the amygdala: From a ‘low road’ to ‘many roads’ of evaluating biological significance. <em>Nature Neuroscience Reviews, 11</em>, 77 -782.</p>
<p>Pessoa, L., McKenna, M., Gutierrez, E., &amp; Ungerleider, L. G. (2002). Neural processing of emotional faces requires attention.<em>Proceedings of the National Academy of Sciences</em>,<em>99</em>(17), 11458-11463.</p>
<p>Phelps, E. A., Ling, S., &amp; Carrasco, M. (2006). Emotion facilitates perception and potentiates the perceptual benefits of attention. <em>Psychological Science, 17</em>, 292-299.</p>
<p>Polyak S. Chicago: University of Chicago Press;The vertebrate visual system.1957.</p>
<p>Proffitt, D. R. (2006). Embodied perception and the economy of action.<em>Perspectives on psychological science</em>,<em>1</em>(2), 110-122.</p>
<p>Purves, D., Wojtach, W.T., &amp; Lotto, R.B.. (2011) Understanding vision in wholly empirical terms.<em>Proceedings of the National Academy of Sciences of the United States of America</em>108(Suppl 3): 15588–15595.</p>
<p>Pylyshyn, 1999</p>
<p>Riesenhuber, M., &amp; Poggio, T. (1999). Hierarchical models of object recognition in cortex.<em>Nature neuroscience</em>,<em>2</em>(11), 1019.</p>
<p>Rohenkohl, G., Gould, I. C., Pessoa, J., &amp; Nobre, A. C. (2014). Combining spatial and temporal expectations to improve visual perception.<em>Journal of vision</em>,<em>14</em>(4), 8-8.</p>
<p>Rolls, E.T. (1999). <em>The Brain and Emotion</em>. Oxford, UK: Oxford University Press.</p>
<p>Sabatinelli, D., Fortune, E. E., Li, Q., Siddiqui, A., Krafft, C., Oliver, W. T., … &amp; Jeffries, J. (2011). Emotional perception: meta-analyses of face and natural scene processing. Neuroimage, 54(3), 2524-2533.</p>
<p>Sander, D., Grafman, J., &amp; Zalla, T. (2003). The human amygdala: an evolved system for relevance detection.<em>Reviews in the Neurosciences</em>,<em>14</em>(4), 303-316.</p>
<p>Schmitz, T. W., De Rosa, E., &amp; Anderson, A. K. (2009). Opposing influences of affective state valence on visual cortical encoding.<em>Journal of Neuroscience</em>,<em>29</em>(22), 7199-7207.</p>
<p>Schmolesky, M. (1995). The Primary Visual Cortex. Webvision: The Organization of the Retina and Visual System, 1–38. <a href="https://doi.org/NBK11524" class="uri">https://doi.org/NBK11524</a></p>
<p>Schroeder, C. E. (2007). Functional anatomy and interaction of fast and slow visual pathways in macaque monkeys. <em>Cerebral Cortex, 17</em>, 1561-1569.</p>
<p>Shi, C. &amp; Davis, M. (2001). Visual pathways involved in fear conditioning measured with fear potentiated startle: behavioral and anatomic studies. <em>Journal of Neuroscience, 21</em>, 9844-55.</p>
<p>Shipp S, Zeki S. The organization of connections between areas V5 and V1 in macaque monkey visual cortex. Eur J Neurosci. 1989;1:309–332. PubMed PMID: 12106142.</p>
<p>Skelly, L. R., &amp; Decety, J. (2012). Passive and motivated perception of emotional faces: Qualitative and quantitative changes in the face processing network. PLoS One, 7(6), e40371.</p>
<p>Siegel, M., Donner, T. H., Oostenveld, R., Fries, P., &amp; Engel, A. K. (2008). Neuronal synchronization along the dorsal visual pathway reflects the focus of spatial attention. <em>Neuron</em>, <em>60</em>, 709-719.</p>
<p>Simonyan &amp; Zisserman, 2014</p>
<p>Somerville, L. H., Wagner, D. D., Wig, G. S., Moran, J. M., Whalen, P. J., &amp; Kelley, W. M. (2013). Interactions between transient and sustained neural signals support the generation and regulation of anxious emotion. <em>Cerebral Cortex</em>, <em>23</em>(1), 49–60. <a href="https://doi.org/10.1093/cercor/bhr373" class="uri">https://doi.org/10.1093/cercor/bhr373</a></p>
<p>Stefanucci, J. K., Proffitt, D. R., Clore, G. L., &amp; Parekh, N. (2008). Skating down a steeper slope: Fear influences the perception of geographical slant.<em>Perception</em>,<em>37</em>(2), 321-323.</p>
<p>Stern, C., Balcetis, E., Cole, S., West, T. V., &amp; Caruso, E. M. (2016). Government instability shifts skin tone representations of and intentions to vote for political candidates.<em>Journal of personality and social psychology</em>,<em>110</em>(1), 76.</p>
<p>Storbeck J, Clore GL. (2005). With sadness comes accuracy, with happiness, false memory: mood and the false memory effect.Psychol. Sci. 16:785–791.</p>
<p>Storbeck, J. &amp; Clore, G. L. (2007). On the interdependence of cognition and emotion.</p>
<p>Cognition &amp; Emotion, 21, 1212-1237</p>
<p>Storbeck, J. (2012). Performance Costs When Emotion Tunes Inappropriate Cognitive Abilities : Implications for Mental Resources and Behavior, <em>141</em>(3), 411–416.</p>
<p>Storbeck, J., Dayboch, J., &amp; Wylie, J. (2016). Fear broadens attention: Fear and happiness motivate attentional flexibility impairing split attentional foci. <em>Emotion.</em> Submitted.</p>
<p>Tapia, E., &amp; Breitmeyer, B. G. (2011). Visual consciousness revisited: magnocellular and parvocellular contributions to conscious and nonconscious vision.<em>Psychological Science</em>,<em>22</em>(7), 934-942.</p>
<p>Teasdale, J. D., &amp; Russell, M. L. (1983). Differential effects of induced mood on the recall of positive, negative and neutral words.<em>British Journal of Clinical Psychology</em>,<em>22</em>(3), 163-171.</p>
<p>Tiedens, L. Z., Unzueta, M. M., &amp; Young, M. J. (2007). An unconscious desire for hierarchy? The motivated perception of dominance complementarity in task partners. Journal of personality and social psychology, 93(3), 402.</p>
<p>Tulving, E. (1972). Episodic and semantic memory.<em>Organization of memory</em>,<em>1</em>, 381-403.</p>
<p>Ullman, S., Assif, L., Fetaya, E., &amp; Harari, D. (2016). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744-2749.</p>
<p>Ungerleider, L. G., &amp; Haxby, J. V. (1994). ‘What’and ‘where’in the human brain.<em>Current opinion in neurobiology</em>,<em>4</em>(2), 157-165.</p>
<p>Van Essen, D. C., Anderson, C. H., &amp; Felleman, D. J. (1992). Information processing in the primate visual system: an integrated systems perspective.<em>Science</em>,<em>255</em>(5043), 419-423.</p>
<p>Weber, J. (1996). Influences upon managerial moral decision making: Nature of the harm and magnitude of consequences.<em>Human Relations</em>,<em>49</em>(1), 1-22.</p>
<p>Whalen, P. J., Rauch, S. L., Etcoff, N. L., McInerney, S. C., Lee, M. B., &amp; Jenike, M. A. (1998). Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge. <em>Journal of Neuroscience</em>, <em>18</em>(1), 411–418. <a href="https://doi.org/9412517" class="uri">https://doi.org/9412517</a></p>
<p>Wiens, S., Peira, N., Golkar, A., &amp; Öhman, A. (2008). Recognizing masked threat: Fear betrays, but disgust you can trust.<em>Emotion</em>,<em>8</em>(6), 810.</p>
<p>Wyatte, D., Curran, T., &amp; O’Reilly, R. (2012). The limits of feedforward vision: recurrent processing promotes robust object recognition when objects are degraded.<em>Journal of Cognitive Neuroscience</em>,<em>24</em>(11), 2248-2261.</p>
<p>Zelazo, P. D., &amp; Cunningham, W. A. (2007). Executive function: Mechanisms underlying emotion regulation.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-methodology-for-microdosing-research-cognitive-behavioral-tasks-as-investigative-tools-for-tracking-low-dose-effects-of-psilocybin.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="brain-training-and-cognition.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/CrumpLab/cognitivetechnologies/tree/master/book/06-Wylie_Object.Rmd",
"text": "Edit"
},
"download": ["cog_tech.pdf", "cog_tech.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
